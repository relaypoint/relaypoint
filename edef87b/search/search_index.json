{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Relaypoint documentation. Relaypoint is a lightweight, high-performance API gateway designed to help startups secure and manage their APIs with minimal configuration. Table of Contents Getting Started Installation Configuration Reference Features Routing Load Balancing Rate Limiting Health Checks Metrics & Monitoring API Keys Examples Troubleshooting Best Practices Why Relaypoint? Relaypoint provides enterprise-grade API gateway features without the complexity: Simple YAML Configuration - Get started in minutes, not days Intelligent Load Balancing - Round-robin, least connections, weighted, and random strategies Flexible Rate Limiting - Per-route, per-IP, and per-API-key rate limiting Health Checks - Automatic backend health monitoring with circuit breaking Prometheus Metrics - Built-in observability with detailed request metrics Zero Dependencies - Single binary deployment with no external dependencies Quick Example # relaypoint.yml server: port: 8080 upstreams: - name: api-backend targets: - url: http://localhost:3001 - url: http://localhost:3002 load_balance: round_robin routes: - name: api path: /api/** upstream: api-backend ./relaypoint -config relaypoint.yml That's it! Relaypoint is now load balancing requests across your backend servers. Support GitHub Issues : Report bugs and request features Discussions : Community Q&A License Relaypoint is licensed under the Apache License 2.0 .","title":"Home"},{"location":"#table-of-contents","text":"Getting Started Installation Configuration Reference Features Routing Load Balancing Rate Limiting Health Checks Metrics & Monitoring API Keys Examples Troubleshooting Best Practices","title":"Table of Contents"},{"location":"#why-relaypoint","text":"Relaypoint provides enterprise-grade API gateway features without the complexity: Simple YAML Configuration - Get started in minutes, not days Intelligent Load Balancing - Round-robin, least connections, weighted, and random strategies Flexible Rate Limiting - Per-route, per-IP, and per-API-key rate limiting Health Checks - Automatic backend health monitoring with circuit breaking Prometheus Metrics - Built-in observability with detailed request metrics Zero Dependencies - Single binary deployment with no external dependencies","title":"Why Relaypoint?"},{"location":"#quick-example","text":"# relaypoint.yml server: port: 8080 upstreams: - name: api-backend targets: - url: http://localhost:3001 - url: http://localhost:3002 load_balance: round_robin routes: - name: api path: /api/** upstream: api-backend ./relaypoint -config relaypoint.yml That's it! Relaypoint is now load balancing requests across your backend servers.","title":"Quick Example"},{"location":"#support","text":"GitHub Issues : Report bugs and request features Discussions : Community Q&A","title":"Support"},{"location":"#license","text":"Relaypoint is licensed under the Apache License 2.0 .","title":"License"},{"location":"best-practises/","text":"This guide covers recommendations for running Relaypoint in production environments. Configuration Best Practices Use Descriptive Names # Good: Descriptive names routes: - name: user-authentication path: /api/v1/auth/** upstream: auth-service - name: user-profile-management path: /api/v1/users/** upstream: user-service # Bad: Generic or missing names routes: - path: /api/v1/auth/** upstream: svc1 - path: /api/v1/users/** upstream: svc2 Order Routes by Specificity routes: # 1. Most specific routes first - name: user-by-id path: /api/users/:id upstream: user-service # 2. Then pattern matches - name: users path: /api/users/** upstream: user-service # 3. Catch-all last - name: default path: /** upstream: default-service Separate Concerns # Separate read and write operations routes: - name: api-read path: /api/** methods: [GET, HEAD, OPTIONS] upstream: api-read-replicas rate_limit: requests_per_second: 1000 - name: api-write path: /api/** methods: [POST, PUT, PATCH, DELETE] upstream: api-primary rate_limit: requests_per_second: 100 Use Health Checks upstreams: - name: critical-service targets: - url: http://backend-1:3000 - url: http://backend-2:3000 health_check: path: /health # Always configure health checks interval: 10s timeout: 2s Security Best Practices Never Commit Secrets # Bad: Secrets in config api_keys: - key: \"pk_live_abc123secret\" name: \"production\" # Better: Use environment variables # config.template.yml api_keys: - key: \"${API_KEY_PRODUCTION}\" name: \"production\" Process with envsubst: envsubst < config.template.yml > config.yml Use Strong API Keys # Generate secure keys openssl rand -base64 32 # Or use a UUID uuidgen Limit Burst Capacity rate_limit: enabled: true default_rps: 100 default_burst: 200 # 2x RPS is reasonable # Avoid: default_burst: 10000 (too high) Protect Sensitive Endpoints routes: # Authentication - very strict - name: login path: /api/auth/login upstream: auth-service rate_limit: enabled: true requests_per_second: 5 burst_size: 10 # Admin endpoints - require API key and strict limits - name: admin path: /admin/** upstream: admin-service rate_limit: enabled: true requests_per_second: 10 burst_size: 20 Set Appropriate Timeouts server: read_timeout: 30s # Prevent slow loris attacks write_timeout: 30s # Limit response time shutdown_timeout: 10s # Graceful shutdown routes: - name: quick-api path: /api/fast/** timeout: 5s # Fast endpoints should be fast - name: reports path: /api/reports/** timeout: 120s # Allow time for heavy operations High Availability Multiple Backend Instances upstreams: - name: api targets: - url: http://api-1:3000 - url: http://api-2:3000 - url: http://api-3:3000 # At least 3 for HA load_balance: round_robin health_check: path: /health interval: 5s timeout: 2s Use Appropriate Load Balancing Scenario Strategy Identical servers, consistent requests round_robin Variable request times least_conn Different server capacities weighted_round_robin Quick Health Checks health_check: path: /health interval: 5s # Frequent checks for quick detection timeout: 2s # Short timeout Multiple Relaypoint Instances For true HA, run multiple Relaypoint instances behind a load balancer: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Relaypoint \u2502 \u2502 Relaypoint \u2502 \u2502 Relaypoint \u2502 \u2502 Node 1 \u2502 \u2502 Node 2 \u2502 \u2502 Node 3 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Performance Optimization Tune Burst for Your Traffic Pattern # Bursty traffic (mobile apps, batch jobs) rate_limit: default_rps: 100 default_burst: 1000 # 10x RPS for bursts # Steady traffic (web apps, APIs) rate_limit: default_rps: 100 default_burst: 200 # 2x RPS Use Least Connections for Variable Workloads upstreams: - name: processing-service targets: - url: http://processor-1:3000 - url: http://processor-2:3000 load_balance: least_conn # Better for variable request times Optimize Health Check Intervals # Critical, fast services health_check: interval: 5s timeout: 1s # Less critical, slower services health_check: interval: 30s timeout: 5s Right-Size Timeouts # Match timeout to expected response time routes: - name: fast-api path: /api/lookup/** timeout: 2s # Fast lookups - name: standard-api path: /api/** timeout: 30s # Standard operations - name: reports path: /api/reports/** timeout: 300s # Long-running reports Monitoring Best Practices Set Up Dashboards Essential panels: Request rate (req/sec) Error rate (%) Latency percentiles (p50, p95, p99) Backend health status Rate limit hits Configure Alerts # Prometheus alerting rules groups: - name: relaypoint rules: # High error rate - alert: HighErrorRate expr: | sum(rate(gateway_errors_total[5m])) / sum(rate(gateway_requests_total[5m])) > 0.01 for: 5m labels: severity: warning # Backend down - alert: BackendDown expr: gateway_upstream_healthy == 0 for: 1m labels: severity: critical # High latency - alert: HighLatency expr: | histogram_quantile(0.95, rate(gateway_request_duration_seconds_bucket[5m]) ) > 1 for: 5m labels: severity: warning Log Aggregation Forward logs to a central system: # Send to file ./relaypoint -config config.yml 2>&1 >> /var/log/relaypoint/relaypoint.log # With logrotate /var/log/relaypoint/*.log { daily rotate 7 compress delaycompress missingok notifempty } Deployment Best Practices Use systemd for Linux [Unit] Description=Relaypoint API Gateway After=network.target [Service] Type=simple User=relaypoint ExecStart=/usr/local/bin/relaypoint -config /etc/relaypoint/config.yml Restart=always RestartSec=5 # Security NoNewPrivileges=true ProtectSystem=strict ProtectHome=true PrivateTmp=true # Resource limits LimitNOFILE=65536 [Install] WantedBy=multi-user.target Graceful Shutdown Configure appropriate shutdown timeout: server: shutdown_timeout: 30s # Allow in-flight requests to complete Configuration Validation Validate before deploying: # Test configuration ./relaypoint -config new-config.yml -validate # Or start briefly timeout 5 ./relaypoint -config new-config.yml Rolling Deployments For zero-downtime updates: Deploy new Relaypoint instance Wait for health check pass Add to load balancer Remove old instance from load balancer Shut down old instance Operational Runbooks Runbook: High Error Rate Check error metrics: curl localhost:9090/metrics | grep errors Identify error type (upstream_not_found, no_healthy_upstream, etc.) Check backend health: curl localhost:9090/metrics | grep upstream_healthy Test backend directly: curl http://backend:3000/health Review recent configuration changes Check backend logs Runbook: High Latency Check latency metrics: curl localhost:9090/metrics | grep duration Compare with backend directly: time curl http://backend:3000/api/test Check in-flight requests: curl localhost:9090/metrics | grep in_flight Review load balancing strategy Check backend resource usage Runbook: Backend Unhealthy Check which backends are unhealthy: curl localhost:9090/metrics | grep upstream_healthy Test health endpoint: curl http://backend:3000/health Check network connectivity: nc -zv backend 3000 Review backend logs Verify health check configuration Capacity Planning Estimate Requirements Metric Consideration Requests/second Each request uses minimal CPU Concurrent connections Memory scales with connections Unique rate limit keys Memory for each unique IP/key Recommended Starting Resources Traffic CPU Memory < 1K req/s 1 core 128 MB 1K-10K req/s 2 cores 256 MB 10K-100K req/s 4 cores 512 MB > 100K req/s 8+ cores 1+ GB Monitor Growth Track over time: Request rate growth Unique IP count (rate limit buckets) Memory usage CPU usage Checklist: Production Readiness Before going to production: [ ] Configuration [ ] Descriptive route and upstream names [ ] Routes ordered by specificity [ ] Health checks configured for all upstreams [ ] Appropriate timeouts set [ ] Security [ ] No secrets in config files [ ] Rate limiting enabled [ ] Sensitive endpoints protected [ ] API keys use secure values [ ] High Availability [ ] Multiple backend instances [ ] Health checks with quick detection [ ] Multiple Relaypoint instances (if critical) [ ] Monitoring [ ] Prometheus scraping metrics [ ] Dashboards created [ ] Alerts configured [ ] Log aggregation set up [ ] Operations [ ] systemd service configured [ ] Graceful shutdown timeout set [ ] Runbooks documented [ ] Backup configuration stored [ ] Testing [ ] Load tested at expected traffic [ ] Failure scenarios tested [ ] Configuration validated Next Steps Examples - Production-ready configurations Troubleshooting - Debug issues Metrics - Set up monitoring","title":"Best practises"},{"location":"best-practises/#configuration-best-practices","text":"","title":"Configuration Best Practices"},{"location":"best-practises/#use-descriptive-names","text":"# Good: Descriptive names routes: - name: user-authentication path: /api/v1/auth/** upstream: auth-service - name: user-profile-management path: /api/v1/users/** upstream: user-service # Bad: Generic or missing names routes: - path: /api/v1/auth/** upstream: svc1 - path: /api/v1/users/** upstream: svc2","title":"Use Descriptive Names"},{"location":"best-practises/#order-routes-by-specificity","text":"routes: # 1. Most specific routes first - name: user-by-id path: /api/users/:id upstream: user-service # 2. Then pattern matches - name: users path: /api/users/** upstream: user-service # 3. Catch-all last - name: default path: /** upstream: default-service","title":"Order Routes by Specificity"},{"location":"best-practises/#separate-concerns","text":"# Separate read and write operations routes: - name: api-read path: /api/** methods: [GET, HEAD, OPTIONS] upstream: api-read-replicas rate_limit: requests_per_second: 1000 - name: api-write path: /api/** methods: [POST, PUT, PATCH, DELETE] upstream: api-primary rate_limit: requests_per_second: 100","title":"Separate Concerns"},{"location":"best-practises/#use-health-checks","text":"upstreams: - name: critical-service targets: - url: http://backend-1:3000 - url: http://backend-2:3000 health_check: path: /health # Always configure health checks interval: 10s timeout: 2s","title":"Use Health Checks"},{"location":"best-practises/#security-best-practices","text":"","title":"Security Best Practices"},{"location":"best-practises/#never-commit-secrets","text":"# Bad: Secrets in config api_keys: - key: \"pk_live_abc123secret\" name: \"production\" # Better: Use environment variables # config.template.yml api_keys: - key: \"${API_KEY_PRODUCTION}\" name: \"production\" Process with envsubst: envsubst < config.template.yml > config.yml","title":"Never Commit Secrets"},{"location":"best-practises/#use-strong-api-keys","text":"# Generate secure keys openssl rand -base64 32 # Or use a UUID uuidgen","title":"Use Strong API Keys"},{"location":"best-practises/#limit-burst-capacity","text":"rate_limit: enabled: true default_rps: 100 default_burst: 200 # 2x RPS is reasonable # Avoid: default_burst: 10000 (too high)","title":"Limit Burst Capacity"},{"location":"best-practises/#protect-sensitive-endpoints","text":"routes: # Authentication - very strict - name: login path: /api/auth/login upstream: auth-service rate_limit: enabled: true requests_per_second: 5 burst_size: 10 # Admin endpoints - require API key and strict limits - name: admin path: /admin/** upstream: admin-service rate_limit: enabled: true requests_per_second: 10 burst_size: 20","title":"Protect Sensitive Endpoints"},{"location":"best-practises/#set-appropriate-timeouts","text":"server: read_timeout: 30s # Prevent slow loris attacks write_timeout: 30s # Limit response time shutdown_timeout: 10s # Graceful shutdown routes: - name: quick-api path: /api/fast/** timeout: 5s # Fast endpoints should be fast - name: reports path: /api/reports/** timeout: 120s # Allow time for heavy operations","title":"Set Appropriate Timeouts"},{"location":"best-practises/#high-availability","text":"","title":"High Availability"},{"location":"best-practises/#multiple-backend-instances","text":"upstreams: - name: api targets: - url: http://api-1:3000 - url: http://api-2:3000 - url: http://api-3:3000 # At least 3 for HA load_balance: round_robin health_check: path: /health interval: 5s timeout: 2s","title":"Multiple Backend Instances"},{"location":"best-practises/#use-appropriate-load-balancing","text":"Scenario Strategy Identical servers, consistent requests round_robin Variable request times least_conn Different server capacities weighted_round_robin","title":"Use Appropriate Load Balancing"},{"location":"best-practises/#quick-health-checks","text":"health_check: path: /health interval: 5s # Frequent checks for quick detection timeout: 2s # Short timeout","title":"Quick Health Checks"},{"location":"best-practises/#multiple-relaypoint-instances","text":"For true HA, run multiple Relaypoint instances behind a load balancer: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Relaypoint \u2502 \u2502 Relaypoint \u2502 \u2502 Relaypoint \u2502 \u2502 Node 1 \u2502 \u2502 Node 2 \u2502 \u2502 Node 3 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Multiple Relaypoint Instances"},{"location":"best-practises/#performance-optimization","text":"","title":"Performance Optimization"},{"location":"best-practises/#tune-burst-for-your-traffic-pattern","text":"# Bursty traffic (mobile apps, batch jobs) rate_limit: default_rps: 100 default_burst: 1000 # 10x RPS for bursts # Steady traffic (web apps, APIs) rate_limit: default_rps: 100 default_burst: 200 # 2x RPS","title":"Tune Burst for Your Traffic Pattern"},{"location":"best-practises/#use-least-connections-for-variable-workloads","text":"upstreams: - name: processing-service targets: - url: http://processor-1:3000 - url: http://processor-2:3000 load_balance: least_conn # Better for variable request times","title":"Use Least Connections for Variable Workloads"},{"location":"best-practises/#optimize-health-check-intervals","text":"# Critical, fast services health_check: interval: 5s timeout: 1s # Less critical, slower services health_check: interval: 30s timeout: 5s","title":"Optimize Health Check Intervals"},{"location":"best-practises/#right-size-timeouts","text":"# Match timeout to expected response time routes: - name: fast-api path: /api/lookup/** timeout: 2s # Fast lookups - name: standard-api path: /api/** timeout: 30s # Standard operations - name: reports path: /api/reports/** timeout: 300s # Long-running reports","title":"Right-Size Timeouts"},{"location":"best-practises/#monitoring-best-practices","text":"","title":"Monitoring Best Practices"},{"location":"best-practises/#set-up-dashboards","text":"Essential panels: Request rate (req/sec) Error rate (%) Latency percentiles (p50, p95, p99) Backend health status Rate limit hits","title":"Set Up Dashboards"},{"location":"best-practises/#configure-alerts","text":"# Prometheus alerting rules groups: - name: relaypoint rules: # High error rate - alert: HighErrorRate expr: | sum(rate(gateway_errors_total[5m])) / sum(rate(gateway_requests_total[5m])) > 0.01 for: 5m labels: severity: warning # Backend down - alert: BackendDown expr: gateway_upstream_healthy == 0 for: 1m labels: severity: critical # High latency - alert: HighLatency expr: | histogram_quantile(0.95, rate(gateway_request_duration_seconds_bucket[5m]) ) > 1 for: 5m labels: severity: warning","title":"Configure Alerts"},{"location":"best-practises/#log-aggregation","text":"Forward logs to a central system: # Send to file ./relaypoint -config config.yml 2>&1 >> /var/log/relaypoint/relaypoint.log # With logrotate /var/log/relaypoint/*.log { daily rotate 7 compress delaycompress missingok notifempty }","title":"Log Aggregation"},{"location":"best-practises/#deployment-best-practices","text":"","title":"Deployment Best Practices"},{"location":"best-practises/#use-systemd-for-linux","text":"[Unit] Description=Relaypoint API Gateway After=network.target [Service] Type=simple User=relaypoint ExecStart=/usr/local/bin/relaypoint -config /etc/relaypoint/config.yml Restart=always RestartSec=5 # Security NoNewPrivileges=true ProtectSystem=strict ProtectHome=true PrivateTmp=true # Resource limits LimitNOFILE=65536 [Install] WantedBy=multi-user.target","title":"Use systemd for Linux"},{"location":"best-practises/#graceful-shutdown","text":"Configure appropriate shutdown timeout: server: shutdown_timeout: 30s # Allow in-flight requests to complete","title":"Graceful Shutdown"},{"location":"best-practises/#configuration-validation","text":"Validate before deploying: # Test configuration ./relaypoint -config new-config.yml -validate # Or start briefly timeout 5 ./relaypoint -config new-config.yml","title":"Configuration Validation"},{"location":"best-practises/#rolling-deployments","text":"For zero-downtime updates: Deploy new Relaypoint instance Wait for health check pass Add to load balancer Remove old instance from load balancer Shut down old instance","title":"Rolling Deployments"},{"location":"best-practises/#operational-runbooks","text":"","title":"Operational Runbooks"},{"location":"best-practises/#runbook-high-error-rate","text":"Check error metrics: curl localhost:9090/metrics | grep errors Identify error type (upstream_not_found, no_healthy_upstream, etc.) Check backend health: curl localhost:9090/metrics | grep upstream_healthy Test backend directly: curl http://backend:3000/health Review recent configuration changes Check backend logs","title":"Runbook: High Error Rate"},{"location":"best-practises/#runbook-high-latency","text":"Check latency metrics: curl localhost:9090/metrics | grep duration Compare with backend directly: time curl http://backend:3000/api/test Check in-flight requests: curl localhost:9090/metrics | grep in_flight Review load balancing strategy Check backend resource usage","title":"Runbook: High Latency"},{"location":"best-practises/#runbook-backend-unhealthy","text":"Check which backends are unhealthy: curl localhost:9090/metrics | grep upstream_healthy Test health endpoint: curl http://backend:3000/health Check network connectivity: nc -zv backend 3000 Review backend logs Verify health check configuration","title":"Runbook: Backend Unhealthy"},{"location":"best-practises/#capacity-planning","text":"","title":"Capacity Planning"},{"location":"best-practises/#estimate-requirements","text":"Metric Consideration Requests/second Each request uses minimal CPU Concurrent connections Memory scales with connections Unique rate limit keys Memory for each unique IP/key","title":"Estimate Requirements"},{"location":"best-practises/#recommended-starting-resources","text":"Traffic CPU Memory < 1K req/s 1 core 128 MB 1K-10K req/s 2 cores 256 MB 10K-100K req/s 4 cores 512 MB > 100K req/s 8+ cores 1+ GB","title":"Recommended Starting Resources"},{"location":"best-practises/#monitor-growth","text":"Track over time: Request rate growth Unique IP count (rate limit buckets) Memory usage CPU usage","title":"Monitor Growth"},{"location":"best-practises/#checklist-production-readiness","text":"Before going to production: [ ] Configuration [ ] Descriptive route and upstream names [ ] Routes ordered by specificity [ ] Health checks configured for all upstreams [ ] Appropriate timeouts set [ ] Security [ ] No secrets in config files [ ] Rate limiting enabled [ ] Sensitive endpoints protected [ ] API keys use secure values [ ] High Availability [ ] Multiple backend instances [ ] Health checks with quick detection [ ] Multiple Relaypoint instances (if critical) [ ] Monitoring [ ] Prometheus scraping metrics [ ] Dashboards created [ ] Alerts configured [ ] Log aggregation set up [ ] Operations [ ] systemd service configured [ ] Graceful shutdown timeout set [ ] Runbooks documented [ ] Backup configuration stored [ ] Testing [ ] Load tested at expected traffic [ ] Failure scenarios tested [ ] Configuration validated","title":"Checklist: Production Readiness"},{"location":"best-practises/#next-steps","text":"Examples - Production-ready configurations Troubleshooting - Debug issues Metrics - Set up monitoring","title":"Next Steps"},{"location":"configuration/","text":"This document provides a complete reference for all Relaypoint configuration options. Configuration File Relaypoint uses YAML for configuration. By default, it looks for relaypoint.yml in the current directory. # Use default configuration file ./relaypoint # Specify configuration file ./relaypoint -config /path/to/config.yml Complete Configuration Example # ============================================================================= # SERVER CONFIGURATION # ============================================================================= server: port: 8080 # Port to listen on (default: 8080) host: \"0.0.0.0\" # Host to bind to (default: 0.0.0.0) read_timeout: 30s # Maximum duration for reading request (default: 30s) write_timeout: 30s # Maximum duration for writing response (default: 30s) shutdown_timeout: 10s # Graceful shutdown timeout (default: 10s) # ============================================================================= # METRICS CONFIGURATION # ============================================================================= metrics: enabled: true # Enable metrics endpoint (default: true) port: 9090 # Metrics server port (default: 9090) path: \"/metrics\" # Metrics endpoint path (default: /metrics) latency_buckets: # Histogram buckets for latency metrics (optional) - 0.005 - 0.01 - 0.025 - 0.05 - 0.1 - 0.25 - 0.5 - 1.0 - 2.5 - 5.0 - 10.0 # ============================================================================= # RATE LIMITING CONFIGURATION # ============================================================================= rate_limit: enabled: true # Enable rate limiting (default: true) default_rps: 100 # Default requests per second (default: 100) default_burst: 200 # Default burst size (default: 200) per_ip: true # Enable per-IP rate limiting (default: true) per_api_key: true # Enable per-API-key rate limiting (default: true) cleanup_interval: 5m # Interval to clean up stale limiters (default: 5m) # ============================================================================= # UPSTREAMS (Backend Services) # ============================================================================= upstreams: - name: user-service # Unique identifier for this upstream (required) targets: # List of backend servers (required, at least one) - url: http://localhost:3001 # Backend URL (required) weight: 2 # Weight for weighted load balancing (default: 1) - url: http://localhost:3002 weight: 1 load_balance: round_robin # Load balancing strategy (default: round_robin) # Options: round_robin, least_conn, random, weighted_round_robin health_check: # Health check configuration (optional) path: /health # Health check endpoint path (required if health_check defined) interval: 10s # Check interval (default: 10s) timeout: 2s # Request timeout (default: 2s) - name: order-service targets: - url: http://localhost:3003 load_balance: least_conn health_check: path: /healthz interval: 5s timeout: 1s # ============================================================================= # ROUTES # ============================================================================= routes: - name: users-api # Route name for identification (optional but recommended) host: api.example.com # Host matching (optional, empty matches all hosts) path: /api/v1/users # URL path pattern (required) methods: # Allowed HTTP methods (optional, empty allows all) - GET - POST upstream: user-service # Target upstream name (required) strip_path: false # Remove matched path prefix before proxying (default: false) headers: # Headers to add to upstream request (optional) X-Custom-Header: \"value\" X-Service-Name: \"users\" rate_limit: # Route-specific rate limit (optional) enabled: true requests_per_second: 50 burst_size: 100 timeout: 30s # Request timeout for this route (optional) retry_count: 3 # Number of retries on failure (optional) - name: users-detail path: /api/v1/users/:id # Path with parameter upstream: user-service - name: orders-api path: /api/v1/orders/** # Wildcard matching upstream: order-service strip_path: true # ============================================================================= # API KEYS # ============================================================================= api_keys: - key: \"pk_live_abc123def456\" # The API key value (required) name: \"production-web-app\" # Human-readable name (required) requests_per_second: 1000 # Rate limit for this key (required) burst_size: 2000 # Burst size for this key (optional, defaults to rps * 2) enabled: true # Whether key is active (default: true) - key: \"pk_test_xyz789\" name: \"development-app\" requests_per_second: 100 burst_size: 200 enabled: true Configuration Sections Server Field Type Default Description port integer 8080 Port number for the gateway to listen on host string \"0.0.0.0\" Host/IP address to bind to read_timeout duration 30s Maximum time to read the entire request write_timeout duration 30s Maximum time to write the response shutdown_timeout duration 10s Time to wait for active connections during shutdown Metrics Field Type Default Description enabled boolean true Enable Prometheus metrics endpoint port integer 9090 Port for the metrics server path string \"/metrics\" Path for the metrics endpoint latency_buckets []float64 See below Histogram bucket boundaries in seconds Default latency buckets: [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0] Rate Limit Field Type Default Description enabled boolean true Enable rate limiting globally default_rps integer 100 Default requests per second default_burst integer 200 Default burst size (token bucket capacity) per_ip boolean true Enable rate limiting per client IP per_api_key boolean true Enable rate limiting per API key cleanup_interval duration 5m How often to clean up inactive rate limiters Upstreams Field Type Required Description name string Yes Unique identifier for the upstream targets []Target Yes List of backend server targets load_balance string No Load balancing strategy (default: round_robin ) health_check HealthCheck No Health check configuration Target Field Type Required Description url string Yes Backend server URL (e.g., http://localhost:3000 ) weight integer No Weight for weighted load balancing (default: 1) HealthCheck Field Type Required Description path string Yes Health check endpoint path (e.g., /health ) interval duration No Time between health checks (default: 10s ) timeout duration No Health check request timeout (default: 2s ) Routes Field Type Required Description name string No Human-readable route name (recommended) host string No Host to match (empty matches all hosts) path string Yes URL path pattern to match methods []string No HTTP methods to match (empty allows all) upstream string Yes Name of the upstream to route to strip_path boolean No Remove matched prefix from path (default: false ) headers map No Headers to add to upstream requests rate_limit RouteRateLimit No Route-specific rate limiting timeout duration No Request timeout for this route retry_count integer No Number of retry attempts on failure RouteRateLimit Field Type Required Description enabled boolean No Enable rate limiting for this route (default: false ) requests_per_second integer Yes Maximum requests per second burst_size integer No Burst capacity (default: requests_per_second * 2 ) API Keys Field Type Required Description key string Yes The API key value (keep secret!) name string Yes Human-readable identifier requests_per_second integer Yes Rate limit for this key burst_size integer No Burst capacity (default: requests_per_second * 2 ) enabled boolean No Whether key is active (default: true ) Path Pattern Syntax Relaypoint supports several path matching patterns: Pattern Description Example Match /exact Exact match /exact only /prefix/* Single segment wildcard /prefix/anything /prefix/** Multi-segment wildcard /prefix/any/path/here /users/:id Named parameter /users/123 (id = \"123\") /users/{id} Named parameter (alt syntax) /users/456 (id = \"456\") Path Matching Priority Routes are matched in priority order: Exact matches (highest priority) Specific path segments Named parameters Single wildcards ( * ) Multi-segment wildcards ( ** ) (lowest priority) Duration Format Durations can be specified as: 100ms - 100 milliseconds 5s - 5 seconds 2m - 2 minutes 1h - 1 hour 1h30m - 1 hour and 30 minutes Environment Variables While Relaypoint primarily uses YAML configuration, you can use environment variables in your configuration by processing the file with envsubst : server: port: ${RELAYPOINT_PORT:-8080} upstreams: - name: backend targets: - url: ${BACKEND_URL} export BACKEND_URL=http://backend:3000 envsubst < relaypoint.yml.template > relaypoint.yml ./relaypoint -config relaypoint.yml Configuration Validation Relaypoint validates configuration on startup: Server port must be between 1 and 65535 At least one route must be defined Each upstream must have a unique name Each upstream must have at least one target Each route must reference an existing upstream Upstream target URLs must be valid If validation fails, Relaypoint will exit with an error message indicating the problem. Next Steps Routing - Advanced routing patterns Load Balancing - Load balancing strategies Rate Limiting - Rate limiting configuration","title":"Configuration"},{"location":"configuration/#configuration-file","text":"Relaypoint uses YAML for configuration. By default, it looks for relaypoint.yml in the current directory. # Use default configuration file ./relaypoint # Specify configuration file ./relaypoint -config /path/to/config.yml","title":"Configuration File"},{"location":"configuration/#complete-configuration-example","text":"# ============================================================================= # SERVER CONFIGURATION # ============================================================================= server: port: 8080 # Port to listen on (default: 8080) host: \"0.0.0.0\" # Host to bind to (default: 0.0.0.0) read_timeout: 30s # Maximum duration for reading request (default: 30s) write_timeout: 30s # Maximum duration for writing response (default: 30s) shutdown_timeout: 10s # Graceful shutdown timeout (default: 10s) # ============================================================================= # METRICS CONFIGURATION # ============================================================================= metrics: enabled: true # Enable metrics endpoint (default: true) port: 9090 # Metrics server port (default: 9090) path: \"/metrics\" # Metrics endpoint path (default: /metrics) latency_buckets: # Histogram buckets for latency metrics (optional) - 0.005 - 0.01 - 0.025 - 0.05 - 0.1 - 0.25 - 0.5 - 1.0 - 2.5 - 5.0 - 10.0 # ============================================================================= # RATE LIMITING CONFIGURATION # ============================================================================= rate_limit: enabled: true # Enable rate limiting (default: true) default_rps: 100 # Default requests per second (default: 100) default_burst: 200 # Default burst size (default: 200) per_ip: true # Enable per-IP rate limiting (default: true) per_api_key: true # Enable per-API-key rate limiting (default: true) cleanup_interval: 5m # Interval to clean up stale limiters (default: 5m) # ============================================================================= # UPSTREAMS (Backend Services) # ============================================================================= upstreams: - name: user-service # Unique identifier for this upstream (required) targets: # List of backend servers (required, at least one) - url: http://localhost:3001 # Backend URL (required) weight: 2 # Weight for weighted load balancing (default: 1) - url: http://localhost:3002 weight: 1 load_balance: round_robin # Load balancing strategy (default: round_robin) # Options: round_robin, least_conn, random, weighted_round_robin health_check: # Health check configuration (optional) path: /health # Health check endpoint path (required if health_check defined) interval: 10s # Check interval (default: 10s) timeout: 2s # Request timeout (default: 2s) - name: order-service targets: - url: http://localhost:3003 load_balance: least_conn health_check: path: /healthz interval: 5s timeout: 1s # ============================================================================= # ROUTES # ============================================================================= routes: - name: users-api # Route name for identification (optional but recommended) host: api.example.com # Host matching (optional, empty matches all hosts) path: /api/v1/users # URL path pattern (required) methods: # Allowed HTTP methods (optional, empty allows all) - GET - POST upstream: user-service # Target upstream name (required) strip_path: false # Remove matched path prefix before proxying (default: false) headers: # Headers to add to upstream request (optional) X-Custom-Header: \"value\" X-Service-Name: \"users\" rate_limit: # Route-specific rate limit (optional) enabled: true requests_per_second: 50 burst_size: 100 timeout: 30s # Request timeout for this route (optional) retry_count: 3 # Number of retries on failure (optional) - name: users-detail path: /api/v1/users/:id # Path with parameter upstream: user-service - name: orders-api path: /api/v1/orders/** # Wildcard matching upstream: order-service strip_path: true # ============================================================================= # API KEYS # ============================================================================= api_keys: - key: \"pk_live_abc123def456\" # The API key value (required) name: \"production-web-app\" # Human-readable name (required) requests_per_second: 1000 # Rate limit for this key (required) burst_size: 2000 # Burst size for this key (optional, defaults to rps * 2) enabled: true # Whether key is active (default: true) - key: \"pk_test_xyz789\" name: \"development-app\" requests_per_second: 100 burst_size: 200 enabled: true","title":"Complete Configuration Example"},{"location":"configuration/#configuration-sections","text":"","title":"Configuration Sections"},{"location":"configuration/#server","text":"Field Type Default Description port integer 8080 Port number for the gateway to listen on host string \"0.0.0.0\" Host/IP address to bind to read_timeout duration 30s Maximum time to read the entire request write_timeout duration 30s Maximum time to write the response shutdown_timeout duration 10s Time to wait for active connections during shutdown","title":"Server"},{"location":"configuration/#metrics","text":"Field Type Default Description enabled boolean true Enable Prometheus metrics endpoint port integer 9090 Port for the metrics server path string \"/metrics\" Path for the metrics endpoint latency_buckets []float64 See below Histogram bucket boundaries in seconds Default latency buckets: [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]","title":"Metrics"},{"location":"configuration/#rate-limit","text":"Field Type Default Description enabled boolean true Enable rate limiting globally default_rps integer 100 Default requests per second default_burst integer 200 Default burst size (token bucket capacity) per_ip boolean true Enable rate limiting per client IP per_api_key boolean true Enable rate limiting per API key cleanup_interval duration 5m How often to clean up inactive rate limiters","title":"Rate Limit"},{"location":"configuration/#upstreams","text":"Field Type Required Description name string Yes Unique identifier for the upstream targets []Target Yes List of backend server targets load_balance string No Load balancing strategy (default: round_robin ) health_check HealthCheck No Health check configuration","title":"Upstreams"},{"location":"configuration/#target","text":"Field Type Required Description url string Yes Backend server URL (e.g., http://localhost:3000 ) weight integer No Weight for weighted load balancing (default: 1)","title":"Target"},{"location":"configuration/#healthcheck","text":"Field Type Required Description path string Yes Health check endpoint path (e.g., /health ) interval duration No Time between health checks (default: 10s ) timeout duration No Health check request timeout (default: 2s )","title":"HealthCheck"},{"location":"configuration/#routes","text":"Field Type Required Description name string No Human-readable route name (recommended) host string No Host to match (empty matches all hosts) path string Yes URL path pattern to match methods []string No HTTP methods to match (empty allows all) upstream string Yes Name of the upstream to route to strip_path boolean No Remove matched prefix from path (default: false ) headers map No Headers to add to upstream requests rate_limit RouteRateLimit No Route-specific rate limiting timeout duration No Request timeout for this route retry_count integer No Number of retry attempts on failure","title":"Routes"},{"location":"configuration/#routeratelimit","text":"Field Type Required Description enabled boolean No Enable rate limiting for this route (default: false ) requests_per_second integer Yes Maximum requests per second burst_size integer No Burst capacity (default: requests_per_second * 2 )","title":"RouteRateLimit"},{"location":"configuration/#api-keys","text":"Field Type Required Description key string Yes The API key value (keep secret!) name string Yes Human-readable identifier requests_per_second integer Yes Rate limit for this key burst_size integer No Burst capacity (default: requests_per_second * 2 ) enabled boolean No Whether key is active (default: true )","title":"API Keys"},{"location":"configuration/#path-pattern-syntax","text":"Relaypoint supports several path matching patterns: Pattern Description Example Match /exact Exact match /exact only /prefix/* Single segment wildcard /prefix/anything /prefix/** Multi-segment wildcard /prefix/any/path/here /users/:id Named parameter /users/123 (id = \"123\") /users/{id} Named parameter (alt syntax) /users/456 (id = \"456\")","title":"Path Pattern Syntax"},{"location":"configuration/#path-matching-priority","text":"Routes are matched in priority order: Exact matches (highest priority) Specific path segments Named parameters Single wildcards ( * ) Multi-segment wildcards ( ** ) (lowest priority)","title":"Path Matching Priority"},{"location":"configuration/#duration-format","text":"Durations can be specified as: 100ms - 100 milliseconds 5s - 5 seconds 2m - 2 minutes 1h - 1 hour 1h30m - 1 hour and 30 minutes","title":"Duration Format"},{"location":"configuration/#environment-variables","text":"While Relaypoint primarily uses YAML configuration, you can use environment variables in your configuration by processing the file with envsubst : server: port: ${RELAYPOINT_PORT:-8080} upstreams: - name: backend targets: - url: ${BACKEND_URL} export BACKEND_URL=http://backend:3000 envsubst < relaypoint.yml.template > relaypoint.yml ./relaypoint -config relaypoint.yml","title":"Environment Variables"},{"location":"configuration/#configuration-validation","text":"Relaypoint validates configuration on startup: Server port must be between 1 and 65535 At least one route must be defined Each upstream must have a unique name Each upstream must have at least one target Each route must reference an existing upstream Upstream target URLs must be valid If validation fails, Relaypoint will exit with an error message indicating the problem.","title":"Configuration Validation"},{"location":"configuration/#next-steps","text":"Routing - Advanced routing patterns Load Balancing - Load balancing strategies Rate Limiting - Rate limiting configuration","title":"Next Steps"},{"location":"examples/","text":"This page provides complete, ready-to-use configuration examples for common use cases. Basic API Gateway The simplest possible configuration for proxying requests to a backend: # basic.yml server: port: 8080 upstreams: - name: backend targets: - url: http://localhost:3000 routes: - name: all-traffic path: /** upstream: backend Load-Balanced Microservices Route to multiple microservices with load balancing and health checks: # microservices.yml server: port: 8080 read_timeout: 30s write_timeout: 30s metrics: enabled: true port: 9090 upstreams: - name: user-service targets: - url: http://users-1:3000 - url: http://users-2:3000 - url: http://users-3:3000 load_balance: round_robin health_check: path: /health interval: 10s timeout: 2s - name: order-service targets: - url: http://orders-1:3000 - url: http://orders-2:3000 load_balance: least_conn health_check: path: /health interval: 10s timeout: 2s - name: product-service targets: - url: http://products:3000 health_check: path: /health interval: 10s timeout: 2s routes: - name: users path: /api/users/** upstream: user-service strip_path: false - name: orders path: /api/orders/** upstream: order-service strip_path: false - name: products path: /api/products/** upstream: product-service strip_path: false Rate-Limited Public API Public API with tiered rate limiting: # public-api.yml server: port: 8080 metrics: enabled: true port: 9090 rate_limit: enabled: true default_rps: 60 # Anonymous users: 60 req/sec default_burst: 120 per_ip: true per_api_key: true cleanup_interval: 5m upstreams: - name: api targets: - url: http://api-1:3000 - url: http://api-2:3000 load_balance: round_robin health_check: path: /health interval: 10s routes: # Public read endpoints - higher limits - name: public-read path: /api/v1/** methods: [GET, HEAD] upstream: api rate_limit: enabled: true requests_per_second: 100 burst_size: 200 # Write endpoints - stricter limits - name: public-write path: /api/v1/** methods: [POST, PUT, PATCH, DELETE] upstream: api rate_limit: enabled: true requests_per_second: 20 burst_size: 40 # Authentication - very strict (prevent brute force) - name: auth path: /api/v1/auth/** upstream: api rate_limit: enabled: true requests_per_second: 5 burst_size: 10 api_keys: # Free tier - key: \"pk_free_sample123\" name: \"free-tier\" requests_per_second: 60 burst_size: 120 enabled: true # Pro tier - key: \"pk_pro_sample456\" name: \"pro-tier\" requests_per_second: 600 burst_size: 1200 enabled: true # Enterprise tier - key: \"pk_ent_sample789\" name: \"enterprise-tier\" requests_per_second: 6000 burst_size: 12000 enabled: true Multi-Tenant SaaS Route based on subdomain for multi-tenant applications: # multi-tenant.yml server: port: 8080 upstreams: - name: tenant-service targets: - url: http://tenant-app:3000 load_balance: round_robin health_check: path: /health interval: 10s - name: main-app targets: - url: http://main-app:3000 load_balance: round_robin health_check: path: /health interval: 10s - name: admin-service targets: - url: http://admin:3000 health_check: path: /health interval: 10s routes: # Admin panel - name: admin host: admin.example.com path: /** upstream: admin-service rate_limit: enabled: true requests_per_second: 100 burst_size: 200 # API for all tenants - name: tenant-api host: \"*.example.com\" path: /api/** upstream: tenant-service headers: X-Tenant-Source: \"subdomain\" # Main marketing site - name: main host: www.example.com path: /** upstream: main-app # Catch-all - name: default path: /** upstream: main-app Canary Deployment Gradually roll out a new version: # canary.yml server: port: 8080 upstreams: # Stable version (90% traffic) - name: api-stable targets: - url: http://api-v1-1:3000 - url: http://api-v1-2:3000 - url: http://api-v1-3:3000 load_balance: round_robin health_check: path: /health interval: 5s # Canary version (10% traffic) - name: api-canary targets: - url: http://api-v2:3000 health_check: path: /health interval: 5s # Combined weighted upstream - name: api-weighted targets: - url: http://api-v1-1:3000 weight: 3 - url: http://api-v1-2:3000 weight: 3 - url: http://api-v1-3:3000 weight: 3 - url: http://api-v2:3000 weight: 1 # 10% of traffic load_balance: weighted_round_robin health_check: path: /health interval: 5s routes: - name: api path: /api/** upstream: api-weighted Read/Write Splitting Route reads to replicas, writes to primary: # read-write-split.yml server: port: 8080 upstreams: # Write to primary - name: primary targets: - url: http://db-primary:3000 health_check: path: /health interval: 5s # Read from replicas - name: replicas targets: - url: http://db-replica-1:3000 - url: http://db-replica-2:3000 - url: http://db-replica-3:3000 load_balance: least_conn health_check: path: /health interval: 5s routes: # Read operations go to replicas - name: reads path: /api/** methods: [GET, HEAD, OPTIONS] upstream: replicas # Write operations go to primary - name: writes path: /api/** methods: [POST, PUT, PATCH, DELETE] upstream: primary API Versioning Support multiple API versions: # versioned-api.yml server: port: 8080 upstreams: - name: api-v1 targets: - url: http://api-v1:3000 health_check: path: /health interval: 10s - name: api-v2 targets: - url: http://api-v2:3000 health_check: path: /health interval: 10s - name: api-v3 targets: - url: http://api-v3:3000 health_check: path: /health interval: 10s routes: # Version 3 (latest) - name: v3 path: /api/v3/** upstream: api-v3 strip_path: false # Version 2 - name: v2 path: /api/v2/** upstream: api-v2 strip_path: false # Version 1 (deprecated, rate limited) - name: v1 path: /api/v1/** upstream: api-v1 strip_path: false rate_limit: enabled: true requests_per_second: 10 burst_size: 20 headers: X-API-Deprecated: \"true\" X-API-Sunset-Date: \"2025-12-31\" # Default to latest - name: default path: /api/** upstream: api-v3 Kubernetes Ingress Replacement Replace Kubernetes Ingress with Relaypoint: # k8s-gateway.yml server: port: 8080 metrics: enabled: true port: 9090 upstreams: - name: frontend targets: - url: http://frontend-service.default.svc.cluster.local:80 health_check: path: /health interval: 10s - name: backend-api targets: - url: http://api-service.default.svc.cluster.local:80 load_balance: round_robin health_check: path: /healthz interval: 10s - name: websocket targets: - url: http://ws-service.default.svc.cluster.local:80 health_check: path: /health interval: 10s routes: # API routes - name: api path: /api/** upstream: backend-api timeout: 30s # WebSocket routes - name: websocket path: /ws/** upstream: websocket timeout: 3600s # Long timeout for websockets # Frontend (catch-all) - name: frontend path: /** upstream: frontend Docker Compose Full Stack Complete example with mock backends: # docker-compose.yml version: \"3.8\" services: relaypoint: image: ghcr.io/relaypoint/relaypoint:latest ports: - \"8080:8080\" - \"9090:9090\" volumes: - ./relaypoint.yml:/etc/relaypoint/relaypoint.yml:ro command: [\"-config\", \"/etc/relaypoint/relaypoint.yml\"] depends_on: - users - orders - products users: image: nginx:alpine volumes: - ./mock/users:/usr/share/nginx/html:ro orders: image: nginx:alpine volumes: - ./mock/orders:/usr/share/nginx/html:ro products: image: nginx:alpine volumes: - ./mock/products:/usr/share/nginx/html:ro prometheus: image: prom/prometheus:latest ports: - \"9091:9090\" volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro grafana: image: grafana/grafana:latest ports: - \"3000:3000\" environment: - GF_SECURITY_ADMIN_PASSWORD=admin # relaypoint.yml server: port: 8080 metrics: enabled: true port: 9090 rate_limit: enabled: true default_rps: 100 default_burst: 200 per_ip: true upstreams: - name: users targets: - url: http://users:80 - name: orders targets: - url: http://orders:80 - name: products targets: - url: http://products:80 routes: - name: users-api path: /api/users/** upstream: users - name: orders-api path: /api/orders/** upstream: orders - name: products-api path: /api/products/** upstream: products # prometheus.yml global: scrape_interval: 15s scrape_configs: - job_name: \"relaypoint\" static_configs: - targets: [\"relaypoint:9090\"] Testing Your Configuration After setting up any configuration: # Start Relaypoint ./relaypoint -config your-config.yml # Test health endpoint curl http://localhost:8080/health # Test routing curl http://localhost:8080/api/users # Check metrics curl http://localhost:9090/metrics # Check stats curl http://localhost:8080/stats Next Steps Configuration Reference - All configuration options Troubleshooting - Debug issues Best Practices - Production recommendations","title":"Examples"},{"location":"examples/#basic-api-gateway","text":"The simplest possible configuration for proxying requests to a backend: # basic.yml server: port: 8080 upstreams: - name: backend targets: - url: http://localhost:3000 routes: - name: all-traffic path: /** upstream: backend","title":"Basic API Gateway"},{"location":"examples/#load-balanced-microservices","text":"Route to multiple microservices with load balancing and health checks: # microservices.yml server: port: 8080 read_timeout: 30s write_timeout: 30s metrics: enabled: true port: 9090 upstreams: - name: user-service targets: - url: http://users-1:3000 - url: http://users-2:3000 - url: http://users-3:3000 load_balance: round_robin health_check: path: /health interval: 10s timeout: 2s - name: order-service targets: - url: http://orders-1:3000 - url: http://orders-2:3000 load_balance: least_conn health_check: path: /health interval: 10s timeout: 2s - name: product-service targets: - url: http://products:3000 health_check: path: /health interval: 10s timeout: 2s routes: - name: users path: /api/users/** upstream: user-service strip_path: false - name: orders path: /api/orders/** upstream: order-service strip_path: false - name: products path: /api/products/** upstream: product-service strip_path: false","title":"Load-Balanced Microservices"},{"location":"examples/#rate-limited-public-api","text":"Public API with tiered rate limiting: # public-api.yml server: port: 8080 metrics: enabled: true port: 9090 rate_limit: enabled: true default_rps: 60 # Anonymous users: 60 req/sec default_burst: 120 per_ip: true per_api_key: true cleanup_interval: 5m upstreams: - name: api targets: - url: http://api-1:3000 - url: http://api-2:3000 load_balance: round_robin health_check: path: /health interval: 10s routes: # Public read endpoints - higher limits - name: public-read path: /api/v1/** methods: [GET, HEAD] upstream: api rate_limit: enabled: true requests_per_second: 100 burst_size: 200 # Write endpoints - stricter limits - name: public-write path: /api/v1/** methods: [POST, PUT, PATCH, DELETE] upstream: api rate_limit: enabled: true requests_per_second: 20 burst_size: 40 # Authentication - very strict (prevent brute force) - name: auth path: /api/v1/auth/** upstream: api rate_limit: enabled: true requests_per_second: 5 burst_size: 10 api_keys: # Free tier - key: \"pk_free_sample123\" name: \"free-tier\" requests_per_second: 60 burst_size: 120 enabled: true # Pro tier - key: \"pk_pro_sample456\" name: \"pro-tier\" requests_per_second: 600 burst_size: 1200 enabled: true # Enterprise tier - key: \"pk_ent_sample789\" name: \"enterprise-tier\" requests_per_second: 6000 burst_size: 12000 enabled: true","title":"Rate-Limited Public API"},{"location":"examples/#multi-tenant-saas","text":"Route based on subdomain for multi-tenant applications: # multi-tenant.yml server: port: 8080 upstreams: - name: tenant-service targets: - url: http://tenant-app:3000 load_balance: round_robin health_check: path: /health interval: 10s - name: main-app targets: - url: http://main-app:3000 load_balance: round_robin health_check: path: /health interval: 10s - name: admin-service targets: - url: http://admin:3000 health_check: path: /health interval: 10s routes: # Admin panel - name: admin host: admin.example.com path: /** upstream: admin-service rate_limit: enabled: true requests_per_second: 100 burst_size: 200 # API for all tenants - name: tenant-api host: \"*.example.com\" path: /api/** upstream: tenant-service headers: X-Tenant-Source: \"subdomain\" # Main marketing site - name: main host: www.example.com path: /** upstream: main-app # Catch-all - name: default path: /** upstream: main-app","title":"Multi-Tenant SaaS"},{"location":"examples/#canary-deployment","text":"Gradually roll out a new version: # canary.yml server: port: 8080 upstreams: # Stable version (90% traffic) - name: api-stable targets: - url: http://api-v1-1:3000 - url: http://api-v1-2:3000 - url: http://api-v1-3:3000 load_balance: round_robin health_check: path: /health interval: 5s # Canary version (10% traffic) - name: api-canary targets: - url: http://api-v2:3000 health_check: path: /health interval: 5s # Combined weighted upstream - name: api-weighted targets: - url: http://api-v1-1:3000 weight: 3 - url: http://api-v1-2:3000 weight: 3 - url: http://api-v1-3:3000 weight: 3 - url: http://api-v2:3000 weight: 1 # 10% of traffic load_balance: weighted_round_robin health_check: path: /health interval: 5s routes: - name: api path: /api/** upstream: api-weighted","title":"Canary Deployment"},{"location":"examples/#readwrite-splitting","text":"Route reads to replicas, writes to primary: # read-write-split.yml server: port: 8080 upstreams: # Write to primary - name: primary targets: - url: http://db-primary:3000 health_check: path: /health interval: 5s # Read from replicas - name: replicas targets: - url: http://db-replica-1:3000 - url: http://db-replica-2:3000 - url: http://db-replica-3:3000 load_balance: least_conn health_check: path: /health interval: 5s routes: # Read operations go to replicas - name: reads path: /api/** methods: [GET, HEAD, OPTIONS] upstream: replicas # Write operations go to primary - name: writes path: /api/** methods: [POST, PUT, PATCH, DELETE] upstream: primary","title":"Read/Write Splitting"},{"location":"examples/#api-versioning","text":"Support multiple API versions: # versioned-api.yml server: port: 8080 upstreams: - name: api-v1 targets: - url: http://api-v1:3000 health_check: path: /health interval: 10s - name: api-v2 targets: - url: http://api-v2:3000 health_check: path: /health interval: 10s - name: api-v3 targets: - url: http://api-v3:3000 health_check: path: /health interval: 10s routes: # Version 3 (latest) - name: v3 path: /api/v3/** upstream: api-v3 strip_path: false # Version 2 - name: v2 path: /api/v2/** upstream: api-v2 strip_path: false # Version 1 (deprecated, rate limited) - name: v1 path: /api/v1/** upstream: api-v1 strip_path: false rate_limit: enabled: true requests_per_second: 10 burst_size: 20 headers: X-API-Deprecated: \"true\" X-API-Sunset-Date: \"2025-12-31\" # Default to latest - name: default path: /api/** upstream: api-v3","title":"API Versioning"},{"location":"examples/#kubernetes-ingress-replacement","text":"Replace Kubernetes Ingress with Relaypoint: # k8s-gateway.yml server: port: 8080 metrics: enabled: true port: 9090 upstreams: - name: frontend targets: - url: http://frontend-service.default.svc.cluster.local:80 health_check: path: /health interval: 10s - name: backend-api targets: - url: http://api-service.default.svc.cluster.local:80 load_balance: round_robin health_check: path: /healthz interval: 10s - name: websocket targets: - url: http://ws-service.default.svc.cluster.local:80 health_check: path: /health interval: 10s routes: # API routes - name: api path: /api/** upstream: backend-api timeout: 30s # WebSocket routes - name: websocket path: /ws/** upstream: websocket timeout: 3600s # Long timeout for websockets # Frontend (catch-all) - name: frontend path: /** upstream: frontend","title":"Kubernetes Ingress Replacement"},{"location":"examples/#docker-compose-full-stack","text":"Complete example with mock backends: # docker-compose.yml version: \"3.8\" services: relaypoint: image: ghcr.io/relaypoint/relaypoint:latest ports: - \"8080:8080\" - \"9090:9090\" volumes: - ./relaypoint.yml:/etc/relaypoint/relaypoint.yml:ro command: [\"-config\", \"/etc/relaypoint/relaypoint.yml\"] depends_on: - users - orders - products users: image: nginx:alpine volumes: - ./mock/users:/usr/share/nginx/html:ro orders: image: nginx:alpine volumes: - ./mock/orders:/usr/share/nginx/html:ro products: image: nginx:alpine volumes: - ./mock/products:/usr/share/nginx/html:ro prometheus: image: prom/prometheus:latest ports: - \"9091:9090\" volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro grafana: image: grafana/grafana:latest ports: - \"3000:3000\" environment: - GF_SECURITY_ADMIN_PASSWORD=admin # relaypoint.yml server: port: 8080 metrics: enabled: true port: 9090 rate_limit: enabled: true default_rps: 100 default_burst: 200 per_ip: true upstreams: - name: users targets: - url: http://users:80 - name: orders targets: - url: http://orders:80 - name: products targets: - url: http://products:80 routes: - name: users-api path: /api/users/** upstream: users - name: orders-api path: /api/orders/** upstream: orders - name: products-api path: /api/products/** upstream: products # prometheus.yml global: scrape_interval: 15s scrape_configs: - job_name: \"relaypoint\" static_configs: - targets: [\"relaypoint:9090\"]","title":"Docker Compose Full Stack"},{"location":"examples/#testing-your-configuration","text":"After setting up any configuration: # Start Relaypoint ./relaypoint -config your-config.yml # Test health endpoint curl http://localhost:8080/health # Test routing curl http://localhost:8080/api/users # Check metrics curl http://localhost:9090/metrics # Check stats curl http://localhost:8080/stats","title":"Testing Your Configuration"},{"location":"examples/#next-steps","text":"Configuration Reference - All configuration options Troubleshooting - Debug issues Best Practices - Production recommendations","title":"Next Steps"},{"location":"getting-started/","text":"This guide will help you get Relaypoint up and running in under 5 minutes. Prerequisites A Linux, macOS, or Windows system One or more backend services to proxy requests to (Optional) Go 1.24+ if building from source Step 1: Download Relaypoint Option A: Download Pre-built Binary Download the latest release for your platform: # Linux (amd64) curl -L https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-linux-amd64.tar.gz | tar xz # Linux (arm64) curl -L https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-linux-arm64.tar.gz | tar xz # macOS (Intel) curl -L https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-darwin-amd64.tar.gz | tar xz # macOS (Apple Silicon) curl -L https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-darwin-arm64.tar.gz | tar xz Option B: Build from Source git clone https://github.com/relaypoint/relaypoint.git cd relaypoint make build The binary will be created at dist/relaypoint . Step 2: Create Configuration Create a file named relaypoint.yml : server: port: 8080 read_timeout: 30s write_timeout: 30s metrics: enabled: true port: 9090 upstreams: - name: my-backend targets: - url: http://localhost:3000 load_balance: round_robin health_check: path: /health interval: 10s timeout: 2s routes: - name: api-routes path: /api/** upstream: my-backend Step 3: Start Relaypoint ./relaypoint -config relaypoint.yml You should see output like: {\"level\":\"INFO\",\"msg\":\"Starting RelayPoint\",\"config\":\"relaypoint.yml\"} {\"level\":\"INFO\",\"msg\":\"configuration loaded\",\"routes\":1,\"upstreams\":1,\"rate_limiting\":true} {\"level\":\"INFO\",\"msg\":\"metrics server starting\",\"port\":9090,\"path\":\"/metrics\"} {\"level\":\"INFO\",\"msg\":\"relaypoint API Gateway starting\",\"address\":\"0.0.0.0:8080\"} Step 4: Test Your Setup # Test the gateway curl http://localhost:8080/api/hello # Check health curl http://localhost:8080/health # View metrics curl http://localhost:9090/metrics # View stats curl http://localhost:8080/stats Step 5: Add More Features Now that you have the basics working, explore additional features: Add Load Balancing upstreams: - name: my-backend targets: - url: http://localhost:3001 weight: 2 - url: http://localhost:3002 weight: 1 - url: http://localhost:3003 weight: 1 load_balance: weighted_round_robin Add Rate Limiting rate_limit: enabled: true default_rps: 100 default_burst: 200 per_ip: true per_api_key: true routes: - name: api-routes path: /api/** upstream: my-backend rate_limit: enabled: true requests_per_second: 50 burst_size: 100 Add API Keys api_keys: - key: \"pk_live_abc123\" name: \"production-app\" requests_per_second: 1000 burst_size: 2000 enabled: true Next Steps Configuration Reference - Complete configuration options Routing - Advanced routing patterns Load Balancing - Load balancing strategies Rate Limiting - Protect your APIs Metrics - Monitoring and observability Getting Help If you run into issues: Check the Troubleshooting Guide Search GitHub Issues Ask in GitHub Discussions","title":"Getting Started"},{"location":"getting-started/#prerequisites","text":"A Linux, macOS, or Windows system One or more backend services to proxy requests to (Optional) Go 1.24+ if building from source","title":"Prerequisites"},{"location":"getting-started/#step-1-download-relaypoint","text":"","title":"Step 1: Download Relaypoint"},{"location":"getting-started/#option-a-download-pre-built-binary","text":"Download the latest release for your platform: # Linux (amd64) curl -L https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-linux-amd64.tar.gz | tar xz # Linux (arm64) curl -L https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-linux-arm64.tar.gz | tar xz # macOS (Intel) curl -L https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-darwin-amd64.tar.gz | tar xz # macOS (Apple Silicon) curl -L https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-darwin-arm64.tar.gz | tar xz","title":"Option A: Download Pre-built Binary"},{"location":"getting-started/#option-b-build-from-source","text":"git clone https://github.com/relaypoint/relaypoint.git cd relaypoint make build The binary will be created at dist/relaypoint .","title":"Option B: Build from Source"},{"location":"getting-started/#step-2-create-configuration","text":"Create a file named relaypoint.yml : server: port: 8080 read_timeout: 30s write_timeout: 30s metrics: enabled: true port: 9090 upstreams: - name: my-backend targets: - url: http://localhost:3000 load_balance: round_robin health_check: path: /health interval: 10s timeout: 2s routes: - name: api-routes path: /api/** upstream: my-backend","title":"Step 2: Create Configuration"},{"location":"getting-started/#step-3-start-relaypoint","text":"./relaypoint -config relaypoint.yml You should see output like: {\"level\":\"INFO\",\"msg\":\"Starting RelayPoint\",\"config\":\"relaypoint.yml\"} {\"level\":\"INFO\",\"msg\":\"configuration loaded\",\"routes\":1,\"upstreams\":1,\"rate_limiting\":true} {\"level\":\"INFO\",\"msg\":\"metrics server starting\",\"port\":9090,\"path\":\"/metrics\"} {\"level\":\"INFO\",\"msg\":\"relaypoint API Gateway starting\",\"address\":\"0.0.0.0:8080\"}","title":"Step 3: Start Relaypoint"},{"location":"getting-started/#step-4-test-your-setup","text":"# Test the gateway curl http://localhost:8080/api/hello # Check health curl http://localhost:8080/health # View metrics curl http://localhost:9090/metrics # View stats curl http://localhost:8080/stats","title":"Step 4: Test Your Setup"},{"location":"getting-started/#step-5-add-more-features","text":"Now that you have the basics working, explore additional features:","title":"Step 5: Add More Features"},{"location":"getting-started/#add-load-balancing","text":"upstreams: - name: my-backend targets: - url: http://localhost:3001 weight: 2 - url: http://localhost:3002 weight: 1 - url: http://localhost:3003 weight: 1 load_balance: weighted_round_robin","title":"Add Load Balancing"},{"location":"getting-started/#add-rate-limiting","text":"rate_limit: enabled: true default_rps: 100 default_burst: 200 per_ip: true per_api_key: true routes: - name: api-routes path: /api/** upstream: my-backend rate_limit: enabled: true requests_per_second: 50 burst_size: 100","title":"Add Rate Limiting"},{"location":"getting-started/#add-api-keys","text":"api_keys: - key: \"pk_live_abc123\" name: \"production-app\" requests_per_second: 1000 burst_size: 2000 enabled: true","title":"Add API Keys"},{"location":"getting-started/#next-steps","text":"Configuration Reference - Complete configuration options Routing - Advanced routing patterns Load Balancing - Load balancing strategies Rate Limiting - Protect your APIs Metrics - Monitoring and observability","title":"Next Steps"},{"location":"getting-started/#getting-help","text":"If you run into issues: Check the Troubleshooting Guide Search GitHub Issues Ask in GitHub Discussions","title":"Getting Help"},{"location":"installation/","text":"This guide covers all installation methods for Relaypoint. System Requirements Requirement Minimum Recommended CPU 1 core 2+ cores Memory 64 MB 256 MB Disk 20 MB 50 MB OS Linux, macOS, Windows Linux Installation Methods Pre-built Binaries Download the latest release from GitHub Releases . Linux # AMD64 curl -LO https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-linux-amd64.tar.gz tar -xzf relaypoint-linux-amd64.tar.gz sudo mv relaypoint /usr/local/bin/ sudo chmod +x /usr/local/bin/relaypoint # ARM64 (Raspberry Pi, AWS Graviton, etc.) curl -LO https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-linux-arm64.tar.gz tar -xzf relaypoint-linux-arm64.tar.gz sudo mv relaypoint /usr/local/bin/ sudo chmod +x /usr/local/bin/relaypoint macOS # Intel Mac curl -LO https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-darwin-amd64.tar.gz tar -xzf relaypoint-darwin-amd64.tar.gz sudo mv relaypoint /usr/local/bin/ # Apple Silicon (M1/M2/M3) curl -LO https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-darwin-arm64.tar.gz tar -xzf relaypoint-darwin-arm64.tar.gz sudo mv relaypoint /usr/local/bin/ Windows Download relaypoint-windows-amd64.zip from GitHub Releases Extract the ZIP file Add the directory to your PATH or move relaypoint.exe to a directory in your PATH # PowerShell Invoke-WebRequest -Uri \"https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-windows-amd64.zip\" -OutFile \"relaypoint.zip\" Expand-Archive -Path \"relaypoint.zip\" -DestinationPath \"C:\\Program Files\\Relaypoint\" Building from Source Prerequisites Go 1.24 or later Git Make (optional, but recommended) Clone and Build # Clone the repository git clone https://github.com/relaypoint/relaypoint.git cd relaypoint # Build using Make make build # Or build directly with Go go build -o relaypoint ./cmd/relaypoint Build Options # Build for all platforms make build-all # Build with version information VERSION=v1.0.0 make build # Build for a specific platform GOOS=linux GOARCH=amd64 go build -o relaypoint-linux-amd64 ./cmd/relaypoint Install Locally # Install to /usr/local/bin (requires sudo) make install # Uninstall make uninstall Verifying Installation After installation, verify Relaypoint is working: # Check version relaypoint -version # Validate configuration relaypoint -config relaypoint.yml -validate # Start in foreground relaypoint -config relaypoint.yml # Test endpoints curl http://localhost:8080/health curl http://localhost:9090/metrics Upgrading Binary Upgrade # Download new version curl -LO https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-linux-amd64.tar.gz tar -xzf relaypoint-linux-amd64.tar.gz sudo mv relaypoint /usr/local/bin/ ## Uninstalling ### Binary Uninstall ```bash # Remove files sudo rm /usr/local/bin/relaypoint sudo rm -rf /etc/relaypoint sudo rm -rf /var/log/relaypoint Next Steps Getting Started - Your first Relaypoint configuration Configuration Reference - Complete configuration options","title":"Installation"},{"location":"installation/#system-requirements","text":"Requirement Minimum Recommended CPU 1 core 2+ cores Memory 64 MB 256 MB Disk 20 MB 50 MB OS Linux, macOS, Windows Linux","title":"System Requirements"},{"location":"installation/#installation-methods","text":"","title":"Installation Methods"},{"location":"installation/#pre-built-binaries","text":"Download the latest release from GitHub Releases .","title":"Pre-built Binaries"},{"location":"installation/#linux","text":"# AMD64 curl -LO https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-linux-amd64.tar.gz tar -xzf relaypoint-linux-amd64.tar.gz sudo mv relaypoint /usr/local/bin/ sudo chmod +x /usr/local/bin/relaypoint # ARM64 (Raspberry Pi, AWS Graviton, etc.) curl -LO https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-linux-arm64.tar.gz tar -xzf relaypoint-linux-arm64.tar.gz sudo mv relaypoint /usr/local/bin/ sudo chmod +x /usr/local/bin/relaypoint","title":"Linux"},{"location":"installation/#macos","text":"# Intel Mac curl -LO https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-darwin-amd64.tar.gz tar -xzf relaypoint-darwin-amd64.tar.gz sudo mv relaypoint /usr/local/bin/ # Apple Silicon (M1/M2/M3) curl -LO https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-darwin-arm64.tar.gz tar -xzf relaypoint-darwin-arm64.tar.gz sudo mv relaypoint /usr/local/bin/","title":"macOS"},{"location":"installation/#windows","text":"Download relaypoint-windows-amd64.zip from GitHub Releases Extract the ZIP file Add the directory to your PATH or move relaypoint.exe to a directory in your PATH # PowerShell Invoke-WebRequest -Uri \"https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-windows-amd64.zip\" -OutFile \"relaypoint.zip\" Expand-Archive -Path \"relaypoint.zip\" -DestinationPath \"C:\\Program Files\\Relaypoint\"","title":"Windows"},{"location":"installation/#building-from-source","text":"","title":"Building from Source"},{"location":"installation/#prerequisites","text":"Go 1.24 or later Git Make (optional, but recommended)","title":"Prerequisites"},{"location":"installation/#clone-and-build","text":"# Clone the repository git clone https://github.com/relaypoint/relaypoint.git cd relaypoint # Build using Make make build # Or build directly with Go go build -o relaypoint ./cmd/relaypoint","title":"Clone and Build"},{"location":"installation/#build-options","text":"# Build for all platforms make build-all # Build with version information VERSION=v1.0.0 make build # Build for a specific platform GOOS=linux GOARCH=amd64 go build -o relaypoint-linux-amd64 ./cmd/relaypoint","title":"Build Options"},{"location":"installation/#install-locally","text":"# Install to /usr/local/bin (requires sudo) make install # Uninstall make uninstall","title":"Install Locally"},{"location":"installation/#verifying-installation","text":"After installation, verify Relaypoint is working: # Check version relaypoint -version # Validate configuration relaypoint -config relaypoint.yml -validate # Start in foreground relaypoint -config relaypoint.yml # Test endpoints curl http://localhost:8080/health curl http://localhost:9090/metrics","title":"Verifying Installation"},{"location":"installation/#upgrading","text":"","title":"Upgrading"},{"location":"installation/#binary-upgrade","text":"# Download new version curl -LO https://github.com/relaypoint/relaypoint/releases/latest/download/relaypoint-linux-amd64.tar.gz tar -xzf relaypoint-linux-amd64.tar.gz sudo mv relaypoint /usr/local/bin/ ## Uninstalling ### Binary Uninstall ```bash # Remove files sudo rm /usr/local/bin/relaypoint sudo rm -rf /etc/relaypoint sudo rm -rf /var/log/relaypoint","title":"Binary Upgrade"},{"location":"installation/#next-steps","text":"Getting Started - Your first Relaypoint configuration Configuration Reference - Complete configuration options","title":"Next Steps"},{"location":"troubleshooting/","text":"This guide helps you diagnose and resolve common issues with Relaypoint. Quick Diagnostics Run these commands to quickly assess the gateway status: # Check if Relaypoint is running curl -s http://localhost:8080/health # Check metrics endpoint curl -s http://localhost:9090/metrics | head -20 # Check stats curl -s http://localhost:8080/stats | jq # Test a route curl -v http://localhost:8080/your/route Startup Issues Configuration File Not Found Error : Failed to load configuration: failed to read config file: open relaypoint.yml: no such file or directory Solution : # Specify the correct path ./relaypoint -config /path/to/your/config.yml # Or create a config in the current directory cat > relaypoint.yml << 'EOF' server: port: 8080 upstreams: - name: backend targets: - url: http://localhost:3000 routes: - name: default path: /** upstream: backend EOF Invalid Configuration Error : Invalid configuration: at least one route must be defined Solution : Ensure your configuration has required fields: # Minimum valid configuration server: port: 8080 upstreams: - name: backend # Required: upstream name targets: - url: http://localhost:3000 # Required: at least one target routes: - name: default # Optional but recommended path: /** # Required: path pattern upstream: backend # Required: must match an upstream name Port Already in Use Error : server error: listen tcp :8080: bind: address already in use Solution : # Find what's using the port lsof -i :8080 # Kill the process or use a different port ./relaypoint -config config.yml # Change port in config # Or in config: server: port: 8081 Invalid Upstream URL Error : invalid upstream URL http://backend: parse \"http://backend\": invalid character \" \" in host name Solution : Ensure URLs are properly formatted: upstreams: - name: backend targets: # Good - url: http://backend:3000 - url: http://192.168.1.100:3000 - url: http://localhost:3000 # Bad - url: backend:3000 # Missing scheme - url: http://back end:3000 # Space in hostname Request Routing Issues 404 Not Found Symptoms : All requests return 404. Causes : No matching route Route path doesn't match request Host-based routing mismatch Debugging : # Check your routes cat config.yml | grep -A5 \"routes:\" # Test with verbose curl curl -v http://localhost:8080/api/users # Check if any routes are configured curl -s http://localhost:9090/metrics | grep requests_total Solution : routes: # Use /** to match all paths - name: catchall path: /** upstream: backend # Or be explicit - name: api path: /api/** # Note the ** for wildcard upstream: backend 502 Bad Gateway Symptoms : Requests return 502 errors. Causes : Backend is not running Wrong backend URL Backend rejecting connections Debugging : # Test backend directly curl -v http://localhost:3000/health # Check backend connectivity from Relaypoint host nc -zv localhost 3000 # Check error metrics curl -s http://localhost:9090/metrics | grep errors_total Solution : Verify backend is running Check target URLs in configuration Ensure no firewall blocking connections 503 Service Unavailable Symptoms : Requests return 503 errors. Causes : All backends are unhealthy Health checks failing Debugging : # Check upstream health curl -s http://localhost:9090/metrics | grep upstream_healthy # Test backend health endpoint directly curl -v http://localhost:3000/health Solution : Fix backend health issues Verify health check path is correct Increase health check timeout if needed Wrong Backend Receiving Requests Symptoms : Requests routed to unexpected backend. Causes : Route priority issues Overlapping path patterns Debugging : # Check which route matched (via metrics) curl http://localhost:8080/your/path curl -s http://localhost:9090/metrics | grep requests_total | tail -5 Solution : Order routes from most specific to least specific: routes: # Most specific first - name: users-by-id path: /api/users/:id upstream: user-detail-service # Then broader patterns - name: users path: /api/users/** upstream: user-service # Catch-all last - name: default path: /** upstream: default-service Load Balancing Issues Traffic Not Distributed Symptoms : All traffic goes to one backend. Causes : Other backends marked unhealthy Only one target configured Configuration error Debugging : # Check backend health curl -s http://localhost:9090/metrics | grep upstream_healthy # Make multiple requests and check response for i in {1..10}; do curl -s http://localhost:8080/api/test | grep -o '\"server\":\"[^\"]*\"' done Solution : Fix unhealthy backends Verify all targets are configured: upstreams: - name: api targets: - url: http://backend-1:3000 - url: http://backend-2:3000 # Make sure all are listed - url: http://backend-3:3000 Backend Overload Symptoms : One backend getting too much traffic. Causes : Using round-robin with unequal backend capacity Weights not configured correctly Solution : upstreams: - name: api targets: - url: http://large-server:3000 weight: 4 # More traffic - url: http://small-server:3000 weight: 1 # Less traffic load_balance: weighted_round_robin Rate Limiting Issues Rate Limits Not Working Symptoms : Requests exceeding limits not blocked. Causes : Rate limiting not enabled High burst allowing spikes Wrong limit type Debugging : # Check if rate limiting is enabled grep -A5 \"rate_limit:\" config.yml # Make rapid requests for i in {1..100}; do curl -s -o /dev/null -w \"%{http_code}\\n\" http://localhost:8080/api/test done | sort | uniq -c Solution : rate_limit: enabled: true # Must be true default_rps: 10 # Lower for testing default_burst: 10 # Lower burst per_ip: true routes: - name: api path: /api/** upstream: backend rate_limit: enabled: true # Enable per-route requests_per_second: 5 burst_size: 10 Legitimate Users Being Blocked Symptoms : Real users getting 429 errors. Causes : Limits too low Shared IP (NAT/proxy) Burst not high enough Solution : rate_limit: default_rps: 100 # Increase RPS default_burst: 500 # Higher burst for spikes # Or use API keys for legitimate users api_keys: - key: \"customer_key\" name: \"verified-customer\" requests_per_second: 1000 burst_size: 2000 Rate Limit Metrics Not Showing Symptoms : No rate limit metrics despite 429 responses. Causes : Metrics not enabled Looking at wrong metrics Debugging : curl -s http://localhost:9090/metrics | grep rate_limit Solution : metrics: enabled: true Health Check Issues All Backends Showing Unhealthy Symptoms : upstream_healthy all showing 0. Causes : Health endpoint not implemented Wrong health check path Health check timeout too short Debugging : # Test health endpoint directly curl -v http://backend:3000/health # Check health check configuration grep -A4 \"health_check:\" config.yml Solution : upstreams: - name: api targets: - url: http://backend:3000 health_check: path: /health # Verify this path exists interval: 10s timeout: 5s # Increase if backend is slow Health Check Flapping Symptoms : Backend alternates healthy/unhealthy. Causes : Network instability Backend at capacity Timeout too aggressive Solution : health_check: path: /health interval: 30s # Less frequent checks timeout: 10s # More generous timeout Metrics Issues Metrics Endpoint Not Available Symptoms : curl localhost:9090/metrics fails. Causes : Metrics disabled Wrong port Metrics server failed to start Debugging : # Check if metrics port is listening netstat -tlnp | grep 9090 # Check Relaypoint logs for errors # (logs go to stderr) Solution : metrics: enabled: true port: 9090 path: /metrics Missing Metrics Symptoms : Expected metrics not appearing. Causes : No traffic to generate metrics Labels don't match query Debugging : # Get all metrics curl -s http://localhost:9090/metrics | grep gateway # Generate some traffic for i in {1..10}; do curl -s http://localhost:8080/api/test; done # Check again curl -s http://localhost:9090/metrics | grep gateway Performance Issues High Latency Symptoms : Requests taking longer than expected. Causes : Backend slow Network issues Connection pooling exhausted Debugging : # Check latency metrics curl -s http://localhost:9090/metrics | grep duration # Compare with direct backend request time curl -s http://backend:3000/api/test time curl -s http://localhost:8080/api/test Solution : Investigate backend performance Check network between Relaypoint and backends Tune timeouts: server: read_timeout: 60s write_timeout: 60s High Memory Usage Symptoms : Relaypoint memory growing over time. Causes : Many unique rate limit keys (IPs) Memory leak (please report!) Solution : rate_limit: cleanup_interval: 1m # More frequent cleanup Connection Errors Symptoms : Intermittent connection failures. Causes : Backend connection limits Too many idle connections Solution : Currently connection pool settings are built-in. Consider scaling backends or using load balancer in front of Relaypoint. Logging and Debugging Enable Debug Information Check logs for detailed information: # Run in foreground to see logs ./relaypoint -config config.yml 2>&1 | tee relaypoint.log # Parse JSON logs ./relaypoint -config config.yml 2>&1 | jq -r '.msg' Common Log Messages Message Meaning Starting RelayPoint Gateway starting configuration loaded Config parsed successfully upstream unhealthy Backend failed health check server error Fatal error, gateway stopped Getting Help If you can't resolve an issue: Search existing issues : GitHub Issues Gather information : ```bash # Version ./relaypoint -version # Configuration (remove secrets) cat config.yml # Metrics curl -s http://localhost:9090/metrics > metrics.txt # Logs ./relaypoint -config config.yml 2>&1 | head -100 ``` Open an issue with: Relaypoint version Configuration (sanitized) Steps to reproduce Expected vs actual behavior Relevant logs/metrics Next Steps Best Practices - Prevent issues before they happen Metrics - Set up monitoring Configuration - Review all options","title":"Troubleshooting"},{"location":"troubleshooting/#quick-diagnostics","text":"Run these commands to quickly assess the gateway status: # Check if Relaypoint is running curl -s http://localhost:8080/health # Check metrics endpoint curl -s http://localhost:9090/metrics | head -20 # Check stats curl -s http://localhost:8080/stats | jq # Test a route curl -v http://localhost:8080/your/route","title":"Quick Diagnostics"},{"location":"troubleshooting/#startup-issues","text":"","title":"Startup Issues"},{"location":"troubleshooting/#configuration-file-not-found","text":"Error : Failed to load configuration: failed to read config file: open relaypoint.yml: no such file or directory Solution : # Specify the correct path ./relaypoint -config /path/to/your/config.yml # Or create a config in the current directory cat > relaypoint.yml << 'EOF' server: port: 8080 upstreams: - name: backend targets: - url: http://localhost:3000 routes: - name: default path: /** upstream: backend EOF","title":"Configuration File Not Found"},{"location":"troubleshooting/#invalid-configuration","text":"Error : Invalid configuration: at least one route must be defined Solution : Ensure your configuration has required fields: # Minimum valid configuration server: port: 8080 upstreams: - name: backend # Required: upstream name targets: - url: http://localhost:3000 # Required: at least one target routes: - name: default # Optional but recommended path: /** # Required: path pattern upstream: backend # Required: must match an upstream name","title":"Invalid Configuration"},{"location":"troubleshooting/#port-already-in-use","text":"Error : server error: listen tcp :8080: bind: address already in use Solution : # Find what's using the port lsof -i :8080 # Kill the process or use a different port ./relaypoint -config config.yml # Change port in config # Or in config: server: port: 8081","title":"Port Already in Use"},{"location":"troubleshooting/#invalid-upstream-url","text":"Error : invalid upstream URL http://backend: parse \"http://backend\": invalid character \" \" in host name Solution : Ensure URLs are properly formatted: upstreams: - name: backend targets: # Good - url: http://backend:3000 - url: http://192.168.1.100:3000 - url: http://localhost:3000 # Bad - url: backend:3000 # Missing scheme - url: http://back end:3000 # Space in hostname","title":"Invalid Upstream URL"},{"location":"troubleshooting/#request-routing-issues","text":"","title":"Request Routing Issues"},{"location":"troubleshooting/#404-not-found","text":"Symptoms : All requests return 404. Causes : No matching route Route path doesn't match request Host-based routing mismatch Debugging : # Check your routes cat config.yml | grep -A5 \"routes:\" # Test with verbose curl curl -v http://localhost:8080/api/users # Check if any routes are configured curl -s http://localhost:9090/metrics | grep requests_total Solution : routes: # Use /** to match all paths - name: catchall path: /** upstream: backend # Or be explicit - name: api path: /api/** # Note the ** for wildcard upstream: backend","title":"404 Not Found"},{"location":"troubleshooting/#502-bad-gateway","text":"Symptoms : Requests return 502 errors. Causes : Backend is not running Wrong backend URL Backend rejecting connections Debugging : # Test backend directly curl -v http://localhost:3000/health # Check backend connectivity from Relaypoint host nc -zv localhost 3000 # Check error metrics curl -s http://localhost:9090/metrics | grep errors_total Solution : Verify backend is running Check target URLs in configuration Ensure no firewall blocking connections","title":"502 Bad Gateway"},{"location":"troubleshooting/#503-service-unavailable","text":"Symptoms : Requests return 503 errors. Causes : All backends are unhealthy Health checks failing Debugging : # Check upstream health curl -s http://localhost:9090/metrics | grep upstream_healthy # Test backend health endpoint directly curl -v http://localhost:3000/health Solution : Fix backend health issues Verify health check path is correct Increase health check timeout if needed","title":"503 Service Unavailable"},{"location":"troubleshooting/#wrong-backend-receiving-requests","text":"Symptoms : Requests routed to unexpected backend. Causes : Route priority issues Overlapping path patterns Debugging : # Check which route matched (via metrics) curl http://localhost:8080/your/path curl -s http://localhost:9090/metrics | grep requests_total | tail -5 Solution : Order routes from most specific to least specific: routes: # Most specific first - name: users-by-id path: /api/users/:id upstream: user-detail-service # Then broader patterns - name: users path: /api/users/** upstream: user-service # Catch-all last - name: default path: /** upstream: default-service","title":"Wrong Backend Receiving Requests"},{"location":"troubleshooting/#load-balancing-issues","text":"","title":"Load Balancing Issues"},{"location":"troubleshooting/#traffic-not-distributed","text":"Symptoms : All traffic goes to one backend. Causes : Other backends marked unhealthy Only one target configured Configuration error Debugging : # Check backend health curl -s http://localhost:9090/metrics | grep upstream_healthy # Make multiple requests and check response for i in {1..10}; do curl -s http://localhost:8080/api/test | grep -o '\"server\":\"[^\"]*\"' done Solution : Fix unhealthy backends Verify all targets are configured: upstreams: - name: api targets: - url: http://backend-1:3000 - url: http://backend-2:3000 # Make sure all are listed - url: http://backend-3:3000","title":"Traffic Not Distributed"},{"location":"troubleshooting/#backend-overload","text":"Symptoms : One backend getting too much traffic. Causes : Using round-robin with unequal backend capacity Weights not configured correctly Solution : upstreams: - name: api targets: - url: http://large-server:3000 weight: 4 # More traffic - url: http://small-server:3000 weight: 1 # Less traffic load_balance: weighted_round_robin","title":"Backend Overload"},{"location":"troubleshooting/#rate-limiting-issues","text":"","title":"Rate Limiting Issues"},{"location":"troubleshooting/#rate-limits-not-working","text":"Symptoms : Requests exceeding limits not blocked. Causes : Rate limiting not enabled High burst allowing spikes Wrong limit type Debugging : # Check if rate limiting is enabled grep -A5 \"rate_limit:\" config.yml # Make rapid requests for i in {1..100}; do curl -s -o /dev/null -w \"%{http_code}\\n\" http://localhost:8080/api/test done | sort | uniq -c Solution : rate_limit: enabled: true # Must be true default_rps: 10 # Lower for testing default_burst: 10 # Lower burst per_ip: true routes: - name: api path: /api/** upstream: backend rate_limit: enabled: true # Enable per-route requests_per_second: 5 burst_size: 10","title":"Rate Limits Not Working"},{"location":"troubleshooting/#legitimate-users-being-blocked","text":"Symptoms : Real users getting 429 errors. Causes : Limits too low Shared IP (NAT/proxy) Burst not high enough Solution : rate_limit: default_rps: 100 # Increase RPS default_burst: 500 # Higher burst for spikes # Or use API keys for legitimate users api_keys: - key: \"customer_key\" name: \"verified-customer\" requests_per_second: 1000 burst_size: 2000","title":"Legitimate Users Being Blocked"},{"location":"troubleshooting/#rate-limit-metrics-not-showing","text":"Symptoms : No rate limit metrics despite 429 responses. Causes : Metrics not enabled Looking at wrong metrics Debugging : curl -s http://localhost:9090/metrics | grep rate_limit Solution : metrics: enabled: true","title":"Rate Limit Metrics Not Showing"},{"location":"troubleshooting/#health-check-issues","text":"","title":"Health Check Issues"},{"location":"troubleshooting/#all-backends-showing-unhealthy","text":"Symptoms : upstream_healthy all showing 0. Causes : Health endpoint not implemented Wrong health check path Health check timeout too short Debugging : # Test health endpoint directly curl -v http://backend:3000/health # Check health check configuration grep -A4 \"health_check:\" config.yml Solution : upstreams: - name: api targets: - url: http://backend:3000 health_check: path: /health # Verify this path exists interval: 10s timeout: 5s # Increase if backend is slow","title":"All Backends Showing Unhealthy"},{"location":"troubleshooting/#health-check-flapping","text":"Symptoms : Backend alternates healthy/unhealthy. Causes : Network instability Backend at capacity Timeout too aggressive Solution : health_check: path: /health interval: 30s # Less frequent checks timeout: 10s # More generous timeout","title":"Health Check Flapping"},{"location":"troubleshooting/#metrics-issues","text":"","title":"Metrics Issues"},{"location":"troubleshooting/#metrics-endpoint-not-available","text":"Symptoms : curl localhost:9090/metrics fails. Causes : Metrics disabled Wrong port Metrics server failed to start Debugging : # Check if metrics port is listening netstat -tlnp | grep 9090 # Check Relaypoint logs for errors # (logs go to stderr) Solution : metrics: enabled: true port: 9090 path: /metrics","title":"Metrics Endpoint Not Available"},{"location":"troubleshooting/#missing-metrics","text":"Symptoms : Expected metrics not appearing. Causes : No traffic to generate metrics Labels don't match query Debugging : # Get all metrics curl -s http://localhost:9090/metrics | grep gateway # Generate some traffic for i in {1..10}; do curl -s http://localhost:8080/api/test; done # Check again curl -s http://localhost:9090/metrics | grep gateway","title":"Missing Metrics"},{"location":"troubleshooting/#performance-issues","text":"","title":"Performance Issues"},{"location":"troubleshooting/#high-latency","text":"Symptoms : Requests taking longer than expected. Causes : Backend slow Network issues Connection pooling exhausted Debugging : # Check latency metrics curl -s http://localhost:9090/metrics | grep duration # Compare with direct backend request time curl -s http://backend:3000/api/test time curl -s http://localhost:8080/api/test Solution : Investigate backend performance Check network between Relaypoint and backends Tune timeouts: server: read_timeout: 60s write_timeout: 60s","title":"High Latency"},{"location":"troubleshooting/#high-memory-usage","text":"Symptoms : Relaypoint memory growing over time. Causes : Many unique rate limit keys (IPs) Memory leak (please report!) Solution : rate_limit: cleanup_interval: 1m # More frequent cleanup","title":"High Memory Usage"},{"location":"troubleshooting/#connection-errors","text":"Symptoms : Intermittent connection failures. Causes : Backend connection limits Too many idle connections Solution : Currently connection pool settings are built-in. Consider scaling backends or using load balancer in front of Relaypoint.","title":"Connection Errors"},{"location":"troubleshooting/#logging-and-debugging","text":"","title":"Logging and Debugging"},{"location":"troubleshooting/#enable-debug-information","text":"Check logs for detailed information: # Run in foreground to see logs ./relaypoint -config config.yml 2>&1 | tee relaypoint.log # Parse JSON logs ./relaypoint -config config.yml 2>&1 | jq -r '.msg'","title":"Enable Debug Information"},{"location":"troubleshooting/#common-log-messages","text":"Message Meaning Starting RelayPoint Gateway starting configuration loaded Config parsed successfully upstream unhealthy Backend failed health check server error Fatal error, gateway stopped","title":"Common Log Messages"},{"location":"troubleshooting/#getting-help","text":"If you can't resolve an issue: Search existing issues : GitHub Issues Gather information : ```bash # Version ./relaypoint -version # Configuration (remove secrets) cat config.yml # Metrics curl -s http://localhost:9090/metrics > metrics.txt # Logs ./relaypoint -config config.yml 2>&1 | head -100 ``` Open an issue with: Relaypoint version Configuration (sanitized) Steps to reproduce Expected vs actual behavior Relevant logs/metrics","title":"Getting Help"},{"location":"troubleshooting/#next-steps","text":"Best Practices - Prevent issues before they happen Metrics - Set up monitoring Configuration - Review all options","title":"Next Steps"},{"location":"features/","text":"Relaypoint offers a wide-array of features that can be configurable Below is a list of features that Relaypoint currently offers. Routing Load Balancing Rate Limiting Health Checks Metrics & Monitoring API Keys","title":"Index"},{"location":"features/api-keys/","text":"Relaypoint supports API key authentication for identifying clients and applying per-key rate limits. Overview API keys in Relaypoint provide: Client identification - Track which application made a request Per-key rate limiting - Different limits for different clients Usage tracking - Metrics per API key Access control - Enable/disable keys instantly Basic Configuration api_keys: - key: \"pk_live_abc123def456\" name: \"production-app\" requests_per_second: 1000 burst_size: 2000 enabled: true Configuration Options Field Type Required Description key string Yes The API key value (keep secret!) name string Yes Human-readable identifier requests_per_second integer Yes Rate limit for this key burst_size integer No Burst capacity (default: 2x RPS) enabled boolean No Whether key is active (default: true) How API Keys Work \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Request \u2502 \u2502 Authorization: Bearer pk_live_abc123 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Relaypoint \u2502 \u2502 \u2502 \u2502 1. Extract API key from request \u2502 \u2502 2. Look up key in configuration \u2502 \u2502 3. Check if key is enabled \u2502 \u2502 4. Apply key-specific rate limit \u2502 \u2502 5. Record metrics with key name \u2502 \u2502 6. Forward request to backend \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Providing API Keys Clients can provide API keys in multiple ways (checked in order): 1. Authorization Header (Recommended) # Bearer token format curl -H \"Authorization: Bearer pk_live_abc123\" https://api.example.com/users # ApiKey format curl -H \"Authorization: ApiKey pk_live_abc123\" https://api.example.com/users 2. X-API-Key Header curl -H \"X-API-Key: pk_live_abc123\" https://api.example.com/users 3. Query Parameter (Least Secure) curl \"https://api.example.com/users?api_key=pk_live_abc123\" \u26a0\ufe0f Warning : Query parameters appear in logs and browser history. Use headers for production. API Key Patterns Tiered Access api_keys: # Free tier - key: \"pk_free_user123\" name: \"free-tier-user\" requests_per_second: 60 burst_size: 100 enabled: true # Pro tier - key: \"pk_pro_user456\" name: \"pro-tier-user\" requests_per_second: 600 burst_size: 1000 enabled: true # Enterprise tier - key: \"pk_ent_user789\" name: \"enterprise-user\" requests_per_second: 6000 burst_size: 10000 enabled: true Development vs Production api_keys: # Production keys - higher limits - key: \"pk_live_webapp\" name: \"webapp-production\" requests_per_second: 5000 burst_size: 10000 enabled: true # Development/staging keys - lower limits - key: \"pk_test_webapp\" name: \"webapp-development\" requests_per_second: 100 burst_size: 200 enabled: true Service-to-Service api_keys: # Internal service keys - high limits - key: \"sk_internal_orders\" name: \"orders-service\" requests_per_second: 50000 burst_size: 100000 enabled: true - key: \"sk_internal_payments\" name: \"payments-service\" requests_per_second: 50000 burst_size: 100000 enabled: true Partner Integrations api_keys: - key: \"pk_partner_acme\" name: \"acme-corp-integration\" requests_per_second: 2000 burst_size: 4000 enabled: true - key: \"pk_partner_bigco\" name: \"bigco-integration\" requests_per_second: 5000 burst_size: 10000 enabled: true Key Naming Conventions Recommended Prefixes Prefix Use Case pk_live_ Production publishable keys pk_test_ Test/development keys sk_live_ Production secret keys sk_test_ Test secret keys Example api_keys: - key: \"pk_live_user_abc123def456789\" name: \"acme-corp-production\" # ... - key: \"pk_test_user_xyz789abc123456\" name: \"acme-corp-development\" # ... Enabling Rate Limiting for API Keys Enable per-API-key rate limiting globally: rate_limit: enabled: true per_api_key: true # Required for API key rate limiting api_keys: - key: \"pk_live_abc123\" name: \"my-app\" requests_per_second: 1000 burst_size: 2000 enabled: true Disabling API Keys Instantly disable a key: api_keys: - key: \"pk_live_compromised_key\" name: \"compromised-client\" requests_per_second: 1000 enabled: false # Key is disabled Disabled keys: Are not checked for rate limits Do not appear in metrics Requests are still processed (unless other limits block them) Note : Disabling a key doesn't block requests entirely. For blocking, implement authentication middleware or use a firewall. Monitoring API Key Usage Metrics curl http://localhost:9090/metrics | grep api_key Output: gateway_api_key_requests_total{key=\"production-app_200\"} 15234 gateway_api_key_requests_total{key=\"production-app_429\"} 45 gateway_api_key_requests_total{key=\"development-app_200\"} 1234 Stats Endpoint curl http://localhost:8080/stats | jq '.[] | select(.key | contains(\"apikey\"))' Grafana Queries # Requests per API key sum by (key) (rate(gateway_api_key_requests_total[5m])) # Top 10 API keys by request volume topk(10, sum by (key) (rate(gateway_api_key_requests_total[5m]))) # Rate limit hits per API key sum by (key) (rate(gateway_rate_limit_hits_total{key=~\".*_apikey\"}[5m])) Security Best Practices 1. Keep Keys Secret # Bad: Key in plain text in version control api_keys: - key: \"pk_live_abc123\" name: \"my-app\" # Better: Use environment variables # Process config with envsubst before loading api_keys: - key: \"${API_KEY_MYAPP}\" name: \"my-app\" 2. Use Strong Key Values # Generate secure keys import secrets key = f\"pk_live_{secrets.token_urlsafe(32)}\" # Result: pk_live_dG9rZW5fdXJsc2FmZV8zMg... # Or use openssl openssl rand -base64 32 | tr -d '/+=' | head -c 32 3. Rotate Keys Regularly # During rotation, both keys are active api_keys: - key: \"pk_live_new_key_abc\" name: \"my-app-v2\" enabled: true - key: \"pk_live_old_key_xyz\" name: \"my-app-v1-deprecated\" enabled: true # Disable after migration 4. Use Different Keys per Environment # Production config api_keys: - key: \"pk_live_production\" name: \"webapp-prod\" enabled: true # Staging config api_keys: - key: \"pk_test_staging\" name: \"webapp-staging\" enabled: true 5. Log Key Names, Not Values Relaypoint logs the key name , never the key value : { \"level\": \"INFO\", \"msg\": \"request processed\", \"api_key_name\": \"production-app\" } Handling Unknown Keys Requests with unrecognized API keys: Are still processed (not blocked) Use default rate limits Do not appear in API key metrics To require API keys, implement authentication middleware in your backend or add a validation route. Example: Full Configuration server: port: 8080 rate_limit: enabled: true default_rps: 60 # Anonymous users default_burst: 120 per_ip: true per_api_key: true upstreams: - name: api targets: - url: http://backend:3000 load_balance: round_robin routes: - name: public-api path: /api/** upstream: api api_keys: # Tier 1: Free - key: \"pk_free_abc123\" name: \"free-tier\" requests_per_second: 60 burst_size: 100 enabled: true # Tier 2: Starter - key: \"pk_starter_def456\" name: \"starter-tier\" requests_per_second: 300 burst_size: 500 enabled: true # Tier 3: Pro - key: \"pk_pro_ghi789\" name: \"pro-tier\" requests_per_second: 1000 burst_size: 2000 enabled: true # Tier 4: Enterprise - key: \"pk_ent_jkl012\" name: \"enterprise-tier\" requests_per_second: 10000 burst_size: 20000 enabled: true # Internal services - key: \"sk_internal_service\" name: \"internal-service\" requests_per_second: 100000 burst_size: 200000 enabled: true # Disabled key (compromised) - key: \"pk_old_compromised\" name: \"compromised-key\" requests_per_second: 0 enabled: false Troubleshooting API Key Not Recognized Symptoms : Requests not tracked with key name. Causes : Key not in configuration Wrong key format in request Typo in key value Solutions : Verify key is in config Check request header format Compare key values exactly Rate Limit Not Applied Symptoms : API key exceeds its limit without 429 errors. Causes : per_api_key: false in rate limit config Key has very high limits Rate limiting disabled Solutions : Enable per_api_key: true Check key's RPS configuration Verify rate_limit.enabled: true Metrics Not Showing Symptoms : API key requests not in metrics. Causes : Key not found in configuration Metrics disabled Solutions : Add key to configuration Enable metrics: metrics.enabled: true Next Steps Rate Limiting - How rate limits work Metrics - Monitor API key usage Best Practices - Security recommendations","title":"API Keys"},{"location":"features/api-keys/#overview","text":"API keys in Relaypoint provide: Client identification - Track which application made a request Per-key rate limiting - Different limits for different clients Usage tracking - Metrics per API key Access control - Enable/disable keys instantly","title":"Overview"},{"location":"features/api-keys/#basic-configuration","text":"api_keys: - key: \"pk_live_abc123def456\" name: \"production-app\" requests_per_second: 1000 burst_size: 2000 enabled: true","title":"Basic Configuration"},{"location":"features/api-keys/#configuration-options","text":"Field Type Required Description key string Yes The API key value (keep secret!) name string Yes Human-readable identifier requests_per_second integer Yes Rate limit for this key burst_size integer No Burst capacity (default: 2x RPS) enabled boolean No Whether key is active (default: true)","title":"Configuration Options"},{"location":"features/api-keys/#how-api-keys-work","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Request \u2502 \u2502 Authorization: Bearer pk_live_abc123 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Relaypoint \u2502 \u2502 \u2502 \u2502 1. Extract API key from request \u2502 \u2502 2. Look up key in configuration \u2502 \u2502 3. Check if key is enabled \u2502 \u2502 4. Apply key-specific rate limit \u2502 \u2502 5. Record metrics with key name \u2502 \u2502 6. Forward request to backend \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"How API Keys Work"},{"location":"features/api-keys/#providing-api-keys","text":"Clients can provide API keys in multiple ways (checked in order):","title":"Providing API Keys"},{"location":"features/api-keys/#1-authorization-header-recommended","text":"# Bearer token format curl -H \"Authorization: Bearer pk_live_abc123\" https://api.example.com/users # ApiKey format curl -H \"Authorization: ApiKey pk_live_abc123\" https://api.example.com/users","title":"1. Authorization Header (Recommended)"},{"location":"features/api-keys/#2-x-api-key-header","text":"curl -H \"X-API-Key: pk_live_abc123\" https://api.example.com/users","title":"2. X-API-Key Header"},{"location":"features/api-keys/#3-query-parameter-least-secure","text":"curl \"https://api.example.com/users?api_key=pk_live_abc123\" \u26a0\ufe0f Warning : Query parameters appear in logs and browser history. Use headers for production.","title":"3. Query Parameter (Least Secure)"},{"location":"features/api-keys/#api-key-patterns","text":"","title":"API Key Patterns"},{"location":"features/api-keys/#tiered-access","text":"api_keys: # Free tier - key: \"pk_free_user123\" name: \"free-tier-user\" requests_per_second: 60 burst_size: 100 enabled: true # Pro tier - key: \"pk_pro_user456\" name: \"pro-tier-user\" requests_per_second: 600 burst_size: 1000 enabled: true # Enterprise tier - key: \"pk_ent_user789\" name: \"enterprise-user\" requests_per_second: 6000 burst_size: 10000 enabled: true","title":"Tiered Access"},{"location":"features/api-keys/#development-vs-production","text":"api_keys: # Production keys - higher limits - key: \"pk_live_webapp\" name: \"webapp-production\" requests_per_second: 5000 burst_size: 10000 enabled: true # Development/staging keys - lower limits - key: \"pk_test_webapp\" name: \"webapp-development\" requests_per_second: 100 burst_size: 200 enabled: true","title":"Development vs Production"},{"location":"features/api-keys/#service-to-service","text":"api_keys: # Internal service keys - high limits - key: \"sk_internal_orders\" name: \"orders-service\" requests_per_second: 50000 burst_size: 100000 enabled: true - key: \"sk_internal_payments\" name: \"payments-service\" requests_per_second: 50000 burst_size: 100000 enabled: true","title":"Service-to-Service"},{"location":"features/api-keys/#partner-integrations","text":"api_keys: - key: \"pk_partner_acme\" name: \"acme-corp-integration\" requests_per_second: 2000 burst_size: 4000 enabled: true - key: \"pk_partner_bigco\" name: \"bigco-integration\" requests_per_second: 5000 burst_size: 10000 enabled: true","title":"Partner Integrations"},{"location":"features/api-keys/#key-naming-conventions","text":"","title":"Key Naming Conventions"},{"location":"features/api-keys/#recommended-prefixes","text":"Prefix Use Case pk_live_ Production publishable keys pk_test_ Test/development keys sk_live_ Production secret keys sk_test_ Test secret keys","title":"Recommended Prefixes"},{"location":"features/api-keys/#example","text":"api_keys: - key: \"pk_live_user_abc123def456789\" name: \"acme-corp-production\" # ... - key: \"pk_test_user_xyz789abc123456\" name: \"acme-corp-development\" # ...","title":"Example"},{"location":"features/api-keys/#enabling-rate-limiting-for-api-keys","text":"Enable per-API-key rate limiting globally: rate_limit: enabled: true per_api_key: true # Required for API key rate limiting api_keys: - key: \"pk_live_abc123\" name: \"my-app\" requests_per_second: 1000 burst_size: 2000 enabled: true","title":"Enabling Rate Limiting for API Keys"},{"location":"features/api-keys/#disabling-api-keys","text":"Instantly disable a key: api_keys: - key: \"pk_live_compromised_key\" name: \"compromised-client\" requests_per_second: 1000 enabled: false # Key is disabled Disabled keys: Are not checked for rate limits Do not appear in metrics Requests are still processed (unless other limits block them) Note : Disabling a key doesn't block requests entirely. For blocking, implement authentication middleware or use a firewall.","title":"Disabling API Keys"},{"location":"features/api-keys/#monitoring-api-key-usage","text":"","title":"Monitoring API Key Usage"},{"location":"features/api-keys/#metrics","text":"curl http://localhost:9090/metrics | grep api_key Output: gateway_api_key_requests_total{key=\"production-app_200\"} 15234 gateway_api_key_requests_total{key=\"production-app_429\"} 45 gateway_api_key_requests_total{key=\"development-app_200\"} 1234","title":"Metrics"},{"location":"features/api-keys/#stats-endpoint","text":"curl http://localhost:8080/stats | jq '.[] | select(.key | contains(\"apikey\"))'","title":"Stats Endpoint"},{"location":"features/api-keys/#grafana-queries","text":"# Requests per API key sum by (key) (rate(gateway_api_key_requests_total[5m])) # Top 10 API keys by request volume topk(10, sum by (key) (rate(gateway_api_key_requests_total[5m]))) # Rate limit hits per API key sum by (key) (rate(gateway_rate_limit_hits_total{key=~\".*_apikey\"}[5m]))","title":"Grafana Queries"},{"location":"features/api-keys/#security-best-practices","text":"","title":"Security Best Practices"},{"location":"features/api-keys/#1-keep-keys-secret","text":"# Bad: Key in plain text in version control api_keys: - key: \"pk_live_abc123\" name: \"my-app\" # Better: Use environment variables # Process config with envsubst before loading api_keys: - key: \"${API_KEY_MYAPP}\" name: \"my-app\"","title":"1. Keep Keys Secret"},{"location":"features/api-keys/#2-use-strong-key-values","text":"# Generate secure keys import secrets key = f\"pk_live_{secrets.token_urlsafe(32)}\" # Result: pk_live_dG9rZW5fdXJsc2FmZV8zMg... # Or use openssl openssl rand -base64 32 | tr -d '/+=' | head -c 32","title":"2. Use Strong Key Values"},{"location":"features/api-keys/#3-rotate-keys-regularly","text":"# During rotation, both keys are active api_keys: - key: \"pk_live_new_key_abc\" name: \"my-app-v2\" enabled: true - key: \"pk_live_old_key_xyz\" name: \"my-app-v1-deprecated\" enabled: true # Disable after migration","title":"3. Rotate Keys Regularly"},{"location":"features/api-keys/#4-use-different-keys-per-environment","text":"# Production config api_keys: - key: \"pk_live_production\" name: \"webapp-prod\" enabled: true # Staging config api_keys: - key: \"pk_test_staging\" name: \"webapp-staging\" enabled: true","title":"4. Use Different Keys per Environment"},{"location":"features/api-keys/#5-log-key-names-not-values","text":"Relaypoint logs the key name , never the key value : { \"level\": \"INFO\", \"msg\": \"request processed\", \"api_key_name\": \"production-app\" }","title":"5. Log Key Names, Not Values"},{"location":"features/api-keys/#handling-unknown-keys","text":"Requests with unrecognized API keys: Are still processed (not blocked) Use default rate limits Do not appear in API key metrics To require API keys, implement authentication middleware in your backend or add a validation route.","title":"Handling Unknown Keys"},{"location":"features/api-keys/#example-full-configuration","text":"server: port: 8080 rate_limit: enabled: true default_rps: 60 # Anonymous users default_burst: 120 per_ip: true per_api_key: true upstreams: - name: api targets: - url: http://backend:3000 load_balance: round_robin routes: - name: public-api path: /api/** upstream: api api_keys: # Tier 1: Free - key: \"pk_free_abc123\" name: \"free-tier\" requests_per_second: 60 burst_size: 100 enabled: true # Tier 2: Starter - key: \"pk_starter_def456\" name: \"starter-tier\" requests_per_second: 300 burst_size: 500 enabled: true # Tier 3: Pro - key: \"pk_pro_ghi789\" name: \"pro-tier\" requests_per_second: 1000 burst_size: 2000 enabled: true # Tier 4: Enterprise - key: \"pk_ent_jkl012\" name: \"enterprise-tier\" requests_per_second: 10000 burst_size: 20000 enabled: true # Internal services - key: \"sk_internal_service\" name: \"internal-service\" requests_per_second: 100000 burst_size: 200000 enabled: true # Disabled key (compromised) - key: \"pk_old_compromised\" name: \"compromised-key\" requests_per_second: 0 enabled: false","title":"Example: Full Configuration"},{"location":"features/api-keys/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"features/api-keys/#api-key-not-recognized","text":"Symptoms : Requests not tracked with key name. Causes : Key not in configuration Wrong key format in request Typo in key value Solutions : Verify key is in config Check request header format Compare key values exactly","title":"API Key Not Recognized"},{"location":"features/api-keys/#rate-limit-not-applied","text":"Symptoms : API key exceeds its limit without 429 errors. Causes : per_api_key: false in rate limit config Key has very high limits Rate limiting disabled Solutions : Enable per_api_key: true Check key's RPS configuration Verify rate_limit.enabled: true","title":"Rate Limit Not Applied"},{"location":"features/api-keys/#metrics-not-showing","text":"Symptoms : API key requests not in metrics. Causes : Key not found in configuration Metrics disabled Solutions : Add key to configuration Enable metrics: metrics.enabled: true","title":"Metrics Not Showing"},{"location":"features/api-keys/#next-steps","text":"Rate Limiting - How rate limits work Metrics - Monitor API key usage Best Practices - Security recommendations","title":"Next Steps"},{"location":"features/health-checks/","text":"Relaypoint continuously monitors backend health to ensure requests are only routed to healthy servers. Overview Health checks provide: Automatic failure detection - Unhealthy backends are removed from rotation Automatic recovery - Recovered backends are added back Zero downtime - Traffic shifts seamlessly between backends Visibility - Health status exposed via metrics Basic Configuration Enable health checks for an upstream: upstreams: - name: api-service targets: - url: http://backend-1:3000 - url: http://backend-2:3000 - url: http://backend-3:3000 load_balance: round_robin health_check: path: /health # Required: Health check endpoint interval: 10s # Check every 10 seconds timeout: 2s # Timeout for health check request Configuration Options Option Type Default Description path string Required Health check endpoint path interval duration 10s Time between health checks timeout duration 2s Request timeout for health check How Health Checks Work \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Relaypoint \u2502 \u2502 \u2502 \u2502 Health Checker (runs every interval) \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 For each upstream: \u2502 \u2502 \u2502 \u2502 For each target: \u2502 \u2502 \u2502 \u2502 1. Send GET request to health path \u2502 \u2502 \u2502 \u2502 2. Wait for response (up to timeout) \u2502 \u2502 \u2502 \u2502 3. Check status code (2xx/3xx = OK) \u2502 \u2502 \u2502 \u2502 4. Update target health status \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 Load Balancer \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Only routes to targets where: \u2502 \u2502 \u2502 \u2502 healthy = true \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Success Criteria A health check is considered successful if: Connection is established within timeout Response is received within timeout HTTP status code is 2xx or 3xx Failure Handling When a backend fails: Time 0:00 - Health check fails Time 0:00 - Backend marked unhealthy Time 0:00 - Traffic stops routing to this backend Time 0:10 - Next health check runs Time 0:10 - If successful, backend marked healthy Time 0:10 - Traffic resumes to this backend Health Check Endpoint Requirements Your backend services should implement a health endpoint that: Minimal Implementation // Go example http.HandleFunc(\"/health\", func(w http.ResponseWriter, r *http.Request) { w.WriteHeader(http.StatusOK) w.Write([]byte(`{\"status\":\"healthy\"}`)) }) # Python Flask example @app.route('/health') def health(): return jsonify({\"status\": \"healthy\"}), 200 // Node.js Express example app.get(\"/health\", (req, res) => { res.json({ status: \"healthy\" }); }); Comprehensive Implementation A robust health check should verify: func healthHandler(w http.ResponseWriter, r *http.Request) { // Check database connection if err := db.Ping(); err != nil { w.WriteHeader(http.StatusServiceUnavailable) json.NewEncoder(w).Encode(map[string]string{ \"status\": \"unhealthy\", \"error\": \"database unavailable\", }) return } // Check cache connection if err := cache.Ping(); err != nil { w.WriteHeader(http.StatusServiceUnavailable) json.NewEncoder(w).Encode(map[string]string{ \"status\": \"unhealthy\", \"error\": \"cache unavailable\", }) return } // All checks passed w.WriteHeader(http.StatusOK) json.NewEncoder(w).Encode(map[string]string{ \"status\": \"healthy\", }) } Multiple Upstreams Configure different health checks per upstream: upstreams: # API service - standard health check - name: api-service targets: - url: http://api-1:3000 - url: http://api-2:3000 health_check: path: /health interval: 10s timeout: 2s # Database proxy - quick checks - name: db-proxy targets: - url: http://db-proxy-1:5432 - url: http://db-proxy-2:5432 health_check: path: /ping interval: 5s timeout: 1s # Slow service - longer timeout - name: report-service targets: - url: http://reports:3000 health_check: path: /healthz interval: 30s timeout: 10s Monitoring Health Status Metrics Health status is exposed via Prometheus metrics: curl http://localhost:9090/metrics | grep upstream_healthy Output: # HELP gateway_upstream_healthy Whether upstream is healthy # TYPE gateway_upstream_healthy gauge gateway_upstream_healthy{key=\"api-service_http://backend-1:3000\"} 1 gateway_upstream_healthy{key=\"api-service_http://backend-2:3000\"} 1 gateway_upstream_healthy{key=\"api-service_http://backend-3:3000\"} 0 Values: 1 = Healthy 0 = Unhealthy Alerting on Unhealthy Backends Prometheus alerting rule: groups: - name: relaypoint rules: - alert: BackendUnhealthy expr: gateway_upstream_healthy == 0 for: 1m labels: severity: warning annotations: summary: \"Backend {{ $labels.key }} is unhealthy\" - alert: AllBackendsUnhealthy expr: sum by (upstream) (gateway_upstream_healthy) == 0 for: 30s labels: severity: critical annotations: summary: \"All backends for {{ $labels.upstream }} are unhealthy\" Grafana Dashboard Query examples: # Healthy backend count per upstream sum by (upstream) (gateway_upstream_healthy) # Unhealthy backend count per upstream count by (upstream) (gateway_upstream_healthy == 0) # Health check success rate (requires additional metrics) rate(gateway_health_checks_success_total[5m]) / rate(gateway_health_checks_total[5m]) Common Health Check Patterns Kubernetes-Style Probes upstreams: - name: k8s-service targets: - url: http://service:3000 health_check: path: /healthz # Kubernetes convention interval: 10s timeout: 2s Liveness vs Readiness If your service distinguishes between liveness and readiness: upstreams: # Use readiness for load balancing decisions - name: api-service targets: - url: http://api:3000 health_check: path: /ready # Readiness endpoint interval: 5s timeout: 2s Deep Health Checks For critical services, verify dependencies: upstreams: - name: critical-service targets: - url: http://critical:3000 health_check: path: /health/deep # Checks all dependencies interval: 30s # Longer interval (expensive check) timeout: 10s # Longer timeout (multiple checks) Failure Scenarios Single Backend Failure Before: Backend 1: \u2713 Healthy \u2190\u2500\u2510 Backend 2: \u2713 Healthy \u2190\u2500\u253c\u2500\u2500 Round Robin Backend 3: \u2713 Healthy \u2190\u2500\u2518 After Backend 2 fails: Backend 1: \u2713 Healthy \u2190\u2500\u2510 Backend 2: \u2717 Unhealthy \u2502 Round Robin Backend 3: \u2713 Healthy \u2190\u2500\u2518 Traffic automatically routes only to healthy backends. All Backends Fail When all backends are unhealthy, Relaypoint: Logs a warning Returns 503 Service Unavailable to clients Continues health checking Resumes traffic when any backend recovers Backend Recovery Time 0:00 - Backend 2 recovers Time 0:05 - Health check runs (interval = 5s) Time 0:05 - Health check succeeds Time 0:05 - Backend 2 marked healthy Time 0:05 - Traffic resumes to Backend 2 Best Practices 1. Keep Health Checks Lightweight // Good: Fast, simple check func health(w http.ResponseWriter, r *http.Request) { w.WriteHeader(http.StatusOK) } // Bad: Slow, complex check on health endpoint func health(w http.ResponseWriter, r *http.Request) { result := runExpensiveQuery() // Don't do this // ... } 2. Use Appropriate Intervals Service Type Recommended Interval High-availability API 5s Standard API 10s Background service 30s Batch processing 60s 3. Set Reasonable Timeouts # Good: Timeout < Interval health_check: interval: 10s timeout: 2s # Bad: Timeout >= Interval health_check: interval: 10s timeout: 15s # Health checks will overlap! 4. Match Health Check to Service Capabilities # Fast service - quick checks - name: cache health_check: path: /ping interval: 5s timeout: 500ms # Slow service - longer checks - name: analytics health_check: path: /health interval: 60s timeout: 10s 5. Use Dedicated Health Endpoints Don't use regular API endpoints for health checks: # Good: Dedicated health endpoint health_check: path: /health # Bad: Using regular endpoint health_check: path: /api/users # May be slow, rate-limited, auth-required Troubleshooting Backend Marked Unhealthy Incorrectly Symptoms : Working backend shows as unhealthy. Causes : Health endpoint returns non-2xx status Health endpoint too slow (timeout) Network connectivity issues Health endpoint requires authentication Solutions : Test health endpoint manually: curl -v http://backend:3000/health Increase timeout if endpoint is slow Check network/firewall between Relaypoint and backend Ensure health endpoint doesn't require auth Health Checks Overloading Backend Symptoms : High CPU/memory from health checks. Causes : Interval too short Health check doing expensive operations Solutions : Increase interval (e.g., 5s \u2192 30s) Optimize health check implementation Use simple ping endpoint instead of full health check Flapping Health Status Symptoms : Backend alternates between healthy/unhealthy. Causes : Intermittent network issues Backend at capacity Timeout too aggressive Solutions : Investigate network stability Scale backend or reduce traffic Increase timeout Gateway Health Endpoint Relaypoint exposes its own health endpoint: curl http://localhost:8080/health Response: { \"status\": \"healthy\" } This endpoint confirms Relaypoint itself is running, independent of backend health. Next Steps Load Balancing - How health affects load balancing Metrics - Monitor health status Troubleshooting - Debug health issues","title":"Health Checks"},{"location":"features/health-checks/#overview","text":"Health checks provide: Automatic failure detection - Unhealthy backends are removed from rotation Automatic recovery - Recovered backends are added back Zero downtime - Traffic shifts seamlessly between backends Visibility - Health status exposed via metrics","title":"Overview"},{"location":"features/health-checks/#basic-configuration","text":"Enable health checks for an upstream: upstreams: - name: api-service targets: - url: http://backend-1:3000 - url: http://backend-2:3000 - url: http://backend-3:3000 load_balance: round_robin health_check: path: /health # Required: Health check endpoint interval: 10s # Check every 10 seconds timeout: 2s # Timeout for health check request","title":"Basic Configuration"},{"location":"features/health-checks/#configuration-options","text":"Option Type Default Description path string Required Health check endpoint path interval duration 10s Time between health checks timeout duration 2s Request timeout for health check","title":"Configuration Options"},{"location":"features/health-checks/#how-health-checks-work","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Relaypoint \u2502 \u2502 \u2502 \u2502 Health Checker (runs every interval) \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 For each upstream: \u2502 \u2502 \u2502 \u2502 For each target: \u2502 \u2502 \u2502 \u2502 1. Send GET request to health path \u2502 \u2502 \u2502 \u2502 2. Wait for response (up to timeout) \u2502 \u2502 \u2502 \u2502 3. Check status code (2xx/3xx = OK) \u2502 \u2502 \u2502 \u2502 4. Update target health status \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 Load Balancer \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Only routes to targets where: \u2502 \u2502 \u2502 \u2502 healthy = true \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"How Health Checks Work"},{"location":"features/health-checks/#success-criteria","text":"A health check is considered successful if: Connection is established within timeout Response is received within timeout HTTP status code is 2xx or 3xx","title":"Success Criteria"},{"location":"features/health-checks/#failure-handling","text":"When a backend fails: Time 0:00 - Health check fails Time 0:00 - Backend marked unhealthy Time 0:00 - Traffic stops routing to this backend Time 0:10 - Next health check runs Time 0:10 - If successful, backend marked healthy Time 0:10 - Traffic resumes to this backend","title":"Failure Handling"},{"location":"features/health-checks/#health-check-endpoint-requirements","text":"Your backend services should implement a health endpoint that:","title":"Health Check Endpoint Requirements"},{"location":"features/health-checks/#minimal-implementation","text":"// Go example http.HandleFunc(\"/health\", func(w http.ResponseWriter, r *http.Request) { w.WriteHeader(http.StatusOK) w.Write([]byte(`{\"status\":\"healthy\"}`)) }) # Python Flask example @app.route('/health') def health(): return jsonify({\"status\": \"healthy\"}), 200 // Node.js Express example app.get(\"/health\", (req, res) => { res.json({ status: \"healthy\" }); });","title":"Minimal Implementation"},{"location":"features/health-checks/#comprehensive-implementation","text":"A robust health check should verify: func healthHandler(w http.ResponseWriter, r *http.Request) { // Check database connection if err := db.Ping(); err != nil { w.WriteHeader(http.StatusServiceUnavailable) json.NewEncoder(w).Encode(map[string]string{ \"status\": \"unhealthy\", \"error\": \"database unavailable\", }) return } // Check cache connection if err := cache.Ping(); err != nil { w.WriteHeader(http.StatusServiceUnavailable) json.NewEncoder(w).Encode(map[string]string{ \"status\": \"unhealthy\", \"error\": \"cache unavailable\", }) return } // All checks passed w.WriteHeader(http.StatusOK) json.NewEncoder(w).Encode(map[string]string{ \"status\": \"healthy\", }) }","title":"Comprehensive Implementation"},{"location":"features/health-checks/#multiple-upstreams","text":"Configure different health checks per upstream: upstreams: # API service - standard health check - name: api-service targets: - url: http://api-1:3000 - url: http://api-2:3000 health_check: path: /health interval: 10s timeout: 2s # Database proxy - quick checks - name: db-proxy targets: - url: http://db-proxy-1:5432 - url: http://db-proxy-2:5432 health_check: path: /ping interval: 5s timeout: 1s # Slow service - longer timeout - name: report-service targets: - url: http://reports:3000 health_check: path: /healthz interval: 30s timeout: 10s","title":"Multiple Upstreams"},{"location":"features/health-checks/#monitoring-health-status","text":"","title":"Monitoring Health Status"},{"location":"features/health-checks/#metrics","text":"Health status is exposed via Prometheus metrics: curl http://localhost:9090/metrics | grep upstream_healthy Output: # HELP gateway_upstream_healthy Whether upstream is healthy # TYPE gateway_upstream_healthy gauge gateway_upstream_healthy{key=\"api-service_http://backend-1:3000\"} 1 gateway_upstream_healthy{key=\"api-service_http://backend-2:3000\"} 1 gateway_upstream_healthy{key=\"api-service_http://backend-3:3000\"} 0 Values: 1 = Healthy 0 = Unhealthy","title":"Metrics"},{"location":"features/health-checks/#alerting-on-unhealthy-backends","text":"Prometheus alerting rule: groups: - name: relaypoint rules: - alert: BackendUnhealthy expr: gateway_upstream_healthy == 0 for: 1m labels: severity: warning annotations: summary: \"Backend {{ $labels.key }} is unhealthy\" - alert: AllBackendsUnhealthy expr: sum by (upstream) (gateway_upstream_healthy) == 0 for: 30s labels: severity: critical annotations: summary: \"All backends for {{ $labels.upstream }} are unhealthy\"","title":"Alerting on Unhealthy Backends"},{"location":"features/health-checks/#grafana-dashboard","text":"Query examples: # Healthy backend count per upstream sum by (upstream) (gateway_upstream_healthy) # Unhealthy backend count per upstream count by (upstream) (gateway_upstream_healthy == 0) # Health check success rate (requires additional metrics) rate(gateway_health_checks_success_total[5m]) / rate(gateway_health_checks_total[5m])","title":"Grafana Dashboard"},{"location":"features/health-checks/#common-health-check-patterns","text":"","title":"Common Health Check Patterns"},{"location":"features/health-checks/#kubernetes-style-probes","text":"upstreams: - name: k8s-service targets: - url: http://service:3000 health_check: path: /healthz # Kubernetes convention interval: 10s timeout: 2s","title":"Kubernetes-Style Probes"},{"location":"features/health-checks/#liveness-vs-readiness","text":"If your service distinguishes between liveness and readiness: upstreams: # Use readiness for load balancing decisions - name: api-service targets: - url: http://api:3000 health_check: path: /ready # Readiness endpoint interval: 5s timeout: 2s","title":"Liveness vs Readiness"},{"location":"features/health-checks/#deep-health-checks","text":"For critical services, verify dependencies: upstreams: - name: critical-service targets: - url: http://critical:3000 health_check: path: /health/deep # Checks all dependencies interval: 30s # Longer interval (expensive check) timeout: 10s # Longer timeout (multiple checks)","title":"Deep Health Checks"},{"location":"features/health-checks/#failure-scenarios","text":"","title":"Failure Scenarios"},{"location":"features/health-checks/#single-backend-failure","text":"Before: Backend 1: \u2713 Healthy \u2190\u2500\u2510 Backend 2: \u2713 Healthy \u2190\u2500\u253c\u2500\u2500 Round Robin Backend 3: \u2713 Healthy \u2190\u2500\u2518 After Backend 2 fails: Backend 1: \u2713 Healthy \u2190\u2500\u2510 Backend 2: \u2717 Unhealthy \u2502 Round Robin Backend 3: \u2713 Healthy \u2190\u2500\u2518 Traffic automatically routes only to healthy backends.","title":"Single Backend Failure"},{"location":"features/health-checks/#all-backends-fail","text":"When all backends are unhealthy, Relaypoint: Logs a warning Returns 503 Service Unavailable to clients Continues health checking Resumes traffic when any backend recovers","title":"All Backends Fail"},{"location":"features/health-checks/#backend-recovery","text":"Time 0:00 - Backend 2 recovers Time 0:05 - Health check runs (interval = 5s) Time 0:05 - Health check succeeds Time 0:05 - Backend 2 marked healthy Time 0:05 - Traffic resumes to Backend 2","title":"Backend Recovery"},{"location":"features/health-checks/#best-practices","text":"","title":"Best Practices"},{"location":"features/health-checks/#1-keep-health-checks-lightweight","text":"// Good: Fast, simple check func health(w http.ResponseWriter, r *http.Request) { w.WriteHeader(http.StatusOK) } // Bad: Slow, complex check on health endpoint func health(w http.ResponseWriter, r *http.Request) { result := runExpensiveQuery() // Don't do this // ... }","title":"1. Keep Health Checks Lightweight"},{"location":"features/health-checks/#2-use-appropriate-intervals","text":"Service Type Recommended Interval High-availability API 5s Standard API 10s Background service 30s Batch processing 60s","title":"2. Use Appropriate Intervals"},{"location":"features/health-checks/#3-set-reasonable-timeouts","text":"# Good: Timeout < Interval health_check: interval: 10s timeout: 2s # Bad: Timeout >= Interval health_check: interval: 10s timeout: 15s # Health checks will overlap!","title":"3. Set Reasonable Timeouts"},{"location":"features/health-checks/#4-match-health-check-to-service-capabilities","text":"# Fast service - quick checks - name: cache health_check: path: /ping interval: 5s timeout: 500ms # Slow service - longer checks - name: analytics health_check: path: /health interval: 60s timeout: 10s","title":"4. Match Health Check to Service Capabilities"},{"location":"features/health-checks/#5-use-dedicated-health-endpoints","text":"Don't use regular API endpoints for health checks: # Good: Dedicated health endpoint health_check: path: /health # Bad: Using regular endpoint health_check: path: /api/users # May be slow, rate-limited, auth-required","title":"5. Use Dedicated Health Endpoints"},{"location":"features/health-checks/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"features/health-checks/#backend-marked-unhealthy-incorrectly","text":"Symptoms : Working backend shows as unhealthy. Causes : Health endpoint returns non-2xx status Health endpoint too slow (timeout) Network connectivity issues Health endpoint requires authentication Solutions : Test health endpoint manually: curl -v http://backend:3000/health Increase timeout if endpoint is slow Check network/firewall between Relaypoint and backend Ensure health endpoint doesn't require auth","title":"Backend Marked Unhealthy Incorrectly"},{"location":"features/health-checks/#health-checks-overloading-backend","text":"Symptoms : High CPU/memory from health checks. Causes : Interval too short Health check doing expensive operations Solutions : Increase interval (e.g., 5s \u2192 30s) Optimize health check implementation Use simple ping endpoint instead of full health check","title":"Health Checks Overloading Backend"},{"location":"features/health-checks/#flapping-health-status","text":"Symptoms : Backend alternates between healthy/unhealthy. Causes : Intermittent network issues Backend at capacity Timeout too aggressive Solutions : Investigate network stability Scale backend or reduce traffic Increase timeout","title":"Flapping Health Status"},{"location":"features/health-checks/#gateway-health-endpoint","text":"Relaypoint exposes its own health endpoint: curl http://localhost:8080/health Response: { \"status\": \"healthy\" } This endpoint confirms Relaypoint itself is running, independent of backend health.","title":"Gateway Health Endpoint"},{"location":"features/health-checks/#next-steps","text":"Load Balancing - How health affects load balancing Metrics - Monitor health status Troubleshooting - Debug health issues","title":"Next Steps"},{"location":"features/load-balancing/","text":"Relaypoint provides multiple load balancing strategies to distribute traffic across your backend servers efficiently. Overview Load balancing ensures high availability and optimal performance by distributing requests across multiple backend instances. Relaypoint supports four load balancing strategies: Strategy Best For Description round_robin General use Equal distribution across all backends least_conn Variable workloads Routes to server with fewest active connections random Simple distribution Random server selection weighted_round_robin Heterogeneous servers Distribution based on server capacity Round Robin (Default) Distributes requests evenly across all healthy backends in order. upstreams: - name: api-service targets: - url: http://backend-1:3000 - url: http://backend-2:3000 - url: http://backend-3:3000 load_balance: round_robin How It Works Request 1 \u2192 Backend 1 Request 2 \u2192 Backend 2 Request 3 \u2192 Backend 3 Request 4 \u2192 Backend 1 Request 5 \u2192 Backend 2 ... When to Use All backend servers have similar capacity Request processing time is consistent Simple, predictable distribution is desired Advantages Simple and predictable Equal distribution of requests No additional tracking overhead Disadvantages Doesn't account for server load May overload slower servers Doesn't consider request complexity Least Connections Routes requests to the backend with the fewest active connections. upstreams: - name: api-service targets: - url: http://backend-1:3000 - url: http://backend-2:3000 - url: http://backend-3:3000 load_balance: least_conn How It Works Backend 1: 5 connections \u2190 New request goes here (lowest) Backend 2: 8 connections Backend 3: 12 connections When to Use Requests have variable processing times Some requests are more resource-intensive Backend servers have similar capacity Advantages Adapts to actual server load Better handling of slow requests Prevents overloading busy servers Disadvantages Slight overhead for connection tracking New servers may be overwhelmed initially Doesn't account for server capacity Random Selects a random healthy backend for each request. upstreams: - name: api-service targets: - url: http://backend-1:3000 - url: http://backend-2:3000 - url: http://backend-3:3000 load_balance: random How It Works Each request is routed to a randomly selected healthy backend. When to Use Simple load distribution without state Testing and development environments When statistical distribution is acceptable Advantages No coordination needed Simple implementation Works well at scale Disadvantages Distribution may be uneven short-term No consideration of server load May cause hot spots temporarily Weighted Round Robin Distributes requests based on assigned weights, allowing more powerful servers to handle more traffic. upstreams: - name: api-service targets: - url: http://large-server:3000 weight: 5 # Handles 5x more traffic - url: http://medium-server:3000 weight: 3 # Handles 3x more traffic - url: http://small-server:3000 weight: 1 # Baseline load_balance: weighted_round_robin How It Works With weights 5:3:1, over 9 requests: Requests 1-5 \u2192 large-server (weight 5) Requests 6-8 \u2192 medium-server (weight 3) Request 9 \u2192 small-server (weight 1) When to Use Backend servers have different capacities Migrating traffic gradually between versions Implementing canary deployments Advantages Accounts for server capacity differences Fine-grained traffic control Useful for gradual rollouts Disadvantages Requires knowledge of server capacity More complex configuration Weights need adjustment as capacity changes Configuring Multiple Upstreams Different services can use different strategies: upstreams: # High-throughput API - use round robin - name: api-service targets: - url: http://api-1:3000 - url: http://api-2:3000 - url: http://api-3:3000 load_balance: round_robin # Long-running operations - use least connections - name: processing-service targets: - url: http://processor-1:3000 - url: http://processor-2:3000 load_balance: least_conn # Mixed capacity cluster - use weighted - name: compute-service targets: - url: http://compute-large:3000 weight: 4 - url: http://compute-small:3000 weight: 1 load_balance: weighted_round_robin Health-Aware Load Balancing All load balancing strategies automatically skip unhealthy backends: upstreams: - name: api-service targets: - url: http://backend-1:3000 - url: http://backend-2:3000 - url: http://backend-3:3000 load_balance: round_robin health_check: path: /health interval: 10s timeout: 2s When a backend fails health checks: Before failure: Request 1 \u2192 Backend 1 Request 2 \u2192 Backend 2 Request 3 \u2192 Backend 3 After Backend 2 fails: Request 1 \u2192 Backend 1 Request 2 \u2192 Backend 3 # Backend 2 skipped Request 3 \u2192 Backend 1 See Health Checks for detailed configuration. Connection Tracking For least_conn strategy, Relaypoint tracks active connections per backend: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Relaypoint \u2502 \u2502 \u2502 \u2502 Backend 1: 5 \u2502\u2500\u2500\u2192 Backend 1 (5 active) \u2502 Backend 2: 3 \u2502\u2500\u2500\u2192 Backend 2 (3 active) \u2502 Backend 3: 8 \u2502\u2500\u2500\u2192 Backend 3 (8 active) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Connections are: Incremented when a request starts Decremented when the response completes (or fails) Choosing a Strategy Use this decision tree: Are your servers different capacities? \u251c\u2500 Yes \u2192 Use weighted_round_robin \u2514\u2500 No \u2502 Do requests have variable processing times? \u251c\u2500 Yes \u2192 Use least_conn \u2514\u2500 No \u2502 Need predictable distribution? \u251c\u2500 Yes \u2192 Use round_robin \u2514\u2500 No \u2192 Use random Strategy Comparison Scenario Recommended Strategy Identical servers, consistent requests round_robin Identical servers, variable requests least_conn Mixed server capacities weighted_round_robin Canary deployment (90/10 split) weighted_round_robin with weights 9:1 Simple setup, stateless random Database read replicas least_conn Static file servers round_robin API with some slow endpoints least_conn Canary Deployments Use weighted round robin for gradual rollouts: # Stage 1: 10% traffic to new version upstreams: - name: api-service targets: - url: http://api-v1:3000 weight: 9 - url: http://api-v2:3000 # New version weight: 1 load_balance: weighted_round_robin # Stage 2: 50% traffic to new version upstreams: - name: api-service targets: - url: http://api-v1:3000 weight: 1 - url: http://api-v2:3000 weight: 1 load_balance: weighted_round_robin # Stage 3: 100% traffic to new version upstreams: - name: api-service targets: - url: http://api-v2:3000 load_balance: round_robin Blue-Green Deployments Switch traffic between environments: # Blue environment active upstreams: - name: api-service targets: - url: http://blue-1:3000 - url: http://blue-2:3000 load_balance: round_robin # Switch to green (update config and reload) upstreams: - name: api-service targets: - url: http://green-1:3000 - url: http://green-2:3000 load_balance: round_robin Monitoring Load Balancing Metrics Relaypoint exposes metrics for monitoring load distribution: curl http://localhost:9090/metrics | grep upstream Key metrics: gateway_requests_total{upstream=\"...\"} - Requests per upstream gateway_upstream_healthy{upstream=\"...\",target=\"...\"} - Backend health status gateway_request_duration_seconds{upstream=\"...\"} - Latency per upstream Stats Endpoint curl http://localhost:8080/stats Returns request counts and latency percentiles per route. Troubleshooting Uneven Distribution Symptom : One backend receives significantly more traffic. Causes : Using round_robin with backends of different speeds Health check failures on some backends Connection pooling affecting least_conn Solutions : Switch to least_conn for variable workloads Check health check logs Review backend logs for errors Backend Overload Symptom : Specific backend becomes overloaded. Causes : Weight configuration too high Backend slower than expected Health checks not detecting issues Solutions : Adjust weights Use least_conn strategy Tune health check intervals All Traffic to One Backend Symptom : Other backends receive no traffic. Causes : Other backends marked unhealthy Configuration error in targets DNS resolution issues Solutions : Check health endpoint on backends Verify target URLs Test direct backend connectivity Next Steps Health Checks - Configure backend health monitoring Rate Limiting - Protect your APIs Metrics - Monitor your gateway","title":"Load Balancing"},{"location":"features/load-balancing/#overview","text":"Load balancing ensures high availability and optimal performance by distributing requests across multiple backend instances. Relaypoint supports four load balancing strategies: Strategy Best For Description round_robin General use Equal distribution across all backends least_conn Variable workloads Routes to server with fewest active connections random Simple distribution Random server selection weighted_round_robin Heterogeneous servers Distribution based on server capacity","title":"Overview"},{"location":"features/load-balancing/#round-robin-default","text":"Distributes requests evenly across all healthy backends in order. upstreams: - name: api-service targets: - url: http://backend-1:3000 - url: http://backend-2:3000 - url: http://backend-3:3000 load_balance: round_robin","title":"Round Robin (Default)"},{"location":"features/load-balancing/#how-it-works","text":"Request 1 \u2192 Backend 1 Request 2 \u2192 Backend 2 Request 3 \u2192 Backend 3 Request 4 \u2192 Backend 1 Request 5 \u2192 Backend 2 ...","title":"How It Works"},{"location":"features/load-balancing/#when-to-use","text":"All backend servers have similar capacity Request processing time is consistent Simple, predictable distribution is desired","title":"When to Use"},{"location":"features/load-balancing/#advantages","text":"Simple and predictable Equal distribution of requests No additional tracking overhead","title":"Advantages"},{"location":"features/load-balancing/#disadvantages","text":"Doesn't account for server load May overload slower servers Doesn't consider request complexity","title":"Disadvantages"},{"location":"features/load-balancing/#least-connections","text":"Routes requests to the backend with the fewest active connections. upstreams: - name: api-service targets: - url: http://backend-1:3000 - url: http://backend-2:3000 - url: http://backend-3:3000 load_balance: least_conn","title":"Least Connections"},{"location":"features/load-balancing/#how-it-works_1","text":"Backend 1: 5 connections \u2190 New request goes here (lowest) Backend 2: 8 connections Backend 3: 12 connections","title":"How It Works"},{"location":"features/load-balancing/#when-to-use_1","text":"Requests have variable processing times Some requests are more resource-intensive Backend servers have similar capacity","title":"When to Use"},{"location":"features/load-balancing/#advantages_1","text":"Adapts to actual server load Better handling of slow requests Prevents overloading busy servers","title":"Advantages"},{"location":"features/load-balancing/#disadvantages_1","text":"Slight overhead for connection tracking New servers may be overwhelmed initially Doesn't account for server capacity","title":"Disadvantages"},{"location":"features/load-balancing/#random","text":"Selects a random healthy backend for each request. upstreams: - name: api-service targets: - url: http://backend-1:3000 - url: http://backend-2:3000 - url: http://backend-3:3000 load_balance: random","title":"Random"},{"location":"features/load-balancing/#how-it-works_2","text":"Each request is routed to a randomly selected healthy backend.","title":"How It Works"},{"location":"features/load-balancing/#when-to-use_2","text":"Simple load distribution without state Testing and development environments When statistical distribution is acceptable","title":"When to Use"},{"location":"features/load-balancing/#advantages_2","text":"No coordination needed Simple implementation Works well at scale","title":"Advantages"},{"location":"features/load-balancing/#disadvantages_2","text":"Distribution may be uneven short-term No consideration of server load May cause hot spots temporarily","title":"Disadvantages"},{"location":"features/load-balancing/#weighted-round-robin","text":"Distributes requests based on assigned weights, allowing more powerful servers to handle more traffic. upstreams: - name: api-service targets: - url: http://large-server:3000 weight: 5 # Handles 5x more traffic - url: http://medium-server:3000 weight: 3 # Handles 3x more traffic - url: http://small-server:3000 weight: 1 # Baseline load_balance: weighted_round_robin","title":"Weighted Round Robin"},{"location":"features/load-balancing/#how-it-works_3","text":"With weights 5:3:1, over 9 requests: Requests 1-5 \u2192 large-server (weight 5) Requests 6-8 \u2192 medium-server (weight 3) Request 9 \u2192 small-server (weight 1)","title":"How It Works"},{"location":"features/load-balancing/#when-to-use_3","text":"Backend servers have different capacities Migrating traffic gradually between versions Implementing canary deployments","title":"When to Use"},{"location":"features/load-balancing/#advantages_3","text":"Accounts for server capacity differences Fine-grained traffic control Useful for gradual rollouts","title":"Advantages"},{"location":"features/load-balancing/#disadvantages_3","text":"Requires knowledge of server capacity More complex configuration Weights need adjustment as capacity changes","title":"Disadvantages"},{"location":"features/load-balancing/#configuring-multiple-upstreams","text":"Different services can use different strategies: upstreams: # High-throughput API - use round robin - name: api-service targets: - url: http://api-1:3000 - url: http://api-2:3000 - url: http://api-3:3000 load_balance: round_robin # Long-running operations - use least connections - name: processing-service targets: - url: http://processor-1:3000 - url: http://processor-2:3000 load_balance: least_conn # Mixed capacity cluster - use weighted - name: compute-service targets: - url: http://compute-large:3000 weight: 4 - url: http://compute-small:3000 weight: 1 load_balance: weighted_round_robin","title":"Configuring Multiple Upstreams"},{"location":"features/load-balancing/#health-aware-load-balancing","text":"All load balancing strategies automatically skip unhealthy backends: upstreams: - name: api-service targets: - url: http://backend-1:3000 - url: http://backend-2:3000 - url: http://backend-3:3000 load_balance: round_robin health_check: path: /health interval: 10s timeout: 2s When a backend fails health checks: Before failure: Request 1 \u2192 Backend 1 Request 2 \u2192 Backend 2 Request 3 \u2192 Backend 3 After Backend 2 fails: Request 1 \u2192 Backend 1 Request 2 \u2192 Backend 3 # Backend 2 skipped Request 3 \u2192 Backend 1 See Health Checks for detailed configuration.","title":"Health-Aware Load Balancing"},{"location":"features/load-balancing/#connection-tracking","text":"For least_conn strategy, Relaypoint tracks active connections per backend: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Relaypoint \u2502 \u2502 \u2502 \u2502 Backend 1: 5 \u2502\u2500\u2500\u2192 Backend 1 (5 active) \u2502 Backend 2: 3 \u2502\u2500\u2500\u2192 Backend 2 (3 active) \u2502 Backend 3: 8 \u2502\u2500\u2500\u2192 Backend 3 (8 active) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Connections are: Incremented when a request starts Decremented when the response completes (or fails)","title":"Connection Tracking"},{"location":"features/load-balancing/#choosing-a-strategy","text":"Use this decision tree: Are your servers different capacities? \u251c\u2500 Yes \u2192 Use weighted_round_robin \u2514\u2500 No \u2502 Do requests have variable processing times? \u251c\u2500 Yes \u2192 Use least_conn \u2514\u2500 No \u2502 Need predictable distribution? \u251c\u2500 Yes \u2192 Use round_robin \u2514\u2500 No \u2192 Use random","title":"Choosing a Strategy"},{"location":"features/load-balancing/#strategy-comparison","text":"Scenario Recommended Strategy Identical servers, consistent requests round_robin Identical servers, variable requests least_conn Mixed server capacities weighted_round_robin Canary deployment (90/10 split) weighted_round_robin with weights 9:1 Simple setup, stateless random Database read replicas least_conn Static file servers round_robin API with some slow endpoints least_conn","title":"Strategy Comparison"},{"location":"features/load-balancing/#canary-deployments","text":"Use weighted round robin for gradual rollouts: # Stage 1: 10% traffic to new version upstreams: - name: api-service targets: - url: http://api-v1:3000 weight: 9 - url: http://api-v2:3000 # New version weight: 1 load_balance: weighted_round_robin # Stage 2: 50% traffic to new version upstreams: - name: api-service targets: - url: http://api-v1:3000 weight: 1 - url: http://api-v2:3000 weight: 1 load_balance: weighted_round_robin # Stage 3: 100% traffic to new version upstreams: - name: api-service targets: - url: http://api-v2:3000 load_balance: round_robin","title":"Canary Deployments"},{"location":"features/load-balancing/#blue-green-deployments","text":"Switch traffic between environments: # Blue environment active upstreams: - name: api-service targets: - url: http://blue-1:3000 - url: http://blue-2:3000 load_balance: round_robin # Switch to green (update config and reload) upstreams: - name: api-service targets: - url: http://green-1:3000 - url: http://green-2:3000 load_balance: round_robin","title":"Blue-Green Deployments"},{"location":"features/load-balancing/#monitoring-load-balancing","text":"","title":"Monitoring Load Balancing"},{"location":"features/load-balancing/#metrics","text":"Relaypoint exposes metrics for monitoring load distribution: curl http://localhost:9090/metrics | grep upstream Key metrics: gateway_requests_total{upstream=\"...\"} - Requests per upstream gateway_upstream_healthy{upstream=\"...\",target=\"...\"} - Backend health status gateway_request_duration_seconds{upstream=\"...\"} - Latency per upstream","title":"Metrics"},{"location":"features/load-balancing/#stats-endpoint","text":"curl http://localhost:8080/stats Returns request counts and latency percentiles per route.","title":"Stats Endpoint"},{"location":"features/load-balancing/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"features/load-balancing/#uneven-distribution","text":"Symptom : One backend receives significantly more traffic. Causes : Using round_robin with backends of different speeds Health check failures on some backends Connection pooling affecting least_conn Solutions : Switch to least_conn for variable workloads Check health check logs Review backend logs for errors","title":"Uneven Distribution"},{"location":"features/load-balancing/#backend-overload","text":"Symptom : Specific backend becomes overloaded. Causes : Weight configuration too high Backend slower than expected Health checks not detecting issues Solutions : Adjust weights Use least_conn strategy Tune health check intervals","title":"Backend Overload"},{"location":"features/load-balancing/#all-traffic-to-one-backend","text":"Symptom : Other backends receive no traffic. Causes : Other backends marked unhealthy Configuration error in targets DNS resolution issues Solutions : Check health endpoint on backends Verify target URLs Test direct backend connectivity","title":"All Traffic to One Backend"},{"location":"features/load-balancing/#next-steps","text":"Health Checks - Configure backend health monitoring Rate Limiting - Protect your APIs Metrics - Monitor your gateway","title":"Next Steps"},{"location":"features/metrics/","text":"Relaypoint provides comprehensive observability through Prometheus metrics and a JSON stats endpoint. Overview Relaypoint exposes: Prometheus metrics at /metrics (default port 9090) JSON stats at /stats (on the main gateway port) Health endpoint at /health (on the main gateway port) Enabling Metrics metrics: enabled: true # Enable metrics (default: true) port: 9090 # Metrics server port (default: 9090) path: \"/metrics\" # Metrics endpoint path (default: /metrics) latency_buckets: # Optional: Custom histogram buckets - 0.005 - 0.01 - 0.025 - 0.05 - 0.1 - 0.25 - 0.5 - 1.0 - 2.5 - 5.0 - 10.0 Available Metrics Request Metrics gateway_requests_total Total number of requests processed. Label Description key Format: {route}_{method}_{status_code} # Total requests sum(gateway_requests_total) # Requests by route sum by (key) (gateway_requests_total) # Request rate (requests/second) rate(gateway_requests_total[5m]) # Requests by status code sum by (status) (gateway_requests_total{key=~\".*_200\"}) gateway_request_duration_seconds Request latency histogram. Label Description key Format: {route}_{method} # Average latency rate(gateway_request_duration_seconds_sum[5m]) / rate(gateway_request_duration_seconds_count[5m]) # 95th percentile latency histogram_quantile(0.95, rate(gateway_request_duration_seconds_bucket[5m])) # 99th percentile latency histogram_quantile(0.99, rate(gateway_request_duration_seconds_bucket[5m])) gateway_requests_in_flight Current number of requests being processed. Label Description key Route name or pattern # Current in-flight requests gateway_requests_in_flight # Total in-flight requests sum(gateway_requests_in_flight) Error Metrics gateway_errors_total Total number of errors. Label Description key Format: {route}_{error_type} Error types: not_found - Route not matched upstream_not_found - Upstream not configured no_healthy_upstream - All backends unhealthy proxy_error - Error proxying to backend # Total errors sum(gateway_errors_total) # Error rate rate(gateway_errors_total[5m]) # Errors by type sum by (key) (gateway_errors_total) Rate Limiting Metrics gateway_rate_limit_hits_total Total number of rate-limited requests. Label Description key Format: {route}_{limit_type} Limit types: route - Route-level rate limit apikey - API key rate limit ip - Per-IP rate limit # Total rate limit hits sum(gateway_rate_limit_hits_total) # Rate limit hits per minute rate(gateway_rate_limit_hits_total[1m]) * 60 # Rate limits by type sum by (key) (gateway_rate_limit_hits_total) API Key Metrics gateway_api_key_requests_total Requests per API key. Label Description key Format: {api_key_name}_{status_code} # Requests by API key sum by (key) (gateway_api_key_requests_total) # Top API keys by request volume topk(10, sum by (key) (rate(gateway_api_key_requests_total[5m]))) Upstream Health Metrics gateway_upstream_healthy Backend health status gauge. Label Description key Format: {upstream}_{target_url} Values: 1 = Healthy 0 = Unhealthy # All backend health status gateway_upstream_healthy # Unhealthy backends gateway_upstream_healthy == 0 # Healthy backend count per upstream sum by (upstream) (gateway_upstream_healthy) JSON Stats Endpoint The /stats endpoint provides real-time statistics in JSON format: curl http://localhost:8080/stats Response: [ { \"key\": \"users-api\", \"request_count\": 15234, \"error_count\": 12, \"p50_latency_ms\": 5.2, \"p90_latency_ms\": 12.8, \"p99_latency_ms\": 45.3 }, { \"key\": \"orders-api\", \"request_count\": 8432, \"error_count\": 3, \"p50_latency_ms\": 8.1, \"p90_latency_ms\": 22.4, \"p99_latency_ms\": 89.7 } ] Prometheus Integration Scrape Configuration Add to your prometheus.yml : scrape_configs: - job_name: \"relaypoint\" static_configs: - targets: [\"relaypoint:9090\"] scrape_interval: 15s metrics_path: /metrics Kubernetes ServiceMonitor apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: relaypoint labels: app: relaypoint spec: selector: matchLabels: app: relaypoint endpoints: - port: metrics interval: 15s path: /metrics Grafana Dashboards Overview Dashboard Create panels for: Request Rate sum(rate(gateway_requests_total[5m])) Error Rate sum(rate(gateway_errors_total[5m])) / sum(rate(gateway_requests_total[5m])) * 100 Latency (p50, p90, p99) histogram_quantile(0.50, sum(rate(gateway_request_duration_seconds_bucket[5m])) by (le)) histogram_quantile(0.90, sum(rate(gateway_request_duration_seconds_bucket[5m])) by (le)) histogram_quantile(0.99, sum(rate(gateway_request_duration_seconds_bucket[5m])) by (le)) Requests In Flight sum(gateway_requests_in_flight) Backend Health sum(gateway_upstream_healthy) Rate Limit Hits sum(rate(gateway_rate_limit_hits_total[5m])) Sample Dashboard JSON { \"title\": \"Relaypoint Gateway\", \"panels\": [ { \"title\": \"Request Rate\", \"type\": \"graph\", \"targets\": [ { \"expr\": \"sum(rate(gateway_requests_total[5m]))\", \"legendFormat\": \"Requests/sec\" } ] }, { \"title\": \"Latency Percentiles\", \"type\": \"graph\", \"targets\": [ { \"expr\": \"histogram_quantile(0.50, sum(rate(gateway_request_duration_seconds_bucket[5m])) by (le))\", \"legendFormat\": \"p50\" }, { \"expr\": \"histogram_quantile(0.90, sum(rate(gateway_request_duration_seconds_bucket[5m])) by (le))\", \"legendFormat\": \"p90\" }, { \"expr\": \"histogram_quantile(0.99, sum(rate(gateway_request_duration_seconds_bucket[5m])) by (le))\", \"legendFormat\": \"p99\" } ] }, { \"title\": \"Backend Health\", \"type\": \"stat\", \"targets\": [ { \"expr\": \"sum(gateway_upstream_healthy)\", \"legendFormat\": \"Healthy Backends\" } ] } ] } Alerting Rules Prometheus Alerting Rules # alerts.yml groups: - name: relaypoint rules: # High error rate - alert: HighErrorRate expr: | sum(rate(gateway_errors_total[5m])) / sum(rate(gateway_requests_total[5m])) > 0.05 for: 5m labels: severity: warning annotations: summary: \"High error rate (> 5%)\" description: \"Error rate is {{ $value | humanizePercentage }}\" # Very high error rate - alert: CriticalErrorRate expr: | sum(rate(gateway_errors_total[5m])) / sum(rate(gateway_requests_total[5m])) > 0.25 for: 2m labels: severity: critical annotations: summary: \"Critical error rate (> 25%)\" # High latency - alert: HighLatency expr: | histogram_quantile(0.95, sum(rate(gateway_request_duration_seconds_bucket[5m])) by (le) ) > 1 for: 5m labels: severity: warning annotations: summary: \"High p95 latency (> 1s)\" description: \"p95 latency is {{ $value | humanizeDuration }}\" # Backend unhealthy - alert: BackendUnhealthy expr: gateway_upstream_healthy == 0 for: 1m labels: severity: warning annotations: summary: \"Backend is unhealthy\" description: \"Backend {{ $labels.key }} is down\" # All backends unhealthy - alert: AllBackendsDown expr: sum by (upstream) (gateway_upstream_healthy) == 0 for: 30s labels: severity: critical annotations: summary: \"All backends for {{ $labels.upstream }} are down\" # High rate limiting - alert: HighRateLimiting expr: | sum(rate(gateway_rate_limit_hits_total[5m])) / sum(rate(gateway_requests_total[5m])) > 0.1 for: 5m labels: severity: warning annotations: summary: \"High rate limiting (> 10% of requests)\" # No requests (service might be down) - alert: NoRequests expr: sum(rate(gateway_requests_total[5m])) == 0 for: 5m labels: severity: warning annotations: summary: \"No requests received in 5 minutes\" Logging Relaypoint outputs structured JSON logs: {\"level\":\"INFO\",\"msg\":\"Starting RelayPoint\",\"config\":\"relaypoint.yml\"} {\"level\":\"INFO\",\"msg\":\"configuration loaded\",\"routes\":5,\"upstreams\":3} {\"level\":\"INFO\",\"msg\":\"relaypoint API Gateway starting\",\"address\":\"0.0.0.0:8080\"} {\"level\":\"WARN\",\"msg\":\"upstream unhealthy\",\"upstream\":\"api-service\",\"target\":\"http://backend-2:3000\"} Log Levels Level Description INFO Normal operational messages WARN Warning conditions (e.g., unhealthy backend) ERROR Error conditions (e.g., configuration errors) Log Aggregation Forward logs to your preferred system: # Send to file ./relaypoint -config relaypoint.yml 2>&1 | tee /var/log/relaypoint.log # Send to journald (systemd) ./relaypoint -config relaypoint.yml # Parse with jq ./relaypoint -config relaypoint.yml 2>&1 | jq -r '.msg' Health Endpoint Check gateway health: curl http://localhost:8080/health Response: { \"status\": \"healthy\" } Use this for: Load balancer health checks Kubernetes liveness probes Monitoring systems Example Monitoring Stack Docker Compose Setup version: \"3.8\" services: relaypoint: image: ghcr.io/relaypoint/relaypoint:latest ports: - \"8080:8080\" - \"9090:9090\" volumes: - ./relaypoint.yml:/etc/relaypoint/relaypoint.yml command: [\"-config\", \"/etc/relaypoint/relaypoint.yml\"] prometheus: image: prom/prometheus:latest ports: - \"9091:9090\" volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml - ./alerts.yml:/etc/prometheus/alerts.yml grafana: image: grafana/grafana:latest ports: - \"3000:3000\" environment: - GF_SECURITY_ADMIN_PASSWORD=admin volumes: - grafana-data:/var/lib/grafana volumes: grafana-data: prometheus.yml global: scrape_interval: 15s rule_files: - /etc/prometheus/alerts.yml scrape_configs: - job_name: \"relaypoint\" static_configs: - targets: [\"relaypoint:9090\"] alerting: alertmanagers: - static_configs: - targets: [\"alertmanager:9093\"] Custom Latency Buckets Configure histogram buckets for your latency profile: metrics: enabled: true latency_buckets: # For low-latency APIs (sub-100ms expected) - 0.001 # 1ms - 0.005 # 5ms - 0.01 # 10ms - 0.025 # 25ms - 0.05 # 50ms - 0.1 # 100ms - 0.25 # 250ms - 0.5 # 500ms - 1.0 # 1s metrics: enabled: true latency_buckets: # For slower APIs (seconds expected) - 0.1 # 100ms - 0.5 # 500ms - 1.0 # 1s - 2.5 # 2.5s - 5.0 # 5s - 10.0 # 10s - 30.0 # 30s - 60.0 # 60s Best Practices 1. Set Up Dashboards First Before deploying to production, have dashboards ready for: Request rate and error rate Latency percentiles Backend health Rate limiting 2. Configure Alerts Essential alerts: High error rate (> 5%) High latency (p95 > threshold) Backend unhealthy No traffic (might indicate gateway down) 3. Use Labels Wisely The default labels are designed for cardinality control. Avoid adding high-cardinality labels (like user IDs) to metrics. 4. Monitor the Monitor Ensure Prometheus can reach the metrics endpoint: curl http://localhost:9090/metrics 5. Retention Planning Consider metric retention based on: Dashboard time ranges needed Storage capacity Compliance requirements Next Steps Health Checks - Monitor backend health Troubleshooting - Debug issues using metrics Best Practices - Production recommendations","title":"Metrics"},{"location":"features/metrics/#overview","text":"Relaypoint exposes: Prometheus metrics at /metrics (default port 9090) JSON stats at /stats (on the main gateway port) Health endpoint at /health (on the main gateway port)","title":"Overview"},{"location":"features/metrics/#enabling-metrics","text":"metrics: enabled: true # Enable metrics (default: true) port: 9090 # Metrics server port (default: 9090) path: \"/metrics\" # Metrics endpoint path (default: /metrics) latency_buckets: # Optional: Custom histogram buckets - 0.005 - 0.01 - 0.025 - 0.05 - 0.1 - 0.25 - 0.5 - 1.0 - 2.5 - 5.0 - 10.0","title":"Enabling Metrics"},{"location":"features/metrics/#available-metrics","text":"","title":"Available Metrics"},{"location":"features/metrics/#request-metrics","text":"","title":"Request Metrics"},{"location":"features/metrics/#gateway_requests_total","text":"Total number of requests processed. Label Description key Format: {route}_{method}_{status_code} # Total requests sum(gateway_requests_total) # Requests by route sum by (key) (gateway_requests_total) # Request rate (requests/second) rate(gateway_requests_total[5m]) # Requests by status code sum by (status) (gateway_requests_total{key=~\".*_200\"})","title":"gateway_requests_total"},{"location":"features/metrics/#gateway_request_duration_seconds","text":"Request latency histogram. Label Description key Format: {route}_{method} # Average latency rate(gateway_request_duration_seconds_sum[5m]) / rate(gateway_request_duration_seconds_count[5m]) # 95th percentile latency histogram_quantile(0.95, rate(gateway_request_duration_seconds_bucket[5m])) # 99th percentile latency histogram_quantile(0.99, rate(gateway_request_duration_seconds_bucket[5m]))","title":"gateway_request_duration_seconds"},{"location":"features/metrics/#gateway_requests_in_flight","text":"Current number of requests being processed. Label Description key Route name or pattern # Current in-flight requests gateway_requests_in_flight # Total in-flight requests sum(gateway_requests_in_flight)","title":"gateway_requests_in_flight"},{"location":"features/metrics/#error-metrics","text":"","title":"Error Metrics"},{"location":"features/metrics/#gateway_errors_total","text":"Total number of errors. Label Description key Format: {route}_{error_type} Error types: not_found - Route not matched upstream_not_found - Upstream not configured no_healthy_upstream - All backends unhealthy proxy_error - Error proxying to backend # Total errors sum(gateway_errors_total) # Error rate rate(gateway_errors_total[5m]) # Errors by type sum by (key) (gateway_errors_total)","title":"gateway_errors_total"},{"location":"features/metrics/#rate-limiting-metrics","text":"","title":"Rate Limiting Metrics"},{"location":"features/metrics/#gateway_rate_limit_hits_total","text":"Total number of rate-limited requests. Label Description key Format: {route}_{limit_type} Limit types: route - Route-level rate limit apikey - API key rate limit ip - Per-IP rate limit # Total rate limit hits sum(gateway_rate_limit_hits_total) # Rate limit hits per minute rate(gateway_rate_limit_hits_total[1m]) * 60 # Rate limits by type sum by (key) (gateway_rate_limit_hits_total)","title":"gateway_rate_limit_hits_total"},{"location":"features/metrics/#api-key-metrics","text":"","title":"API Key Metrics"},{"location":"features/metrics/#gateway_api_key_requests_total","text":"Requests per API key. Label Description key Format: {api_key_name}_{status_code} # Requests by API key sum by (key) (gateway_api_key_requests_total) # Top API keys by request volume topk(10, sum by (key) (rate(gateway_api_key_requests_total[5m])))","title":"gateway_api_key_requests_total"},{"location":"features/metrics/#upstream-health-metrics","text":"","title":"Upstream Health Metrics"},{"location":"features/metrics/#gateway_upstream_healthy","text":"Backend health status gauge. Label Description key Format: {upstream}_{target_url} Values: 1 = Healthy 0 = Unhealthy # All backend health status gateway_upstream_healthy # Unhealthy backends gateway_upstream_healthy == 0 # Healthy backend count per upstream sum by (upstream) (gateway_upstream_healthy)","title":"gateway_upstream_healthy"},{"location":"features/metrics/#json-stats-endpoint","text":"The /stats endpoint provides real-time statistics in JSON format: curl http://localhost:8080/stats Response: [ { \"key\": \"users-api\", \"request_count\": 15234, \"error_count\": 12, \"p50_latency_ms\": 5.2, \"p90_latency_ms\": 12.8, \"p99_latency_ms\": 45.3 }, { \"key\": \"orders-api\", \"request_count\": 8432, \"error_count\": 3, \"p50_latency_ms\": 8.1, \"p90_latency_ms\": 22.4, \"p99_latency_ms\": 89.7 } ]","title":"JSON Stats Endpoint"},{"location":"features/metrics/#prometheus-integration","text":"","title":"Prometheus Integration"},{"location":"features/metrics/#scrape-configuration","text":"Add to your prometheus.yml : scrape_configs: - job_name: \"relaypoint\" static_configs: - targets: [\"relaypoint:9090\"] scrape_interval: 15s metrics_path: /metrics","title":"Scrape Configuration"},{"location":"features/metrics/#kubernetes-servicemonitor","text":"apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: relaypoint labels: app: relaypoint spec: selector: matchLabels: app: relaypoint endpoints: - port: metrics interval: 15s path: /metrics","title":"Kubernetes ServiceMonitor"},{"location":"features/metrics/#grafana-dashboards","text":"","title":"Grafana Dashboards"},{"location":"features/metrics/#overview-dashboard","text":"Create panels for: Request Rate sum(rate(gateway_requests_total[5m])) Error Rate sum(rate(gateway_errors_total[5m])) / sum(rate(gateway_requests_total[5m])) * 100 Latency (p50, p90, p99) histogram_quantile(0.50, sum(rate(gateway_request_duration_seconds_bucket[5m])) by (le)) histogram_quantile(0.90, sum(rate(gateway_request_duration_seconds_bucket[5m])) by (le)) histogram_quantile(0.99, sum(rate(gateway_request_duration_seconds_bucket[5m])) by (le)) Requests In Flight sum(gateway_requests_in_flight) Backend Health sum(gateway_upstream_healthy) Rate Limit Hits sum(rate(gateway_rate_limit_hits_total[5m]))","title":"Overview Dashboard"},{"location":"features/metrics/#sample-dashboard-json","text":"{ \"title\": \"Relaypoint Gateway\", \"panels\": [ { \"title\": \"Request Rate\", \"type\": \"graph\", \"targets\": [ { \"expr\": \"sum(rate(gateway_requests_total[5m]))\", \"legendFormat\": \"Requests/sec\" } ] }, { \"title\": \"Latency Percentiles\", \"type\": \"graph\", \"targets\": [ { \"expr\": \"histogram_quantile(0.50, sum(rate(gateway_request_duration_seconds_bucket[5m])) by (le))\", \"legendFormat\": \"p50\" }, { \"expr\": \"histogram_quantile(0.90, sum(rate(gateway_request_duration_seconds_bucket[5m])) by (le))\", \"legendFormat\": \"p90\" }, { \"expr\": \"histogram_quantile(0.99, sum(rate(gateway_request_duration_seconds_bucket[5m])) by (le))\", \"legendFormat\": \"p99\" } ] }, { \"title\": \"Backend Health\", \"type\": \"stat\", \"targets\": [ { \"expr\": \"sum(gateway_upstream_healthy)\", \"legendFormat\": \"Healthy Backends\" } ] } ] }","title":"Sample Dashboard JSON"},{"location":"features/metrics/#alerting-rules","text":"","title":"Alerting Rules"},{"location":"features/metrics/#prometheus-alerting-rules","text":"# alerts.yml groups: - name: relaypoint rules: # High error rate - alert: HighErrorRate expr: | sum(rate(gateway_errors_total[5m])) / sum(rate(gateway_requests_total[5m])) > 0.05 for: 5m labels: severity: warning annotations: summary: \"High error rate (> 5%)\" description: \"Error rate is {{ $value | humanizePercentage }}\" # Very high error rate - alert: CriticalErrorRate expr: | sum(rate(gateway_errors_total[5m])) / sum(rate(gateway_requests_total[5m])) > 0.25 for: 2m labels: severity: critical annotations: summary: \"Critical error rate (> 25%)\" # High latency - alert: HighLatency expr: | histogram_quantile(0.95, sum(rate(gateway_request_duration_seconds_bucket[5m])) by (le) ) > 1 for: 5m labels: severity: warning annotations: summary: \"High p95 latency (> 1s)\" description: \"p95 latency is {{ $value | humanizeDuration }}\" # Backend unhealthy - alert: BackendUnhealthy expr: gateway_upstream_healthy == 0 for: 1m labels: severity: warning annotations: summary: \"Backend is unhealthy\" description: \"Backend {{ $labels.key }} is down\" # All backends unhealthy - alert: AllBackendsDown expr: sum by (upstream) (gateway_upstream_healthy) == 0 for: 30s labels: severity: critical annotations: summary: \"All backends for {{ $labels.upstream }} are down\" # High rate limiting - alert: HighRateLimiting expr: | sum(rate(gateway_rate_limit_hits_total[5m])) / sum(rate(gateway_requests_total[5m])) > 0.1 for: 5m labels: severity: warning annotations: summary: \"High rate limiting (> 10% of requests)\" # No requests (service might be down) - alert: NoRequests expr: sum(rate(gateway_requests_total[5m])) == 0 for: 5m labels: severity: warning annotations: summary: \"No requests received in 5 minutes\"","title":"Prometheus Alerting Rules"},{"location":"features/metrics/#logging","text":"Relaypoint outputs structured JSON logs: {\"level\":\"INFO\",\"msg\":\"Starting RelayPoint\",\"config\":\"relaypoint.yml\"} {\"level\":\"INFO\",\"msg\":\"configuration loaded\",\"routes\":5,\"upstreams\":3} {\"level\":\"INFO\",\"msg\":\"relaypoint API Gateway starting\",\"address\":\"0.0.0.0:8080\"} {\"level\":\"WARN\",\"msg\":\"upstream unhealthy\",\"upstream\":\"api-service\",\"target\":\"http://backend-2:3000\"}","title":"Logging"},{"location":"features/metrics/#log-levels","text":"Level Description INFO Normal operational messages WARN Warning conditions (e.g., unhealthy backend) ERROR Error conditions (e.g., configuration errors)","title":"Log Levels"},{"location":"features/metrics/#log-aggregation","text":"Forward logs to your preferred system: # Send to file ./relaypoint -config relaypoint.yml 2>&1 | tee /var/log/relaypoint.log # Send to journald (systemd) ./relaypoint -config relaypoint.yml # Parse with jq ./relaypoint -config relaypoint.yml 2>&1 | jq -r '.msg'","title":"Log Aggregation"},{"location":"features/metrics/#health-endpoint","text":"Check gateway health: curl http://localhost:8080/health Response: { \"status\": \"healthy\" } Use this for: Load balancer health checks Kubernetes liveness probes Monitoring systems","title":"Health Endpoint"},{"location":"features/metrics/#example-monitoring-stack","text":"","title":"Example Monitoring Stack"},{"location":"features/metrics/#docker-compose-setup","text":"version: \"3.8\" services: relaypoint: image: ghcr.io/relaypoint/relaypoint:latest ports: - \"8080:8080\" - \"9090:9090\" volumes: - ./relaypoint.yml:/etc/relaypoint/relaypoint.yml command: [\"-config\", \"/etc/relaypoint/relaypoint.yml\"] prometheus: image: prom/prometheus:latest ports: - \"9091:9090\" volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml - ./alerts.yml:/etc/prometheus/alerts.yml grafana: image: grafana/grafana:latest ports: - \"3000:3000\" environment: - GF_SECURITY_ADMIN_PASSWORD=admin volumes: - grafana-data:/var/lib/grafana volumes: grafana-data:","title":"Docker Compose Setup"},{"location":"features/metrics/#prometheusyml","text":"global: scrape_interval: 15s rule_files: - /etc/prometheus/alerts.yml scrape_configs: - job_name: \"relaypoint\" static_configs: - targets: [\"relaypoint:9090\"] alerting: alertmanagers: - static_configs: - targets: [\"alertmanager:9093\"]","title":"prometheus.yml"},{"location":"features/metrics/#custom-latency-buckets","text":"Configure histogram buckets for your latency profile: metrics: enabled: true latency_buckets: # For low-latency APIs (sub-100ms expected) - 0.001 # 1ms - 0.005 # 5ms - 0.01 # 10ms - 0.025 # 25ms - 0.05 # 50ms - 0.1 # 100ms - 0.25 # 250ms - 0.5 # 500ms - 1.0 # 1s metrics: enabled: true latency_buckets: # For slower APIs (seconds expected) - 0.1 # 100ms - 0.5 # 500ms - 1.0 # 1s - 2.5 # 2.5s - 5.0 # 5s - 10.0 # 10s - 30.0 # 30s - 60.0 # 60s","title":"Custom Latency Buckets"},{"location":"features/metrics/#best-practices","text":"","title":"Best Practices"},{"location":"features/metrics/#1-set-up-dashboards-first","text":"Before deploying to production, have dashboards ready for: Request rate and error rate Latency percentiles Backend health Rate limiting","title":"1. Set Up Dashboards First"},{"location":"features/metrics/#2-configure-alerts","text":"Essential alerts: High error rate (> 5%) High latency (p95 > threshold) Backend unhealthy No traffic (might indicate gateway down)","title":"2. Configure Alerts"},{"location":"features/metrics/#3-use-labels-wisely","text":"The default labels are designed for cardinality control. Avoid adding high-cardinality labels (like user IDs) to metrics.","title":"3. Use Labels Wisely"},{"location":"features/metrics/#4-monitor-the-monitor","text":"Ensure Prometheus can reach the metrics endpoint: curl http://localhost:9090/metrics","title":"4. Monitor the Monitor"},{"location":"features/metrics/#5-retention-planning","text":"Consider metric retention based on: Dashboard time ranges needed Storage capacity Compliance requirements","title":"5. Retention Planning"},{"location":"features/metrics/#next-steps","text":"Health Checks - Monitor backend health Troubleshooting - Debug issues using metrics Best Practices - Production recommendations","title":"Next Steps"},{"location":"features/rate-limiting/","text":"Relaypoint provides flexible rate limiting to protect your APIs from abuse and ensure fair usage across clients. Overview Rate limiting controls how many requests clients can make within a given time period. Relaypoint uses a token bucket algorithm that provides: Smooth request throttling Burst capacity for legitimate traffic spikes Per-client, per-route, and per-API-key limits How Token Bucket Works \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Token Bucket \u2502 \u2502 \u2502 \u2502 Capacity: 100 tokens (burst) \u2502 \u2502 Refill Rate: 50 tokens/second \u2502 \u2502 \u2502 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2502 \u2502 Current: 75 tokens \u2502 \u2502 \u2502 \u2502 Each request consumes 1 token \u2502 \u2502 Tokens refill continuously \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Tokens represent available request capacity Requests consume tokens (1 token per request) Tokens refill at a steady rate Burst size is the maximum tokens (bucket capacity) When empty, requests are rejected with 429 Too Many Requests Basic Configuration Enable rate limiting globally: rate_limit: enabled: true default_rps: 100 # 100 requests per second default_burst: 200 # Allow bursts up to 200 requests per_ip: true # Rate limit per client IP per_api_key: true # Rate limit per API key cleanup_interval: 5m # Clean up inactive limiters Rate Limiting Layers Relaypoint applies rate limits at multiple levels: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Request \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 1. Route Rate Limit \u2502 \u2502 (if configured for route) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 2. API Key Rate Limit \u2502 \u2502 (if API key provided) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 3. IP Rate Limit \u2502 \u2502 (default per-IP) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Backend Service \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 If any layer rejects the request, a 429 response is returned. Per-Route Rate Limiting Apply specific limits to individual routes: routes: # High-volume endpoint - generous limits - name: read-api path: /api/v1/data/** upstream: data-service methods: - GET rate_limit: enabled: true requests_per_second: 1000 burst_size: 2000 # Expensive operation - strict limits - name: process-api path: /api/v1/process upstream: processor methods: - POST rate_limit: enabled: true requests_per_second: 10 burst_size: 20 # Authentication - prevent brute force - name: auth-api path: /api/v1/auth/** upstream: auth-service rate_limit: enabled: true requests_per_second: 5 burst_size: 10 Per-IP Rate Limiting Limit requests from individual IP addresses: rate_limit: enabled: true default_rps: 100 default_burst: 200 per_ip: true IP Detection Relaypoint detects client IP in this order: X-Forwarded-For header (first IP in chain) X-Real-IP header Direct connection IP Handling Proxies If Relaypoint is behind a load balancer or proxy, ensure proper headers are forwarded: # Nginx example proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Real-IP $remote_addr; API Key Rate Limiting Different API keys can have different limits: rate_limit: enabled: true per_api_key: true api_keys: # Premium tier - high limits - key: \"pk_live_premium_abc123\" name: \"premium-customer\" requests_per_second: 10000 burst_size: 20000 enabled: true # Standard tier - moderate limits - key: \"pk_live_standard_def456\" name: \"standard-customer\" requests_per_second: 1000 burst_size: 2000 enabled: true # Free tier - strict limits - key: \"pk_live_free_ghi789\" name: \"free-user\" requests_per_second: 100 burst_size: 150 enabled: true # Disabled key - key: \"pk_live_disabled_xyz\" name: \"suspended-account\" enabled: false API Key Detection Relaypoint looks for API keys in: Authorization: Bearer <key> header Authorization: ApiKey <key> header X-API-Key: <key> header ?api_key=<key> query parameter Example requests: # Authorization header (preferred) curl -H \"Authorization: Bearer pk_live_abc123\" http://localhost:8080/api # X-API-Key header curl -H \"X-API-Key: pk_live_abc123\" http://localhost:8080/api # Query parameter (less secure) curl \"http://localhost:8080/api?api_key=pk_live_abc123\" Rate Limit Response When rate limited, clients receive: HTTP/1.1 429 Too Many Requests Content-Type: text/plain Retry-After: 1 Too Many Requests The Retry-After header indicates when to retry (in seconds). Choosing RPS and Burst Values Understanding the Relationship RPS (Requests Per Second) = Sustained rate Burst = Maximum spike capacity Example : RPS=100, Burst=500 Steady traffic: 100 requests/second \u2713 Spike of 500 requests: Allowed (consumes burst) After spike: Must wait for tokens to refill Guidelines Use Case RPS Burst Notes Public API 60 120 1 request/second average Authenticated user 100 200 2x burst for flexibility Internal service 1000 2000 Higher for trusted services Webhook receiver 500 1000 Handle notification bursts Login endpoint 5 10 Prevent brute force Search endpoint 30 60 Expensive operation Burst Sizing Tips Burst = 2x RPS : Good default for most APIs Burst = 10x RPS : Handles traffic spikes from batch jobs Burst = RPS : Strict limiting, no spike allowance Configuration Examples Public API with Tiers rate_limit: enabled: true default_rps: 60 # Anonymous users default_burst: 120 per_ip: true per_api_key: true api_keys: # Tier 1: Free - key: \"free_tier_key\" name: \"free\" requests_per_second: 60 burst_size: 120 enabled: true # Tier 2: Pro - key: \"pro_tier_key\" name: \"pro\" requests_per_second: 600 burst_size: 1200 enabled: true # Tier 3: Enterprise - key: \"enterprise_key\" name: \"enterprise\" requests_per_second: 6000 burst_size: 12000 enabled: true routes: # All routes use tier-based limits - name: api path: /api/** upstream: api-service Protecting Sensitive Endpoints routes: # Login - strict limit to prevent brute force - name: login path: /api/auth/login upstream: auth-service methods: [POST] rate_limit: enabled: true requests_per_second: 3 burst_size: 5 # Password reset - very strict - name: password-reset path: /api/auth/reset-password upstream: auth-service methods: [POST] rate_limit: enabled: true requests_per_second: 1 burst_size: 3 # Registration - moderate - name: register path: /api/auth/register upstream: auth-service methods: [POST] rate_limit: enabled: true requests_per_second: 5 burst_size: 10 Microservices with Different Limits routes: # Read-heavy service - high limits - name: catalog path: /catalog/** upstream: catalog-service rate_limit: enabled: true requests_per_second: 500 burst_size: 1000 # Write service - lower limits - name: orders path: /orders/** upstream: order-service rate_limit: enabled: true requests_per_second: 50 burst_size: 100 # Report generation - very low limits - name: reports path: /reports/** upstream: report-service rate_limit: enabled: true requests_per_second: 5 burst_size: 10 Monitoring Rate Limits Metrics curl http://localhost:9090/metrics | grep rate_limit Key metrics: gateway_rate_limit_hits_total{route=\"...\",type=\"...\"} - Count of rate-limited requests Types: route , apikey , ip Stats Endpoint curl http://localhost:8080/stats Shows request counts including rate-limited requests. Alerting Set up alerts for high rate limit hits: # Prometheus alerting rule example groups: - name: relaypoint rules: - alert: HighRateLimitHits expr: rate(gateway_rate_limit_hits_total[5m]) > 100 for: 5m labels: severity: warning annotations: summary: \"High rate limit hits detected\" Best Practices 1. Start Generous, Then Tighten # Start with high limits rate_limit: default_rps: 1000 default_burst: 2000 # Monitor and adjust based on actual usage rate_limit: default_rps: 200 default_burst: 400 2. Document Your Limits Provide clear documentation for API consumers: ## Rate Limits | Tier | Requests/Second | Burst | | ---------- | --------------- | ----- | | Free | 60 | 120 | | Pro | 600 | 1200 | | Enterprise | 6000 | 12000 | Rate-limited requests return `429 Too Many Requests` with a `Retry-After` header. 3. Different Limits for Different Operations routes: # Reads are cheap - path: /api/** methods: [GET] rate_limit: requests_per_second: 1000 # Writes are expensive - path: /api/** methods: [POST, PUT, DELETE] rate_limit: requests_per_second: 100 4. Consider Time of Day Use lower limits during known high-traffic periods to ensure service stability. 5. Exempt Internal Services For service-to-service communication, use high limits: api_keys: - key: \"internal_service_key\" name: \"internal-services\" requests_per_second: 100000 burst_size: 200000 enabled: true Troubleshooting Legitimate Users Being Limited Symptoms : Real users hitting rate limits unexpectedly. Solutions : Increase burst size for traffic spikes Check if shared IP (NAT/proxy) is causing issues Consider API key tiers instead of IP limiting Rate Limits Not Working Symptoms : Requests exceeding limits not being blocked. Causes : Rate limiting not enabled Route-specific limit not configured API key has unlimited access Solutions : Verify rate_limit.enabled: true in config Add explicit rate limit to route Check API key configuration Memory Usage Growing Symptoms : Memory increasing over time. Cause : Rate limiter tracking many unique clients. Solution : Ensure cleanup is configured: rate_limit: cleanup_interval: 5m # Clean up inactive limiters Next Steps Health Checks - Monitor backend health Metrics - Monitor rate limiting API Keys - Manage API keys","title":"Rate Limiting"},{"location":"features/rate-limiting/#overview","text":"Rate limiting controls how many requests clients can make within a given time period. Relaypoint uses a token bucket algorithm that provides: Smooth request throttling Burst capacity for legitimate traffic spikes Per-client, per-route, and per-API-key limits","title":"Overview"},{"location":"features/rate-limiting/#how-token-bucket-works","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Token Bucket \u2502 \u2502 \u2502 \u2502 Capacity: 100 tokens (burst) \u2502 \u2502 Refill Rate: 50 tokens/second \u2502 \u2502 \u2502 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2502 \u2502 Current: 75 tokens \u2502 \u2502 \u2502 \u2502 Each request consumes 1 token \u2502 \u2502 Tokens refill continuously \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Tokens represent available request capacity Requests consume tokens (1 token per request) Tokens refill at a steady rate Burst size is the maximum tokens (bucket capacity) When empty, requests are rejected with 429 Too Many Requests","title":"How Token Bucket Works"},{"location":"features/rate-limiting/#basic-configuration","text":"Enable rate limiting globally: rate_limit: enabled: true default_rps: 100 # 100 requests per second default_burst: 200 # Allow bursts up to 200 requests per_ip: true # Rate limit per client IP per_api_key: true # Rate limit per API key cleanup_interval: 5m # Clean up inactive limiters","title":"Basic Configuration"},{"location":"features/rate-limiting/#rate-limiting-layers","text":"Relaypoint applies rate limits at multiple levels: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Request \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 1. Route Rate Limit \u2502 \u2502 (if configured for route) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 2. API Key Rate Limit \u2502 \u2502 (if API key provided) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 3. IP Rate Limit \u2502 \u2502 (default per-IP) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Backend Service \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 If any layer rejects the request, a 429 response is returned.","title":"Rate Limiting Layers"},{"location":"features/rate-limiting/#per-route-rate-limiting","text":"Apply specific limits to individual routes: routes: # High-volume endpoint - generous limits - name: read-api path: /api/v1/data/** upstream: data-service methods: - GET rate_limit: enabled: true requests_per_second: 1000 burst_size: 2000 # Expensive operation - strict limits - name: process-api path: /api/v1/process upstream: processor methods: - POST rate_limit: enabled: true requests_per_second: 10 burst_size: 20 # Authentication - prevent brute force - name: auth-api path: /api/v1/auth/** upstream: auth-service rate_limit: enabled: true requests_per_second: 5 burst_size: 10","title":"Per-Route Rate Limiting"},{"location":"features/rate-limiting/#per-ip-rate-limiting","text":"Limit requests from individual IP addresses: rate_limit: enabled: true default_rps: 100 default_burst: 200 per_ip: true","title":"Per-IP Rate Limiting"},{"location":"features/rate-limiting/#ip-detection","text":"Relaypoint detects client IP in this order: X-Forwarded-For header (first IP in chain) X-Real-IP header Direct connection IP","title":"IP Detection"},{"location":"features/rate-limiting/#handling-proxies","text":"If Relaypoint is behind a load balancer or proxy, ensure proper headers are forwarded: # Nginx example proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Real-IP $remote_addr;","title":"Handling Proxies"},{"location":"features/rate-limiting/#api-key-rate-limiting","text":"Different API keys can have different limits: rate_limit: enabled: true per_api_key: true api_keys: # Premium tier - high limits - key: \"pk_live_premium_abc123\" name: \"premium-customer\" requests_per_second: 10000 burst_size: 20000 enabled: true # Standard tier - moderate limits - key: \"pk_live_standard_def456\" name: \"standard-customer\" requests_per_second: 1000 burst_size: 2000 enabled: true # Free tier - strict limits - key: \"pk_live_free_ghi789\" name: \"free-user\" requests_per_second: 100 burst_size: 150 enabled: true # Disabled key - key: \"pk_live_disabled_xyz\" name: \"suspended-account\" enabled: false","title":"API Key Rate Limiting"},{"location":"features/rate-limiting/#api-key-detection","text":"Relaypoint looks for API keys in: Authorization: Bearer <key> header Authorization: ApiKey <key> header X-API-Key: <key> header ?api_key=<key> query parameter Example requests: # Authorization header (preferred) curl -H \"Authorization: Bearer pk_live_abc123\" http://localhost:8080/api # X-API-Key header curl -H \"X-API-Key: pk_live_abc123\" http://localhost:8080/api # Query parameter (less secure) curl \"http://localhost:8080/api?api_key=pk_live_abc123\"","title":"API Key Detection"},{"location":"features/rate-limiting/#rate-limit-response","text":"When rate limited, clients receive: HTTP/1.1 429 Too Many Requests Content-Type: text/plain Retry-After: 1 Too Many Requests The Retry-After header indicates when to retry (in seconds).","title":"Rate Limit Response"},{"location":"features/rate-limiting/#choosing-rps-and-burst-values","text":"","title":"Choosing RPS and Burst Values"},{"location":"features/rate-limiting/#understanding-the-relationship","text":"RPS (Requests Per Second) = Sustained rate Burst = Maximum spike capacity Example : RPS=100, Burst=500 Steady traffic: 100 requests/second \u2713 Spike of 500 requests: Allowed (consumes burst) After spike: Must wait for tokens to refill","title":"Understanding the Relationship"},{"location":"features/rate-limiting/#guidelines","text":"Use Case RPS Burst Notes Public API 60 120 1 request/second average Authenticated user 100 200 2x burst for flexibility Internal service 1000 2000 Higher for trusted services Webhook receiver 500 1000 Handle notification bursts Login endpoint 5 10 Prevent brute force Search endpoint 30 60 Expensive operation","title":"Guidelines"},{"location":"features/rate-limiting/#burst-sizing-tips","text":"Burst = 2x RPS : Good default for most APIs Burst = 10x RPS : Handles traffic spikes from batch jobs Burst = RPS : Strict limiting, no spike allowance","title":"Burst Sizing Tips"},{"location":"features/rate-limiting/#configuration-examples","text":"","title":"Configuration Examples"},{"location":"features/rate-limiting/#public-api-with-tiers","text":"rate_limit: enabled: true default_rps: 60 # Anonymous users default_burst: 120 per_ip: true per_api_key: true api_keys: # Tier 1: Free - key: \"free_tier_key\" name: \"free\" requests_per_second: 60 burst_size: 120 enabled: true # Tier 2: Pro - key: \"pro_tier_key\" name: \"pro\" requests_per_second: 600 burst_size: 1200 enabled: true # Tier 3: Enterprise - key: \"enterprise_key\" name: \"enterprise\" requests_per_second: 6000 burst_size: 12000 enabled: true routes: # All routes use tier-based limits - name: api path: /api/** upstream: api-service","title":"Public API with Tiers"},{"location":"features/rate-limiting/#protecting-sensitive-endpoints","text":"routes: # Login - strict limit to prevent brute force - name: login path: /api/auth/login upstream: auth-service methods: [POST] rate_limit: enabled: true requests_per_second: 3 burst_size: 5 # Password reset - very strict - name: password-reset path: /api/auth/reset-password upstream: auth-service methods: [POST] rate_limit: enabled: true requests_per_second: 1 burst_size: 3 # Registration - moderate - name: register path: /api/auth/register upstream: auth-service methods: [POST] rate_limit: enabled: true requests_per_second: 5 burst_size: 10","title":"Protecting Sensitive Endpoints"},{"location":"features/rate-limiting/#microservices-with-different-limits","text":"routes: # Read-heavy service - high limits - name: catalog path: /catalog/** upstream: catalog-service rate_limit: enabled: true requests_per_second: 500 burst_size: 1000 # Write service - lower limits - name: orders path: /orders/** upstream: order-service rate_limit: enabled: true requests_per_second: 50 burst_size: 100 # Report generation - very low limits - name: reports path: /reports/** upstream: report-service rate_limit: enabled: true requests_per_second: 5 burst_size: 10","title":"Microservices with Different Limits"},{"location":"features/rate-limiting/#monitoring-rate-limits","text":"","title":"Monitoring Rate Limits"},{"location":"features/rate-limiting/#metrics","text":"curl http://localhost:9090/metrics | grep rate_limit Key metrics: gateway_rate_limit_hits_total{route=\"...\",type=\"...\"} - Count of rate-limited requests Types: route , apikey , ip","title":"Metrics"},{"location":"features/rate-limiting/#stats-endpoint","text":"curl http://localhost:8080/stats Shows request counts including rate-limited requests.","title":"Stats Endpoint"},{"location":"features/rate-limiting/#alerting","text":"Set up alerts for high rate limit hits: # Prometheus alerting rule example groups: - name: relaypoint rules: - alert: HighRateLimitHits expr: rate(gateway_rate_limit_hits_total[5m]) > 100 for: 5m labels: severity: warning annotations: summary: \"High rate limit hits detected\"","title":"Alerting"},{"location":"features/rate-limiting/#best-practices","text":"","title":"Best Practices"},{"location":"features/rate-limiting/#1-start-generous-then-tighten","text":"# Start with high limits rate_limit: default_rps: 1000 default_burst: 2000 # Monitor and adjust based on actual usage rate_limit: default_rps: 200 default_burst: 400","title":"1. Start Generous, Then Tighten"},{"location":"features/rate-limiting/#2-document-your-limits","text":"Provide clear documentation for API consumers: ## Rate Limits | Tier | Requests/Second | Burst | | ---------- | --------------- | ----- | | Free | 60 | 120 | | Pro | 600 | 1200 | | Enterprise | 6000 | 12000 | Rate-limited requests return `429 Too Many Requests` with a `Retry-After` header.","title":"2. Document Your Limits"},{"location":"features/rate-limiting/#3-different-limits-for-different-operations","text":"routes: # Reads are cheap - path: /api/** methods: [GET] rate_limit: requests_per_second: 1000 # Writes are expensive - path: /api/** methods: [POST, PUT, DELETE] rate_limit: requests_per_second: 100","title":"3. Different Limits for Different Operations"},{"location":"features/rate-limiting/#4-consider-time-of-day","text":"Use lower limits during known high-traffic periods to ensure service stability.","title":"4. Consider Time of Day"},{"location":"features/rate-limiting/#5-exempt-internal-services","text":"For service-to-service communication, use high limits: api_keys: - key: \"internal_service_key\" name: \"internal-services\" requests_per_second: 100000 burst_size: 200000 enabled: true","title":"5. Exempt Internal Services"},{"location":"features/rate-limiting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"features/rate-limiting/#legitimate-users-being-limited","text":"Symptoms : Real users hitting rate limits unexpectedly. Solutions : Increase burst size for traffic spikes Check if shared IP (NAT/proxy) is causing issues Consider API key tiers instead of IP limiting","title":"Legitimate Users Being Limited"},{"location":"features/rate-limiting/#rate-limits-not-working","text":"Symptoms : Requests exceeding limits not being blocked. Causes : Rate limiting not enabled Route-specific limit not configured API key has unlimited access Solutions : Verify rate_limit.enabled: true in config Add explicit rate limit to route Check API key configuration","title":"Rate Limits Not Working"},{"location":"features/rate-limiting/#memory-usage-growing","text":"Symptoms : Memory increasing over time. Cause : Rate limiter tracking many unique clients. Solution : Ensure cleanup is configured: rate_limit: cleanup_interval: 5m # Clean up inactive limiters","title":"Memory Usage Growing"},{"location":"features/rate-limiting/#next-steps","text":"Health Checks - Monitor backend health Metrics - Monitor rate limiting API Keys - Manage API keys","title":"Next Steps"},{"location":"features/routing/","text":"Relaypoint provides a powerful and flexible routing system to direct incoming requests to the appropriate backend services. Basic Routing At its simplest, a route maps a URL path to an upstream: routes: - name: api path: /api upstream: backend-service This routes exact matches of /api to the backend-service upstream. Path Patterns Exact Matching Matches only the exact path: routes: - name: users-list path: /api/users upstream: user-service Request Path Match /api/users \u2713 Yes /api/users/ \u2717 No /api/users/123 \u2717 No Single-Segment Wildcard ( * ) Matches exactly one path segment: routes: - name: user-by-id path: /api/users/* upstream: user-service Request Path Match /api/users/123 \u2713 Yes /api/users/abc \u2713 Yes /api/users \u2717 No /api/users/123/orders \u2717 No Multi-Segment Wildcard ( ** ) Matches zero or more path segments: routes: - name: api-catchall path: /api/** upstream: api-service Request Path Match /api \u2713 Yes /api/users \u2713 Yes /api/users/123 \u2713 Yes /api/users/123/orders/456 \u2713 Yes /other \u2717 No Named Parameters Capture path segments as named parameters: routes: # Colon syntax - name: user-detail path: /api/users/:userId upstream: user-service # Curly brace syntax (alternative) - name: order-detail path: /api/orders/{orderId} upstream: order-service # Multiple parameters - name: user-order path: /api/users/:userId/orders/:orderId upstream: order-service Pattern Request Path Captured Parameters /users/:id /users/123 id=123 /users/:userId/orders/:orderId /users/42/orders/99 userId=42, orderId=99 Route Priority When multiple routes could match a request, Relaypoint uses priority ordering: More specific paths take precedence over less specific ones Exact segments beat wildcards Single wildcards ( * ) beat multi-segment wildcards ( ** ) Named parameters are treated like single wildcards Example Priority routes: # Priority 1: Exact match (highest) - name: users-list path: /api/v1/users upstream: users # Priority 2: Specific path with wildcard - name: user-by-id path: /api/v1/users/* upstream: users # Priority 3: Less specific path - name: v1-api path: /api/v1/** upstream: api # Priority 4: Catch-all (lowest) - name: catchall path: /** upstream: default Request Path Matched Route /api/v1/users users-list /api/v1/users/123 user-by-id /api/v1/orders v1-api /api/v2/anything catchall Host-Based Routing Route requests based on the Host header: routes: # Route by specific host - name: api-routes host: api.example.com path: /** upstream: api-service # Wildcard host matching - name: tenant-routes host: \"*.example.com\" path: /** upstream: tenant-service # Default (no host specified matches all) - name: default path: /** upstream: default-service Host Request Path Matched Route api.example.com /users api-routes tenant1.example.com /data tenant-routes other.com /anything default Method Filtering Restrict routes to specific HTTP methods: routes: # Read operations - name: users-read path: /api/users/** methods: - GET - HEAD upstream: user-read-service # Write operations - name: users-write path: /api/users/** methods: - POST - PUT - PATCH - DELETE upstream: user-write-service If methods is not specified or empty, all HTTP methods are allowed. Path Stripping Remove the matched path prefix before forwarding to the upstream: routes: - name: user-service path: /api/v1/users/** upstream: user-service strip_path: true strip_path Request Path Upstream Path false /api/v1/users/123 /api/v1/users/123 true /api/v1/users/123 /123 This is useful when your backend services don't expect the gateway prefix. Header Injection Add custom headers to upstream requests: routes: - name: api path: /api/** upstream: api-service headers: X-Gateway: \"relaypoint\" X-Request-Source: \"public\" X-Service-Version: \"v1\" These headers are added to every request forwarded to the upstream. Request Forwarding Headers Relaypoint automatically adds standard proxy headers: Header Description X-Forwarded-For Client IP address (appended to existing) X-Forwarded-Host Original Host header X-Forwarded-Proto Original protocol ( http or https ) X-Real-IP Client IP address Route-Specific Rate Limiting Apply rate limits to specific routes: routes: - name: expensive-operation path: /api/process upstream: processor rate_limit: enabled: true requests_per_second: 10 burst_size: 20 - name: normal-api path: /api/** upstream: api-service # No rate limit - uses global defaults See Rate Limiting for more details. Route-Specific Timeouts Set custom timeouts per route: routes: - name: quick-api path: /api/fast/** upstream: fast-service timeout: 5s - name: slow-operation path: /api/reports/** upstream: report-service timeout: 120s Combining Patterns Create sophisticated routing rules by combining patterns: upstreams: - name: user-service targets: - url: http://users:3000 - name: order-service targets: - url: http://orders:3000 - name: admin-service targets: - url: http://admin:3000 routes: # Admin routes with strict rate limiting - name: admin-api host: admin.example.com path: /api/** methods: - GET - POST - PUT - DELETE upstream: admin-service rate_limit: enabled: true requests_per_second: 100 burst_size: 150 headers: X-Admin-Request: \"true\" # User service - read operations - name: users-read path: /api/v1/users/** methods: - GET upstream: user-service # User service - write operations (stricter rate limit) - name: users-write path: /api/v1/users/** methods: - POST - PUT - DELETE upstream: user-service rate_limit: enabled: true requests_per_second: 20 burst_size: 30 # Order service with path stripping - name: orders path: /api/v1/orders/** upstream: order-service strip_path: true timeout: 60s Common Routing Patterns Versioned API routes: - name: v2-api path: /api/v2/** upstream: api-v2 - name: v1-api path: /api/v1/** upstream: api-v1 - name: latest-api path: /api/** upstream: api-v2 # Default to latest Microservices Gateway routes: - name: users path: /users/** upstream: user-service strip_path: true - name: products path: /products/** upstream: product-service strip_path: true - name: orders path: /orders/** upstream: order-service strip_path: true Multi-Tenant Routing routes: # Tenant-specific subdomains - name: tenant-api host: \"*.app.example.com\" path: /api/** upstream: tenant-service # Main application - name: main-api host: app.example.com path: /api/** upstream: main-service Read/Write Splitting routes: - name: read-operations path: /api/** methods: - GET - HEAD - OPTIONS upstream: read-replicas - name: write-operations path: /api/** methods: - POST - PUT - PATCH - DELETE upstream: primary-database Debugging Routes To understand which route is matching your requests: Check the logs - Relaypoint logs route matches at debug level Use the stats endpoint - GET /stats shows request counts per route Check metrics - Prometheus metrics include route labels # View stats curl http://localhost:8080/stats | jq # Check specific route in metrics curl http://localhost:9090/metrics | grep gateway_requests_total Next Steps Load Balancing - Configure backend load balancing Rate Limiting - Protect your APIs Health Checks - Monitor backend health","title":"Routing"},{"location":"features/routing/#basic-routing","text":"At its simplest, a route maps a URL path to an upstream: routes: - name: api path: /api upstream: backend-service This routes exact matches of /api to the backend-service upstream.","title":"Basic Routing"},{"location":"features/routing/#path-patterns","text":"","title":"Path Patterns"},{"location":"features/routing/#exact-matching","text":"Matches only the exact path: routes: - name: users-list path: /api/users upstream: user-service Request Path Match /api/users \u2713 Yes /api/users/ \u2717 No /api/users/123 \u2717 No","title":"Exact Matching"},{"location":"features/routing/#single-segment-wildcard","text":"Matches exactly one path segment: routes: - name: user-by-id path: /api/users/* upstream: user-service Request Path Match /api/users/123 \u2713 Yes /api/users/abc \u2713 Yes /api/users \u2717 No /api/users/123/orders \u2717 No","title":"Single-Segment Wildcard (*)"},{"location":"features/routing/#multi-segment-wildcard","text":"Matches zero or more path segments: routes: - name: api-catchall path: /api/** upstream: api-service Request Path Match /api \u2713 Yes /api/users \u2713 Yes /api/users/123 \u2713 Yes /api/users/123/orders/456 \u2713 Yes /other \u2717 No","title":"Multi-Segment Wildcard (**)"},{"location":"features/routing/#named-parameters","text":"Capture path segments as named parameters: routes: # Colon syntax - name: user-detail path: /api/users/:userId upstream: user-service # Curly brace syntax (alternative) - name: order-detail path: /api/orders/{orderId} upstream: order-service # Multiple parameters - name: user-order path: /api/users/:userId/orders/:orderId upstream: order-service Pattern Request Path Captured Parameters /users/:id /users/123 id=123 /users/:userId/orders/:orderId /users/42/orders/99 userId=42, orderId=99","title":"Named Parameters"},{"location":"features/routing/#route-priority","text":"When multiple routes could match a request, Relaypoint uses priority ordering: More specific paths take precedence over less specific ones Exact segments beat wildcards Single wildcards ( * ) beat multi-segment wildcards ( ** ) Named parameters are treated like single wildcards","title":"Route Priority"},{"location":"features/routing/#example-priority","text":"routes: # Priority 1: Exact match (highest) - name: users-list path: /api/v1/users upstream: users # Priority 2: Specific path with wildcard - name: user-by-id path: /api/v1/users/* upstream: users # Priority 3: Less specific path - name: v1-api path: /api/v1/** upstream: api # Priority 4: Catch-all (lowest) - name: catchall path: /** upstream: default Request Path Matched Route /api/v1/users users-list /api/v1/users/123 user-by-id /api/v1/orders v1-api /api/v2/anything catchall","title":"Example Priority"},{"location":"features/routing/#host-based-routing","text":"Route requests based on the Host header: routes: # Route by specific host - name: api-routes host: api.example.com path: /** upstream: api-service # Wildcard host matching - name: tenant-routes host: \"*.example.com\" path: /** upstream: tenant-service # Default (no host specified matches all) - name: default path: /** upstream: default-service Host Request Path Matched Route api.example.com /users api-routes tenant1.example.com /data tenant-routes other.com /anything default","title":"Host-Based Routing"},{"location":"features/routing/#method-filtering","text":"Restrict routes to specific HTTP methods: routes: # Read operations - name: users-read path: /api/users/** methods: - GET - HEAD upstream: user-read-service # Write operations - name: users-write path: /api/users/** methods: - POST - PUT - PATCH - DELETE upstream: user-write-service If methods is not specified or empty, all HTTP methods are allowed.","title":"Method Filtering"},{"location":"features/routing/#path-stripping","text":"Remove the matched path prefix before forwarding to the upstream: routes: - name: user-service path: /api/v1/users/** upstream: user-service strip_path: true strip_path Request Path Upstream Path false /api/v1/users/123 /api/v1/users/123 true /api/v1/users/123 /123 This is useful when your backend services don't expect the gateway prefix.","title":"Path Stripping"},{"location":"features/routing/#header-injection","text":"Add custom headers to upstream requests: routes: - name: api path: /api/** upstream: api-service headers: X-Gateway: \"relaypoint\" X-Request-Source: \"public\" X-Service-Version: \"v1\" These headers are added to every request forwarded to the upstream.","title":"Header Injection"},{"location":"features/routing/#request-forwarding-headers","text":"Relaypoint automatically adds standard proxy headers: Header Description X-Forwarded-For Client IP address (appended to existing) X-Forwarded-Host Original Host header X-Forwarded-Proto Original protocol ( http or https ) X-Real-IP Client IP address","title":"Request Forwarding Headers"},{"location":"features/routing/#route-specific-rate-limiting","text":"Apply rate limits to specific routes: routes: - name: expensive-operation path: /api/process upstream: processor rate_limit: enabled: true requests_per_second: 10 burst_size: 20 - name: normal-api path: /api/** upstream: api-service # No rate limit - uses global defaults See Rate Limiting for more details.","title":"Route-Specific Rate Limiting"},{"location":"features/routing/#route-specific-timeouts","text":"Set custom timeouts per route: routes: - name: quick-api path: /api/fast/** upstream: fast-service timeout: 5s - name: slow-operation path: /api/reports/** upstream: report-service timeout: 120s","title":"Route-Specific Timeouts"},{"location":"features/routing/#combining-patterns","text":"Create sophisticated routing rules by combining patterns: upstreams: - name: user-service targets: - url: http://users:3000 - name: order-service targets: - url: http://orders:3000 - name: admin-service targets: - url: http://admin:3000 routes: # Admin routes with strict rate limiting - name: admin-api host: admin.example.com path: /api/** methods: - GET - POST - PUT - DELETE upstream: admin-service rate_limit: enabled: true requests_per_second: 100 burst_size: 150 headers: X-Admin-Request: \"true\" # User service - read operations - name: users-read path: /api/v1/users/** methods: - GET upstream: user-service # User service - write operations (stricter rate limit) - name: users-write path: /api/v1/users/** methods: - POST - PUT - DELETE upstream: user-service rate_limit: enabled: true requests_per_second: 20 burst_size: 30 # Order service with path stripping - name: orders path: /api/v1/orders/** upstream: order-service strip_path: true timeout: 60s","title":"Combining Patterns"},{"location":"features/routing/#common-routing-patterns","text":"","title":"Common Routing Patterns"},{"location":"features/routing/#versioned-api","text":"routes: - name: v2-api path: /api/v2/** upstream: api-v2 - name: v1-api path: /api/v1/** upstream: api-v1 - name: latest-api path: /api/** upstream: api-v2 # Default to latest","title":"Versioned API"},{"location":"features/routing/#microservices-gateway","text":"routes: - name: users path: /users/** upstream: user-service strip_path: true - name: products path: /products/** upstream: product-service strip_path: true - name: orders path: /orders/** upstream: order-service strip_path: true","title":"Microservices Gateway"},{"location":"features/routing/#multi-tenant-routing","text":"routes: # Tenant-specific subdomains - name: tenant-api host: \"*.app.example.com\" path: /api/** upstream: tenant-service # Main application - name: main-api host: app.example.com path: /api/** upstream: main-service","title":"Multi-Tenant Routing"},{"location":"features/routing/#readwrite-splitting","text":"routes: - name: read-operations path: /api/** methods: - GET - HEAD - OPTIONS upstream: read-replicas - name: write-operations path: /api/** methods: - POST - PUT - PATCH - DELETE upstream: primary-database","title":"Read/Write Splitting"},{"location":"features/routing/#debugging-routes","text":"To understand which route is matching your requests: Check the logs - Relaypoint logs route matches at debug level Use the stats endpoint - GET /stats shows request counts per route Check metrics - Prometheus metrics include route labels # View stats curl http://localhost:8080/stats | jq # Check specific route in metrics curl http://localhost:9090/metrics | grep gateway_requests_total","title":"Debugging Routes"},{"location":"features/routing/#next-steps","text":"Load Balancing - Configure backend load balancing Rate Limiting - Protect your APIs Health Checks - Monitor backend health","title":"Next Steps"}]}